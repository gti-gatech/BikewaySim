{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bicycle Infrastructure Dating\n",
    "This notebook is for examining all of the bicycling network files available and determining the approximate build date (when facility was available for use).\n",
    "\n",
    "For approximate build date, we just need dates for anything built between 2012-2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = json.load((Path.cwd().parent / 'config.json').open('rb'))\n",
    "export_fp = Path(config['project_directory']) / 'Cycling_Infra_Dating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studyarea_geo = gpd.read_file(config['studyarea']).to_crs('epsg:4326')#.unary_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add reference osm files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years = [2014,2015,2016,2023]\n",
    "\n",
    "# for year in years:\n",
    "#     raw_osm = gpd.read_file(Path(config['project_directory'])/f\"OSM_Download/osm_{year}.gpkg\",layer='raw')\n",
    "#     raw_osm.to_file(export_fp/'reference_layers.gpkg',layer=f\"osm_{year}\")\n",
    "#     del raw_osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a version of OSM for editing and adding data to (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# osm = gpd.read_file(export_fp/'reference_layers.gpkg',layer='osm_2023')\n",
    "# overwrite = False\n",
    "# if overwrite:\n",
    "#     osm.to_file(export_fp/'osm_edit.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City of Atlanta / Atlanta Regional Commission / Garber Processing\n",
    "NOTE: There are some data quality issues with the CoA and ARC datasets in that sometimes facilities are marked as there when they aren't. Refine this in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City of Atlanta \n",
    "Has year installed date and other notes. Contains sharrows. Also contains \"planned\" infrastructure that may not have actually been completed. Some street names have changed like \"Confederate\" to \"United\"\n",
    "\n",
    "Links to Infra Installation Dates (move to an excel sheet):\n",
    "- https://www.letspropelatl.org/infra-tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO, resolve difference between the old inventory and the old as some streets had their facilities upgraded recently (Edgewood) and (Confederate/United)\n",
    "coa = gpd.read_file('D:/RAW/City_of_Atlanta/coa_bike_facilities_new.geojson',mask=studyarea_geo)\n",
    "coa.to_crs(config['projected_crs_epsg'],inplace=True)\n",
    "\n",
    "# only keep existing and ones where the year is defined\n",
    "coa = coa[(coa['Status']=='Existing') & coa['YearInstalled'].notna()]\n",
    "# remove uncessary columns\n",
    "coa.drop(columns=['GlobalID','Shape__Length','LengthMi','CrossSectionNotes','Status'],inplace=True)\n",
    "\n",
    "#rename the id/year column\n",
    "coa.rename(columns={'OBJECTID':'id','YearInstalled':'year'},inplace=True)\n",
    "\n",
    "#mark facilities that need to be dated\n",
    "coa['need_date'] = (coa['year'] >= 2012) & (coa['year'] <= 2016)\n",
    "\n",
    "# rename columns for consistency\n",
    "coa.columns = ['coa_'+col.lower() if col != 'geometry' else 'geometry' for col in coa.columns.tolist()]\n",
    "\n",
    "# convert facility type to OSM (use highest protection if two different types)\n",
    "osm_types = ['sharrow','bike lane','buffered bike lane','cycletrack','multi use path']\n",
    "coa_conversion = {\n",
    "       'Protected Bike Lane': osm_types[3], \n",
    "       'Protected Bike Lane / Bike Lane': osm_types[3],\n",
    "       'Two-Way Cycle Track': osm_types[3], \n",
    "       'Uphill Bike Lane / Downhill Sharrows': osm_types[1],\n",
    "       'Sharrows': osm_types[0], \n",
    "       'Bike Lane': osm_types[1], \n",
    "       'Bike Lane ': osm_types[1],\n",
    "       'Uphill Buffered Bike Lane / Downhill Sharrows': osm_types[2],\n",
    "       'Buffered Bike Lane': osm_types[2], \n",
    "       'Buffered Contraflow Bike Lane / Bike Lane': osm_types[1],\n",
    "       'Shared-Use Path': osm_types[4], \n",
    "       'Neighborhood Greenway': osm_types[0], \n",
    "       'Bike Lane / Sharrows': osm_types[1],\n",
    "       'Shared-Use Path / Bike Lane': osm_types[4], \n",
    "       'Buffered Bike Lane / Bike Lane': osm_types[2],\n",
    "       'Buffered Bike Lane / Shared-Use Path': osm_types[4],\n",
    "       'Shared-Use Path / Sharrows': osm_types[4],\n",
    "       'Uphill Protected Bike Lane / Downhill Sharrows': osm_types[3], \n",
    "       'Shared Path': osm_types[4]\n",
    "}\n",
    "coa['coa_osm_type'] = coa['coa_facilitytype'].map(coa_conversion)\n",
    "\n",
    "#export\n",
    "coa.to_file(export_fp/'reference_layers.gpkg',layer='coa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atlanta Regional Commission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc = gpd.read_file('D:\\RAW\\Atlanta_Regional_Comission\\Bike_Ped_Trail_Inventory_January2024.geojson',mask=studyarea_geo)\n",
    "arc.to_crs(config['projected_crs_epsg'],inplace=True)\n",
    "\n",
    "# the only na values are OTP\n",
    "#arc[arc['Year_2'].isna()].explore()\n",
    "\n",
    "# remove post 2016\n",
    "arc = arc[arc['Year_2'] < 2016]\n",
    "\n",
    "#clean the columns\n",
    "arc = arc[['OBJECTID_1','Name', 'spec','Width', 'Material', 'Year_2', 'geometry']]\n",
    "\n",
    "#rename the id column\n",
    "arc.rename(columns={'OBJECTID_1':'id','Year_2':'year'},inplace=True)\n",
    "\n",
    "#mark facilities that need to be dated\n",
    "arc['need_date'] = (arc['year'] >= 2012) & (arc['year'] <= 2016)\n",
    "\n",
    "# rename columns for consistency\n",
    "arc.columns = ['arc_'+col.lower() if col != 'geometry' else 'geometry' for col in arc.columns.tolist()]\n",
    "\n",
    "# remove these facilities\n",
    "remove = ['Paved shoulder','Park Trail','Campus Path']\n",
    "arc = arc[arc['arc_spec'].isin(remove)==False]\n",
    "\n",
    "# convert to osm name\n",
    "arc_conversion = {\n",
    "    'Hard surface multi-use path': 'multi use path',\n",
    "    'Protected bike lane': 'cycletrack',\n",
    "    'Uphill bike lanes / downhill sharrows': 'bike lane',\n",
    "    'Conventional bike lane': 'bike lane',\n",
    "    'Buffered bike lane': 'buffered bike lane',\n",
    "    'Bike lane with parking': 'bike lane',\n",
    "    'Buffered bike lane (BUS ONLY lane)': 'buffered bike lane',\n",
    "    #'Paved shoulder': 'bike lane', # consider dumping these\n",
    "    'Two way cycle track': 'cycletrack',\n",
    "    'Bike lane': 'bike lane',\n",
    "    'Buffered/bike lane': 'buffered bike lane',\n",
    "    'Shared use path or greenway': 'multi use path',\n",
    "    'Side path': 'multi use path',\n",
    "    'Bike lane and Side path': 'bike lane',\n",
    "    'Uphill bike lane/downhill sharrow': 'bike lane',\n",
    "    'Side Path': 'multi use path',\n",
    "    'Climbing lane': 'bike lane',\n",
    "    'Sidepath': 'multi use path',\n",
    "    #'Park Trail': 'multi use path',\n",
    "    'Bike Lane with parking': 'bike lane',\n",
    "    #'Campus Path': 'multi use path',\n",
    "    'Bike Lane': 'bike lane'\n",
    "    #'Park Path': 'multi use path'\n",
    "}\n",
    "arc['arc_osm_type'] = arc['arc_spec'].map(arc_conversion)\n",
    "\n",
    "#export\n",
    "arc.to_file(export_fp/'reference_layers.gpkg',layer='arc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Michael Garber (come back to later for the osm conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# garber = gpd.read_file('D:/RAW/Michael Garber/data_for_reid.shp',mask=studyarea_geo)\n",
    "# garber.to_crs(config['projected_crs_epsg'],inplace=True)\n",
    "\n",
    "# # remove post 2016\n",
    "# garber = garber[(pd.to_datetime(garber['rbbn_dt']).apply(lambda row: row.year) <= 2016) | (garber['rbbn_dt'].isna())]\n",
    "\n",
    "# #clean the columns\n",
    "# garber = garber[['edge_id','infr_6_', 'in_6___', 'osm_nm_sm',\n",
    "#        'rbbn_dt', 'geometry']]\n",
    "\n",
    "# #rename\n",
    "# garber.columns = ['id','Infra1','Infra2','Name','Ribbon Date','geometry']\n",
    "\n",
    "# #mark facilities that need to be dated\n",
    "# garber['need_date'] = (pd.to_datetime(garber['Ribbon Date']).apply(lambda row: row.year) >= 2012) | (garber['Ribbon Date'].isna())\n",
    "\n",
    "# # rename columns for consistency\n",
    "# garber.columns = ['garber_'+col.lower() if col != 'geometry' else 'geometry' for col in garber.columns.tolist()]\n",
    "\n",
    "# #export\n",
    "# garber.to_file(export_fp/'reference_layers.gpkg',layer='garber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# garber['garber_infra1'].append(garber['garber_infra2']).unique().tolist()\n",
    "# garber[garber['garber_infra1']=='off_street_trail_dirt'].explore()\n",
    "# cycleways_osm['facility_fwd'].append(cycleways_osm['facility_rev']).unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggested Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name check functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overwrite_check(overwrite_edit_version,confirm):\n",
    "    if (overwrite_edit_version == False) | (overwrite_edit_version == False):\n",
    "        print('Overwrite or confirm set to false')\n",
    "        return False\n",
    "    elif (overwrite_edit_version) & (confirm == True):\n",
    "        print('WARNING: OVERWRITE ENABLED')\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# List of common suffixes and their abbreviated forms\n",
    "suffixes = {\n",
    "    'street': ['st'],\n",
    "    'avenue': ['ave'],\n",
    "    'boulevard': ['blvd'],\n",
    "    'drive': ['dr'],\n",
    "    'lane': ['ln'],\n",
    "    'road': ['rd'],\n",
    "    'court': ['ct'],\n",
    "    'circle': ['cir'],\n",
    "    'way': ['wy'],\n",
    "    'place': ['pl']\n",
    "    # Add more suffixes and their abbreviations as needed\n",
    "}\n",
    "\n",
    "# Function to remove suffixes from street names\n",
    "def remove_suffix(street_name):\n",
    "    \n",
    "    if street_name is None:\n",
    "        return None\n",
    "\n",
    "    # Lowercase evertyhing\n",
    "    street_name = street_name.lower()\n",
    "\n",
    "    # remove periods\n",
    "    street_name = street_name.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "    # Split the street name into words\n",
    "    words = street_name.split()\n",
    "\n",
    "    # Remove directional indicators\n",
    "    directions = ['north', 'south', 'east', 'west', 'northeast', 'southeast', 'northwest', 'southwest', 'n', 'e', 's', 'w', 'ne', 'se', 'nw', 'sw','wb']\n",
    "    for direction in directions:\n",
    "        words = [word for word in words if word.lower() != direction]\n",
    "\n",
    "\n",
    "\n",
    "    # Remove suffixes\n",
    "    for suffix, abbreviations in suffixes.items():\n",
    "        # Remove full suffix\n",
    "        if words[-1].lower().endswith(suffix):\n",
    "            words[-1] = words[-1][:-(len(suffix))]\n",
    "        # Remove abbreviated suffix\n",
    "        for abbr in abbreviations:\n",
    "            if words[-1].lower().endswith(abbr):\n",
    "                words[-1] = words[-1][:-(len(abbr))]\n",
    "\n",
    "    # Reconstruct the street name with spaces\n",
    "    cleaned_street_name = ' '.join(words)\n",
    "    return cleaned_street_name.strip()  # Remove any leading or trailing whitespace\n",
    "\n",
    "# # Example list of street names\n",
    "# street_names = ['Berne St (WB)','Bill Kennedy','Main St', 'Elm Avenue', 'Maple Blvd', 'Oak Dr', 'Pine Ln', 'Northwest 1st St', 'ne 2nd Ave', 'Eagle Row']\n",
    "\n",
    "# # Remove suffixes from street names\n",
    "# cleaned_street_names = [remove_suffix(name) for name in street_names]\n",
    "\n",
    "# # Print cleaned street names\n",
    "# for name in cleaned_street_names:\n",
    "#     print(name)\n",
    "\n",
    "def name_check(name1,name2):\n",
    "    if (name1 is None) | (name2 is None):\n",
    "        return False\n",
    "    name1_words = name1.split()\n",
    "    name2_words = name2.split()\n",
    "    #check if any part of the name is right\n",
    "    check1 = [True for name1_word in name1_words if name1_word in name2_words]\n",
    "    #check2 = [True if name2_word in name1_words else False for name2_word in name2_words]\n",
    "    if len(check1) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def suggested_matches(cycleways_osm,other_source,other_name,buffer_ft,max_hausdorff_dist):\n",
    "    \n",
    "    cycleways_osm_buffered = cycleways_osm.copy()\n",
    "    other_source = other_source.copy()\n",
    "\n",
    "    # buffer the cycleways\n",
    "    cycleways_osm_buffered.geometry = cycleways_osm_buffered.buffer(buffer_ft)\n",
    "    \n",
    "    # intersect with coa (returns coa linestrings)\n",
    "    overlap = gpd.overlay(other_source,cycleways_osm_buffered)\n",
    "\n",
    "    # create new fields\n",
    "    overlap['accept_match'] = None\n",
    "    overlap['examined'] = None\n",
    "    overlap['notes'] = None # always good to have an extra field for notes\n",
    "    \n",
    "    #street name check if for bike lanes / sharrows / cycletracks\n",
    "    overlap['name'] = overlap['name'].apply(lambda row: remove_suffix(row))\n",
    "    overlap[f\"{other_name}_name\"] = overlap[f\"{other_name}_name\"].apply(lambda row: remove_suffix(row))\n",
    "    overlap['name_check'] = overlap.apply(lambda row: name_check(row['name'],row[f\"{other_name}_name\"]),axis=1)\n",
    "\n",
    "    ## AUTO REJECTS ##\n",
    "\n",
    "    # reject duplicate matches (same other feature by attributes but different id)\n",
    "    overlap.loc[overlap.drop(columns=[f\"{other_name}_id\",'geometry']).duplicated(),'accept_match'] = False\n",
    "\n",
    "    # reject if one facility is a mup or cycletrack and the other is a bike lane/sharrow\n",
    "    # cycletrack grouped with mups because they might get mis-classified as one\n",
    "    #TODO this might rmeove matches where there is a cycletrack on one side of the road but not the other?\n",
    "    off_street_infra = ['multi use path','cycletrack']\n",
    "    street_infra = ['bike lane','buffered bike lane','sharrow']\n",
    "    osm_street_infra = overlap[[\"facility_fwd\",'facility_rev']].isin(street_infra).any(axis=1)\n",
    "    other_street_infra = overlap[f\"{other_name}_osm_type\"].isin(street_infra)\n",
    "    osm_is_mup = (overlap[[\"facility_fwd\",'facility_rev']].isin(off_street_infra)).any(axis=1)\n",
    "    other_is_mup = overlap[f\"{other_name}_osm_type\"].isin(off_street_infra)\n",
    "    condition = (osm_street_infra & other_is_mup) | (other_street_infra & osm_is_mup)\n",
    "    overlap.loc[condition,'accept_match'] = False\n",
    "\n",
    "    # reject match if street infrastructure and name does not match\n",
    "    overlap.loc[osm_street_infra & other_street_infra & (overlap['name_check']==False),'accept_match'] = False\n",
    "\n",
    "    # reject if osm says sharrow for both directions and other is not a sharrow\n",
    "    overlap.loc[(overlap[['facility_fwd','facility_rev']] == 'sharrow').all(axis=1) & (overlap[f\"{other_name}_osm_type\"] != 'sharrow'),'accept_match'] = False\n",
    "    # and reject if other says sharrow but osm does not\n",
    "    overlap.loc[(overlap[['facility_fwd','facility_rev']] != 'sharrow').all(axis=1) & (overlap[f\"{other_name}_osm_type\"] == 'sharrow'),'accept_match'] = False\n",
    "\n",
    "    # mark the remaining one to many matches (don't count rejects)\n",
    "    only_one_na = overlap.groupby('osmid')['accept_match'].transform(lambda x: x.isna().sum()==1)\n",
    "    is_duplicated = overlap['osmid'].duplicated(keep=False)\n",
    "    overlap['one_to_many'] = ~(only_one_na | (is_duplicated == False))\n",
    "\n",
    "    ## AUTO ACCEPTS\n",
    "\n",
    "    # accept if name check is correct and it's one to one\n",
    "    overlap.loc[overlap['name_check'] & (overlap['one_to_many']==False) & overlap['accept_match'].isna(),'accept_match'] = True\n",
    "    # accept if both are multi-use paths and it's a one to one match\n",
    "    overlap.loc[osm_is_mup & other_is_mup & (overlap['one_to_many']==False) & overlap['accept_match'].isna(),'accept_match'] = True\n",
    "\n",
    "    # replace with osm geometry\n",
    "    overlap = pd.merge(overlap,cycleways_osm[['osmid','geometry']],on='osmid')\n",
    "    \n",
    "    overlap['hausdorff_dist'] = overlap.apply(lambda row: row['geometry_x'].hausdorff_distance(row['geometry_y']),axis=1)\n",
    "    overlap.drop(columns=['geometry_x'],inplace=True)\n",
    "    overlap.rename(columns={'geometry_y':'geometry'},inplace=True)\n",
    "    overlap = gpd.GeoDataFrame(overlap,geometry='geometry')\n",
    "\n",
    "    #for non-assigned values give it to the one with the min hausdorff distance as long as it doesn't exceed the maximum amount    \n",
    "    min_hausdorff = overlap[overlap['accept_match']!=0].groupby('osmid')['hausdorff_dist'].idxmin().tolist()\n",
    "    no_match = overlap.groupby('osmid')['accept_match'].transform(lambda x: (x != 1).all())\n",
    "    overlap.loc[overlap.index.isin(min_hausdorff) & (overlap['hausdorff_dist']<=max_hausdorff_dist) & no_match & overlap['accept_match'].isna(),'accept_match'] = True\n",
    "\n",
    "    #auto assign anything that wasn't accepted as false\n",
    "    match = overlap.groupby('osmid')['accept_match'].transform(lambda x: (x == 1).any())\n",
    "    overlap.loc[match & overlap['accept_match'].isna(),'accept_match'] = False\n",
    "\n",
    "    return overlap\n",
    "\n",
    "# from shapely.geometry import LineString, MultiLineString\n",
    "# #from shapely import hausdorff_distance\n",
    "\n",
    "# line1 = LineString([(0, 0), (1, 1), (2, 2)])\n",
    "# multiline = MultiLineString([[(0, 0), (1, 2)], [(2, 3), (3, 4)]])\n",
    "\n",
    "# distance = multiline.hausdorff_distance(line1)\n",
    "# print(\"Hausdorff Distance:\", distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_ft = 100\n",
    "max_hausdorff_dist_ft = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_osm = gpd.read_file(Path(config['project_directory'])/f\"Network/networks.gpkg\",layer='osm_links')\n",
    "\n",
    "# import the 2023 OSM bicycle network\n",
    "cycleways_osm = gpd.read_file(export_fp/'osm_cycleways.gpkg')\n",
    "cycleways_osm = cycleways_osm.loc[cycleways_osm['year']=='2023',['osmid','highway','name','facility_fwd','facility_rev','geometry']]\n",
    "\n",
    "# add cycleway info\n",
    "#cycleways_osm = pd.merge(network_osm,cycleways_osm,on='osmid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import processed versions\n",
    "coa = gpd.read_file(export_fp/'reference_layers.gpkg',layer='coa')\n",
    "arc = gpd.read_file(export_fp/'reference_layers.gpkg',layer='arc')\n",
    "#garber = gpd.read_file(export_fp/'reference_layers.gpkg',layer='garber')\n",
    "\n",
    "#perform overlap\n",
    "coa_overlap = suggested_matches(cycleways_osm,coa,'coa',buffer_ft,max_hausdorff_dist_ft)\n",
    "arc_overlap = suggested_matches(cycleways_osm,arc,'arc',buffer_ft,max_hausdorff_dist_ft)\n",
    "#garber_overlap = suggested_matches(cycleways_osm,garber,'garber_id','garber_name',buffer_ft)\n",
    "\n",
    "#export (only run once to prevent lost progress)\n",
    "overwrite = True\n",
    "if overwrite:\n",
    "    coa_overlap.to_file(export_fp/'suggested_matches.gpkg',layer='coa')\n",
    "    arc_overlap.to_file(export_fp/'suggested_matches.gpkg',layer='arc')\n",
    "    #garber_overlap.to_file(export_fp/'suggested_matches.gpkg',layer='garber')\n",
    "\n",
    "#update method (to prevent overwriting)\n",
    "#probably don't need this but use for adding new fields instead of overwriting the exiting one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Matches:')\n",
    "print('coa:',coa_overlap.shape[0],'arc:',arc_overlap.shape[0])\n",
    "print('Undecided:')\n",
    "print('coa:',coa_overlap['accept_match'].isna().sum(),'arc:',arc_overlap['accept_match'].isna().sum())\n",
    "print('Accept:')\n",
    "print('coa:',(coa_overlap['accept_match']==1).sum(),'arc:',(arc_overlap['accept_match']==1).sum())\n",
    "print('Reject:')\n",
    "print('coa:',(coa_overlap['accept_match']==0).sum(),'arc:',(arc_overlap['accept_match']==0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go through each in QGIS and confirm matches before running next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Infrastructure (ID version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the suggested infra\n",
    "coa_overlap = gpd.read_file(export_fp/'suggested_matches.gpkg',layer='coa')\n",
    "arc_overlap = gpd.read_file(export_fp/'suggested_matches.gpkg',layer='arc')\n",
    "#garber_overlap = gpd.read_file(export_fp/'suggested_matches.gpkg',layer='garber')\n",
    "\n",
    "#get ids of accepted matches\n",
    "suggested_coa_ids = coa_overlap.loc[coa_overlap['accept_match'] == '1','coa_id'].unique().tolist()\n",
    "suggested_arc_ids = arc_overlap.loc[arc_overlap['accept_match'] == '1','arc_id'].unique().tolist()\n",
    "#suggested_garber_ids = garber_overlap.loc[garber_overlap['accept_match'] == '1','garber_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import raw versions\n",
    "coa = gpd.read_file(export_fp/'reference_layers.gpkg',layer='coa')\n",
    "arc = gpd.read_file(export_fp/'reference_layers.gpkg',layer='arc')\n",
    "#garber = gpd.read_file(export_fp/'reference_layers.gpkg',layer='garber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what's not covered\n",
    "coa_inv = coa[coa['coa_id'].isin(suggested_coa_ids) == False].copy()\n",
    "arc_inv = arc[arc['arc_id'].isin(suggested_arc_ids) == False].copy()\n",
    "#garber_inv = garber[garber['garber_id'].isin(suggested_garber_ids) == False]\n",
    "print(coa_inv.shape[0],arc_inv.shape[0])#,garber_inv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "if overwrite:\n",
    "    coa_inv['included'] = None\n",
    "    coa_inv['suggested_osmid'] = None\n",
    "    coa_inv['notes'] = None\n",
    "    coa_inv.to_file(export_fp/'id_difference.gpkg',layer='coa')\n",
    "    \n",
    "    arc_inv['included'] = None\n",
    "    arc_inv['suggested_osmid'] = None\n",
    "    arc_inv['notes'] = None\n",
    "    arc_inv.to_file(export_fp/'id_difference.gpkg',layer='arc')\n",
    "\n",
    "    # garber_inv['included'] = None\n",
    "    # garber_inv['suggested_osmid'] = None\n",
    "    # garber_inv['notes'] = None\n",
    "    # garber_inv.to_file(export_fp/'id_difference.gpkg',layer='garber')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further manual processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add dates to OSM cycleways network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycleways_osm = gpd.read_file(export_fp/'osm_cycleways.gpkg')\n",
    "cycleways_osm = cycleways_osm.loc[cycleways_osm['year']=='2023',['osmid','highway','name','facility_fwd','facility_rev','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coa_overlap.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_cols = ['coa_id', 'coa_facilitytype', 'coa_builtby', 'coa_name',\n",
    "#        'coa_onroad', 'coa_bothside', 'coa_year', 'coa_need_date',\n",
    "#        'coa_osm_type', 'osmid']\n",
    "accepted_coa = coa_overlap.loc[coa_overlap['accept_match']=='1',['osmid','coa_year']]\n",
    "cycleways_osm_coa = pd.merge(cycleways_osm,accepted_coa,on='osmid',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_cols = ['arc_id', 'arc_name', 'arc_spec', 'arc_width', 'arc_material',\n",
    "#        'arc_year', 'arc_need_date', 'arc_osm_type', 'osmid']\n",
    "accepted_arc = arc_overlap.loc[arc_overlap['accept_match']=='1',['osmid','arc_year']]\n",
    "cycleways_osm_coa_arc = pd.merge(cycleways_osm_coa,accepted_arc,on='osmid',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycleways_osm_coa_arc[cycleways_osm_coa_arc[['arc_year','coa_year']].notnull().any(axis=1)].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#if both coa and arc date, take the coa date, otherwise arc\n",
    "def date_fix(row):\n",
    "    if row['coa_year'] != row['arc_year']:\n",
    "        return row['coa_year']\n",
    "    else:\n",
    "        return row['arc_year']\n",
    "cycleways_osm_coa_arc['year'] = cycleways_osm_coa_arc.apply(lambda row: date_fix(row),axis=1)\n",
    "cycleways_osm_coa_arc.loc[cycleways_osm_coa_arc['year'].isna(),'year'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the osm earliest appearance, if it's post 2020 then it definitely wasn't in the study area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Set certain facilities to year=0 if it's not a bike lane or PATH trail (i.e., facilities not present in the newer data sources)\n",
    "cycleways_osm_coa_arc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycleways_osm_coa_arc['min_date'] = cycleways_osm_coa_arc[['arc_year','coa_year']].min(axis=1)\n",
    "# cycleways_osm_coa_arc['max_date'] = cycleways_osm_coa_arc[['arc_year','coa_year']].max(axis=1)\n",
    "\n",
    "# #set null value\n",
    "# cycleways_osm_coa_arc.loc[cycleways_osm_coa_arc['min_date'].isna(),'min_date'] = 0\n",
    "# cycleways_osm_coa_arc.loc[cycleways_osm_coa_arc['max_date'].isna(),'max_date'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycleways_osm_coa_arc.to_file(export_fp/'osm_cycleways_w_dates.gpkg',layer='test_dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <!-- # Missing Infrastructure (Geometry Version)\n",
    "# buffer_ft = 100\n",
    "# overwrite_diff = False\n",
    "# confirm_diff = False\n",
    "# # import the 2023 OSM bicycle network\n",
    "# cycleways_osm = gpd.read_file(export_fp/'osm_cycleways.gpkg')\n",
    "# cycleways_osm = cycleways_osm.loc[cycleways_osm['year']=='2023']\n",
    "\n",
    "# # get unary union of all features after buffering\n",
    "# cycleways_osm_all = cycleways_osm.buffer(buffer_ft).unary_union\n",
    "# arc_diff = arc[arc.geometry.intersects(cycleways_osm_all) == False]\n",
    "# coa_diff = coa[coa.geometry.intersects(cycleways_osm_all) == False]\n",
    "# garber_diff = garber[garber.geometry.intersects(cycleways_osm_all) == False]\n",
    "# overwrite = False\n",
    "# if overwrite:\n",
    "#     coa_diff['valid_difference'] = None\n",
    "#     coa_diff['notes'] = None\n",
    "#     coa_diff.to_file(export_fp/'differences.gpkg',layer='coa')\n",
    "    \n",
    "#     arc_diff['valid_difference'] = None\n",
    "#     arc_diff['notes'] = None\n",
    "#     arc_diff.to_file(export_fp/'differences.gpkg',layer='arc')\n",
    "\n",
    "#     garber_diff['valid_difference'] = None\n",
    "#     garber_diff['notes'] = None\n",
    "#     garber_diff.to_file(export_fp/'differences.gpkg',layer='garber')\n",
    "\n",
    "# raw_osm = gpd.read_file(Path(config['project_directory'])/f\"OSM_Download/osm_{config['geofabrik_year']}.gpkg\",layer='raw')\n",
    "# raw_osm.to_crs(config['projected_crs_epsg'],inplace=True)\n",
    "# final_confirm = False\n",
    "# if (overwrite_check(overwrite_diff,confirm_diff) == True) & (final_confirm == True):\n",
    "#     raw_osm['arc_feature_id'] = None\n",
    "#     raw_osm['coa_feature_id'] = None\n",
    "#     raw_osm['garber_feature_id'] = None\n",
    "#     raw_osm.to_file(export_fp/'differences.gpkg',layer='osm_edit')\n",
    "# # raw_osm = gpd.read_file(Path(config['project_directory'])/f\"OSM_Download/osm_{config['geofabrik_year']}.gpkg\",layer='raw')\n",
    "# # raw_osm.to_crs(config['projected_crs_epsg'],inplace=True)\n",
    "# # final_confirm = True\n",
    "# # if (overwrite_check(overwrite_diff,confirm_diff) == True) & (final_confirm == True):\n",
    "# #     raw_osm['arc_feature_id'] = None\n",
    "# #     raw_osm['coa_feature_id'] = None\n",
    "# #     raw_osm['garber_feature_id'] = None\n",
    "# #     raw_osm.to_file(export_fp/'differences.gpkg',layer='osm_edit')\n",
    "# # #based on the 2023-01-01 Geofabrik Georgia Extract\n",
    "# # #osm = gpd.read_file(Path(config['project_directory'])/'Network/networks.gpkg',layer='osm_links')\n",
    "# # osm = gpd.read_file(Path(config['project_directory'])/f\"OSM_Download/osm_2023.gpkg\",layer='raw',ignore_geometry=True)\n",
    "# # #osm = pd.merge(osm,raw_osm,on='osmid',how='left')\n",
    "\n",
    "# # #create new fields for install dates\n",
    "# # osm['install_year'] = None\n",
    "# # osm['install_month'] = None\n",
    "# # osm['install_day'] = None\n",
    "\n",
    "# # #create new fields for updated fwd and rev infra types\n",
    "# # osm['facility_fwd'] = None\n",
    "# # osm['facility_rev'] = None\n",
    "\n",
    "# # #create field for notes\n",
    "# # osm['notes'] = None\n",
    "\n",
    "# # #create field for edit date\n",
    "# # osm['last_edited'] = None -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
