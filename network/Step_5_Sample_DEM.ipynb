{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Adding Elevation Data from USGS to OSM\n",
    "---\n",
    "Method based on [Liu et al. 2018](https://doi.org/10.1016/j.trc.2018.05.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import contextily as cx\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.plot import show\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import box, mapping\n",
    "from shapely.ops import Point\n",
    "from tqdm import tqdm\n",
    "import pyproj\n",
    "import math\n",
    "import json\n",
    "from shapely.ops import LineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load((Path.cwd().parent / 'config.json').open('rb'))\n",
    "network_fp = Path(config['project_directory']) / 'OSM_Download'\n",
    "export_fp = Path(config['project_directory']) / 'Network'\n",
    "if network_fp.exists() == False:\n",
    "    network_fp.mkdir()\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get TIFF download links from [USGS.gov](https://apps.nationalmap.gov/downloader/#/?z=4&y=37.99999999999999&x=-95&basemap=usgs_topo&datasets=&layerIds=)\n",
    "It accepts polygon drawing, shapefiles, kmls, etc. Select elevation products (3DEP) and then the 1 m DEM option. Then search for products. It will give the option to download a text/csv file with links to the TIFF. The download_dems.py script can be used to download the DEM files, or these links can be used to read the files in without saving them to hard drive (takes ~9gb for itp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO script the download process to take in a study area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Network to Add Elevation to\n",
    "- Option 1: Import the non-network form OSM (NEW)\n",
    "- Option 2: Import the network version of OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links = gpd.read_file(export_fp / \"networks.gpkg\",layer=\"osm_links\")\n",
    "# #set the osmid as the index\n",
    "# links.set_index('osm_linkid',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = gpd.read_file(network_fp / f\"osm_{config['geofabrik_year']}.gpkg\",layer=\"raw\")\n",
    "#set the osmid as the index\n",
    "links.set_index('osmid',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Network to Same CRS as TIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read the list of raster links needed for 1m resolution elevation data\n",
    "# tiff_links = open(Path.home()/'Documents/GitHub/BikewaySimDev/add_elevation_data/dem_links.txt','r')\n",
    "# tiff_links = [text.split('\\n')[0] for text in tiff_links.readlines()]\n",
    "tiff_links = list((Path(config['usgs']) / 'dem_files').glob('*.tif'))\n",
    "\n",
    "#set the sampling distance (default is 10 meters)\n",
    "interpolate_dist_m = 10\n",
    "#open the first one to just get the crs\n",
    "src = rasterio.open(tiff_links[0])\n",
    "dem_crs = src.crs\n",
    "\n",
    "#make sure links are in the same crs as tif (I'm not sure how to do coordinate conversions with rasterio yet)\n",
    "links.to_crs(src.crs,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How long are the edges?\n",
    "Some of the links are just going to be too short to add elevation to in a meaningful way. For these, try using a shorter interval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#links['osm_url'] = links.loc[~links['osmid'].isna(),'osmid'].apply(lambda row: 'https://www.openstreetmap.org/way/' + str(int(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links['length_m'] = links.length\n",
    "links[links['length_m'] < 2000].hist(column='length_m',bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_frequency = 10 # in meters\n",
    "print((links.length < sampling_frequency).sum(),'links of',links.shape[0] ,'total were less than',sampling_frequency,'meters')\n",
    "print(links[links.length<sampling_frequency].length.sum().round(0),'of',links.length.sum().round(0),'meters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links[links.length<sampling_frequency]['highway'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#links.loc[(links.length < 10) & (links['highway']!='steps'),['highway','name','geometry']].explore(tooltip=False,popup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to plot tiff files for examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting code to examine a tiff file\n",
    "raster_data = src.read(1)\n",
    "na_value = -999999.0\n",
    "raster_data[raster_data == na_value] = np.nan\n",
    "\n",
    "# Plot the raster\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Raster Plot\")\n",
    "show(raster_data, transform=src.transform, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate points on each line in the dataset starting from the begining and always ending before the last point. Change \"interpolate_dist_m\" to change the interpolation distance (default is 10 meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_points_dict = {}\n",
    "\n",
    "#takes around 47 seconds for network the size of ITP\n",
    "for index, row in tqdm(links.iterrows(),total=links.shape[0]):\n",
    "    \n",
    "    line = row.geometry\n",
    "\n",
    "    interpolated_points = []\n",
    "    interpolated_distances = []\n",
    "\n",
    "    #start with interpolate_dist_m and add +interpolate_dist_m until current_dist_m is longer than the line\n",
    "    current_dist_m = interpolate_dist_m\n",
    "\n",
    "    while current_dist_m < line.length:\n",
    "        interpolated_point = line.interpolate(current_dist_m)\n",
    "        interpolated_points += [mapping(interpolated_point)['coordinates']]\n",
    "        interpolated_distances.append(current_dist_m)\n",
    "        current_dist_m += interpolate_dist_m\n",
    "\n",
    "    coords = line.coords\n",
    "    first_point = coords[0]\n",
    "    last_point = coords[-1]\n",
    "    \n",
    "    # only use first and last point if the line is less than interpolate_dist_m\n",
    "    if line.length <= interpolate_dist_m:\n",
    "        interpolated_points = [first_point,last_point]\n",
    "        distances = [0,line.length]\n",
    "    else:\n",
    "        interpolated_points = [first_point] + interpolated_points #+ [last_point]\n",
    "        distances = [0] + interpolated_distances #+ [line.length]\n",
    "    \n",
    "    #TODO maybe just store these all as one array?\n",
    "    interpolated_points_dict[index] = {\n",
    "        'geometry': np.array(interpolated_points),\n",
    "        'distances': np.array(distances),\n",
    "        # create array of nan values to fill with elevation (m) sampled from the tiff using nanmax\n",
    "        #UPDATE nanmax doesn't like two np.nans so using -999 instead\n",
    "        'elevations': np.array([-999 for x in range(0,len(interpolated_points))])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open each tiff file in the .txt and sample values to replace the nan values on the interpolated points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#took 17-25 mins to run for ITP area\n",
    "for tiff_link in tqdm(tiff_links):\n",
    "    #open the raster using the link\n",
    "    src = rasterio.open(tiff_link)\n",
    "\n",
    "    #find all links that intersect with the current raster\n",
    "    xmin, ymin, xmax, ymax = src.bounds\n",
    "    bbox = box(xmin,ymin,xmax,ymax)\n",
    "    intersection = links.intersects(bbox)\n",
    "\n",
    "    if (intersection == True).any():\n",
    "        #print('intersection detected')\n",
    "        for index in links[intersection].index:\n",
    "            dict_values = interpolated_points_dict.get(index)\n",
    "\n",
    "            sampled = np.array([val[0] for val in src.sample(dict_values['geometry'])])\n",
    "            \n",
    "            # deal with na values (because the default nan value is -99999)\n",
    "            sampled[sampled < 0] = np.nan\n",
    "\n",
    "            #if at least one non-null value\n",
    "            if np.isnan(sampled).all() == False:\n",
    "                interpolated_points_dict[index]['elevations'] = np.nanmax([sampled,dict_values['elevations']],axis=0).round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for storing the interpolated points with sampled elevation data\n",
    "import pickle\n",
    "with (export_fp/'elevation.pkl').open('wb') as fh:\n",
    "    pickle.dump(interpolated_points_dict,fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move on to Step 6 (Below code is test code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating grade from sampled elevation\n",
    "# #for storing the interpolated points with sampled elevation data\n",
    "# import pickle\n",
    "# with (network_filepath.parent/'elevation.pkl').open('rb') as fh:\n",
    "#     interpolated_points_dict = pickle.load(fh)\n",
    "# Calculate average up-grade and down-grade for each link using the interpolated points with sampled elevations\n",
    "# # all_rise = {}\n",
    "# # all_down = {}\n",
    "# # max_rise = {}\n",
    "# # min_rise = {}\n",
    "# grade_threshold = 15\n",
    "\n",
    "# for key, item in tqdm(interpolated_points_dict.items(),total=len(interpolated_points_dict)):     \n",
    "\n",
    "#     #find the total distance to get average grade\n",
    "#     total_distance = np.array(item['distances']).max()\n",
    "    \n",
    "#     #caluclate the elevation change between points and add to list\n",
    "#     elevation_deltas = []\n",
    "#     grades = []\n",
    "#     bad_grades = []\n",
    "#     for x in range(0,len(item['elevations'])-1):        \n",
    "#         elev1 = item['elevations'][x]\n",
    "#         elev2 = item['elevations'][x+1]\n",
    "#         dist1 = item['distances'][x]\n",
    "#         dist2 = item['distances'][x+1]\n",
    "#         elevation_delta = (elev2-elev1)\n",
    "#         elevation_deltas.append(elevation_delta)\n",
    "#         distance_delta = (dist2-dist1)\n",
    "\n",
    "#         segment_grade = elevation_delta / distance_delta * 100\n",
    "#         grades.append(segment_grade)\n",
    "\n",
    "#         #flag potentially bad elevation points\n",
    "#         if np.abs(segment_grade) >= grade_threshold:\n",
    "#             bad_grades.append(x)\n",
    "\n",
    "#     #get total up and down\n",
    "#     elevation_deltas = np.asarray(elevation_deltas)\n",
    "#     up = elevation_deltas[elevation_deltas>0].sum()\n",
    "#     down = np.absolute(elevation_deltas[elevation_deltas<0].sum()) #take absolute value at this point\n",
    "\n",
    "#     #get average up and down grade\n",
    "#     up_grade = up / total_distance * 100\n",
    "#     down_grade = down / total_distance * 100\n",
    "    \n",
    "#     interpolated_points_dict[key].update(\n",
    "#         {\n",
    "#             'up': up,\n",
    "#             'down': down, \n",
    "#             'up_grade': up_grade,\n",
    "#             'down_grade': down_grade,\n",
    "#             'elevation_deltas': elevation_deltas,\n",
    "#             'grade_segments': grades,\n",
    "#             'bad_grades': bad_grades\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "# # Tasks\n",
    "# - We have a good way to plot/examine\n",
    "# - We just need to clean the rest of the data\n",
    "# - We have taken out underpasses, so now we should look at bridges and sample lidar stuff and then examine the results\n",
    "# - Once that is done, smooth everything and caclulate the different variables\n",
    "# # Plotting\n",
    "# We want to better visualize the problem and identify the different types of situations:\n",
    "# - Vertical profile with bad grades highlighed\n",
    "#     - Different colors for up/down (arrows?)\n",
    "#     - Ignore vertical exageration for now\n",
    "# - Grade profile just showing the grades with bad ones highlighted (maybe even a dual axis graph)\n",
    "# - Static map of where the link is + bad grades highlighted\n",
    "# - Really, we want to export all the bad figures with a link to the osm way in the title\n",
    "\n",
    "\n",
    "# # use west paces ferry to start with\n",
    "# #linkid = 42901\n",
    "\n",
    "# fig, (ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(12,3))\n",
    "\n",
    "# # for linkid, item in tqdm(interpolated_points_dict.items()):\n",
    "\n",
    "# #     if len(item['bad_grades']) == 0:\n",
    "# #         continue\n",
    "\n",
    "# #     if len(item['distances']) <= 2:\n",
    "# #         continue\n",
    "\n",
    "# cond = True\n",
    "# while cond == True:\n",
    "#     linkid = links['linkid'].sample(1).item()\n",
    "#     item = interpolated_points_dict[linkid]\n",
    "#     if (len(item['bad_grades']) > 0) & (len(item['distances']) > 2):\n",
    "#         cond = False\n",
    "    \n",
    "\n",
    "# #https://github.com/geopandas/geopandas/issues/2279\n",
    "# #https://stackoverflow.com/questions/8247973/how-do-i-specify-an-arrow-like-linestyle-in-matplotlib\n",
    "\n",
    "# #Extract values\n",
    "# x = np.array(item['distances'])\n",
    "# y = np.array(item['elevations'])\n",
    "\n",
    "# #gives you a boolean mask\n",
    "# bad_up_grades = np.array(item['grade_segments']) >= grade_threshold\n",
    "# bad_down_grades = np.array(item['grade_segments']) <= -grade_threshold\n",
    "\n",
    "# #add an additional value\n",
    "# bad_up_grades = np.hstack([np.array((False)),bad_up_grades])\n",
    "# bad_down_grades = np.hstack([np.array((False)),bad_down_grades])\n",
    "\n",
    "# bad_up_x = x[bad_up_grades]\n",
    "# bad_up_y = y[bad_up_grades]\n",
    "\n",
    "# bad_down_x = x[bad_down_grades]\n",
    "# bad_down_y = y[bad_down_grades]\n",
    "\n",
    "# #First Figure\n",
    "# # Plot the original data and the smoothed curve\n",
    "# ax1.plot(x, y, 'o', label='Original Data')\n",
    "# #ax1.plot(x, new_y, 'x',label='Resampled with Lidar')\n",
    "# ax1.grid(True,linestyle='-.')\n",
    "\n",
    "# # plot bad points\n",
    "# ax1.plot(bad_up_x,bad_up_y,'o',color='green',label=f'Above {grade_threshold}%')\n",
    "# ax1.plot(bad_down_x,bad_down_y,'o',color='red',label=f'Below -{grade_threshold}%')\n",
    "\n",
    "# #ax1.set_title(f'{road_name} (Linkid:{linkid}) Veritcal Cross-Section')\n",
    "# ax1.set_xlabel('Distance (m)')\n",
    "# ax1.set_ylabel('Elevation (m)')\n",
    "# ax1.legend()\n",
    "\n",
    "# #Second and Third figure\n",
    "\n",
    "# #create geos to plot\n",
    "# points = np.array([Point(x,y) for x,y in item['geometry']])\n",
    "# line = LineString(item['geometry'])\n",
    "# minx, miny, maxx, maxy = line.bounds\n",
    "# # line_gdf = gpd.GeoDataFrame({'geometry':line},crs=dem_crs,index=[0])\n",
    "\n",
    "# # use mask to just get bad one\n",
    "# bad_up_grades_points = points[bad_up_grades]\n",
    "# bad_up_grades_points = gpd.GeoDataFrame({'geometry':bad_up_grades_points},geometry='geometry',crs=dem_crs)\n",
    "# bad_up_grades_points['type'] = '> 15%'\n",
    "\n",
    "# bad_down_grades_points = points[bad_down_grades]\n",
    "# bad_down_grades_points = gpd.GeoDataFrame({'geometry':bad_down_grades_points},geometry='geometry',crs=dem_crs)\n",
    "# bad_down_grades_points['type'] = '< -15%'\n",
    "\n",
    "# bad_grades_points = pd.concat([bad_up_grades_points,bad_down_grades_points])\n",
    "\n",
    "\n",
    "# color_dict = {\n",
    "#     '> 15%': 'green',\n",
    "#     '< -15%': 'red',\n",
    "# }\n",
    "# bad_grades_points['color'] = bad_grades_points['type'].map(color_dict)\n",
    "\n",
    "# bad_grades_points.plot(ax=ax2,color=bad_grades_points['color'],zorder=4)\n",
    "# bad_grades_points.plot(ax=ax3,color=bad_grades_points['color'],zorder=4)\n",
    "\n",
    "# #for drawing the link\n",
    "# x = np.array([x for x, y in item['geometry']])\n",
    "# y = np.array([y for x, y in item['geometry']])\n",
    "# # length of line segment\n",
    "# ds=10\n",
    "# # number of line segments per interval\n",
    "# Ns = np.round(np.sqrt( (x[1:]-x[:-1])**2 + (y[1:]-y[:-1])**2 ) / ds).astype(int)\n",
    "# # sub-divide intervals w.r.t. Ns\n",
    "# subdiv = lambda x, Ns=Ns: np.concatenate([ np.linspace(x[ii], x[ii+1], Ns[ii]) for ii, _ in enumerate(x[:-1]) ])\n",
    "# x, y = subdiv(x), subdiv(y)\n",
    "# ax2.quiver(x[:-1], y[:-1], x[1:]-x[:-1], y[1:]-y[:-1], scale_units='xy', angles='xy', scale=1, width=.004, headlength=4, headwidth=4)\n",
    "# ax3.quiver(x[:-1], y[:-1], x[1:]-x[:-1], y[1:]-y[:-1], scale_units='xy', angles='xy', scale=1, width=.004, headlength=4, headwidth=4)\n",
    "\n",
    "# #If you want a legend for map (didnot figure out arrow in the legend)\n",
    "# # from matplotlib.lines import Line2D\n",
    "# # from matplotlib.patches import Arrow\n",
    "# # custom_points = [Line2D([0], [0], marker=\"o\", linestyle=\"none\", markersize=10, color=color) for color in color_dict.values()]\n",
    "# # #custom_line = Arrow(0,0,dx=0.1,dy=0,width=0.1,linewidth=0.5,color='black')\n",
    "# # #custom_points.append()\n",
    "# # leg_points = ax2.legend(custom_points, ['> 15%','< -15%'])#, title = 'Legend', alignment='right')\n",
    "# # ax2.add_artist(leg_points)\n",
    "\n",
    "# #make sure fig is square\n",
    "# padding = 100\n",
    "# x_diff = np.abs(maxx - minx)\n",
    "# y_diff = np.abs(maxy - miny)\n",
    "# diff = (x_diff - y_diff)/2\n",
    "# if diff > 0:\n",
    "#     ax2.set_xlim(minx-padding,maxx+padding)\n",
    "#     ax2.set_ylim(miny-padding-diff,maxy+padding+diff)\n",
    "\n",
    "#     ax3.set_xlim(minx-padding,maxx+padding)\n",
    "#     ax3.set_ylim(miny-padding-diff,maxy+padding+diff)\n",
    "# else:\n",
    "#     ax2.set_xlim(minx-padding-np.abs(diff),maxx+padding+np.abs(diff))\n",
    "#     ax2.set_ylim(miny-padding,maxy+padding)\n",
    "\n",
    "#     ax3.set_xlim(minx-padding-np.abs(diff),maxx+padding+np.abs(diff))\n",
    "#     ax3.set_ylim(miny-padding,maxy+padding)\n",
    "\n",
    "# ax2.set_axis_off()\n",
    "# ax3.set_axis_off()\n",
    "\n",
    "# cx.add_basemap(ax2,zoom=17,source=cx.providers.MapTiler.Satellite(key=maptilerapikey),crs=bad_grades_points.crs,alpha=0.5)\n",
    "# cx.add_basemap(ax3,zoom=16,source=cx.providers.MapTiler.Streets(key=maptilerapikey),crs=bad_grades_points.crs)\n",
    "\n",
    "# #maybe if we wanted a high res version of this later\n",
    "# #https://stackoverflow.com/questions/42483449/mapbox-gl-js-export-map-to-png-or-pdf\n",
    "\n",
    "# name = links.loc[links['linkid']==linkid,'name'].item()\n",
    "# plt.suptitle(f'{name} ({linkid}) vertical profile')\n",
    "# plt.show()\n",
    "\n",
    "# # try: \n",
    "# #     plt.savefig(Path(f'D:/bad_grades/{name}_{linkid}.png'),dpi=300)\n",
    "# # except:\n",
    "# #     'invalid filename'\n",
    "\n",
    "# # #clear the axis\n",
    "# # ax1.cla()\n",
    "# # ax2.cla()\n",
    "# # ax3.cla()\n",
    "\n",
    "# from scipy.interpolate import splrep, splev\n",
    "\n",
    "# #pick a random one to examine\n",
    "# #linkid = int(abnormal_links.loc[new,'linkid'].sample(1).item())\n",
    "# linkid = links.loc[still_bad,'linkid'].sample(1).item()\n",
    "# #linkid = 53107\n",
    "\n",
    "# #linkid = 28303#44061\n",
    "# item = interpolated_points_dict[linkid]\n",
    "\n",
    "# #title\n",
    "# road_name = links.loc[links.index==linkid,'name'].item()\n",
    "\n",
    "# #ax1 elements\n",
    "# x = np.array(item['distances'])\n",
    "# y = np.array(item['elevations'])\n",
    "# new_y = np.array(lidar_sampled[linkid])\n",
    "\n",
    "# #ax2 elements\n",
    "# grade_x = np.array(range(0,len(x) - 1))\n",
    "# grade_delta = np.diff(y) / 10 * 100\n",
    "# lidar_delta = np.diff(new_y) / 10 * 100\n",
    "\n",
    "# fig, (ax1,ax2) = plt.subplots(ncols=2,figsize=(9,3))\n",
    "\n",
    "# # Plot the original data and the smoothed curve\n",
    "# ax1.plot(x, y, 'o', label='Original Data')\n",
    "# ax1.plot(x, new_y, 'x',label='Resampled with Lidar')\n",
    "# ax1.grid(True,linestyle='-.')\n",
    "\n",
    "# #find min\n",
    "# y_min = np.min([y.min(),new_y.min()])\n",
    "# y_max = np.max([y.max(),new_y.max()])\n",
    "\n",
    "# ax1.set_xlim(0,np.ceil(x.max() / 10) * 10)\n",
    "# ax1.set_ylim(np.floor(y_min/10)*10,np.ceil(y_max/10)*10)\n",
    "# ax1.set_box_aspect(1)\n",
    "\n",
    "# #ax1.set_title(f'{road_name} (Linkid:{linkid}) Veritcal Cross-Section')\n",
    "# ax1.set_xlabel('Distance (m)')\n",
    "# ax1.set_ylabel('Elevation (m)')\n",
    "# ax1.legend()\n",
    "\n",
    "# ax2.plot(grade_x, grade_delta, 'o', label='Original Grade')\n",
    "# ax2.plot(grade_x, lidar_delta, 'x', label='lidar grade')\n",
    "\n",
    "\n",
    "# ax2.set_xlabel('Distance (m)')\n",
    "# ax2.set_ylabel('Grade %')\n",
    "# ax2.set_box_aspect(1)\n",
    "# ax2.legend()\n",
    "\n",
    "# fig.suptitle(f'{road_name} (Linkid:{linkid})')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # Set ticks\n",
    "# #plt.xlim(0,x.max())\n",
    "# #plt.ylim()\n",
    "\n",
    "# # Plot the dropped points as 'x's\n",
    "# #plt.scatter(x_dropped, y_dropped, marker='x', color='red', label='Dropped Points')\n",
    "\n",
    "\n",
    "# #turn item into a dataframe\n",
    "# #df = pd.DataFrame.from_dict(item,orient='columns')\n",
    "\n",
    "# # df['elev_change'] = df['elevations'].diff()\n",
    "# # df['dist_change'] = df['distances'].diff()\n",
    "# # df['change_rate'] = np.abs(df['elev_change'] / df['dist_change'])\n",
    "\n",
    "#     total_down =  \n",
    "#     all_rise[key] = total_rise\n",
    "#     all_down[key] = total_down\n",
    "\n",
    "#     if total_rise > 0:\n",
    "#         max_rise[key] = elevation_deltas[elevation_deltas>0].max()\n",
    "#     else:\n",
    "#         max_rise[key] = 0\n",
    "\n",
    "#     if total_down > 0:\n",
    "#         min_rise[key] = np.absolute(elevation_deltas[elevation_deltas<0].min())\n",
    "#     else:\n",
    "#         min_rise[key] = 0\n",
    "\n",
    "# #forward direction\n",
    "# links['rise_m'] = pd.Series(all_rise).round(3)\n",
    "# links['maxrise_m'] = pd.Series(max_rise).round(3)\n",
    "# links['up_grade'] = (links['rise_m'] / links.geometry.length * 100).round(1)\n",
    "# links.loc[links.length > interpolate_dist_m,'max_grade'] = (links['maxrise_m'] / interpolate_dist_m * 100).round(1)\n",
    "# links.loc[links['max_grade'].isna(),'max_grade'] = (links['maxrise_m'] / links.length * 100).round(1)\n",
    "\n",
    "# #reverse direction\n",
    "# links['down_m'] = pd.Series(all_down).round(3)\n",
    "# links['minrise_m'] = pd.Series(min_rise).round(3)\n",
    "# links['down_grade'] = (links['down_m'] / links.geometry.length * 100).round(1)\n",
    "# links.loc[links.length > interpolate_dist_m,'min_grade'] = (links['minrise_m'] / interpolate_dist_m * 100).round(1)\n",
    "# links.loc[links['min_grade'].isna(),'min_grade'] = (links['minrise_m'] / links.length * 100).round(1)\n",
    "# #reproject data and export\n",
    "# #links.to_crs('epsg:2240',inplace=True)\n",
    "# #links.to_crs(prev_crs).reset_index(drop=True).to_file(project_dir/'reconciled_network.gpkg',layer='links_w_signals_elevation')\n",
    "# # Sample bridge elevations using LIDAR data\n",
    "# Bridges and tunnels will have inaccurate grades becuase they don't follow the terrain. DEMs represent the lowest point possible so that they show the underlying natural terrain. Raw LIDAR data will have points representing builidngs, trees, and other things on the earth's surface.\n",
    "\n",
    "# The download lidar script will download .LAZ files for the study area (LIDAR data is also available from the USGS site). These are then converted into TIF files using the lidar notebook. Once the TIF files have been made, the following code blocks sample new data that replaces previous elevation samples if the new sampled elevation is greater than the existing.\n",
    "\n",
    "# A new column is created for the link geodataframe to examine where the newly sampled points increased/decreased the accuracy of the elevation data. The assumption is that more accurate data should reduce the number of extreme grade changes and by extension the average grade. It may not do that in all cases, so the next step will be to smooth the data.\n",
    "# linkids = []\n",
    "# for key, item in interpolated_points_dict.items():\n",
    "#     if len(item['bad_grades']) > 0:\n",
    "#         linkids.append(key)\n",
    "# print(len(linkids),'exceed the threshold')\n",
    "# exceeds_threshold = links.loc[linkids]\n",
    "# #grab ones that are not bridges and don't exceed the threshold\n",
    "# not_bridges = exceeds_threshold.loc[exceeds_threshold['bridge'].isna() ,['linkid','geometry']]\n",
    "# print(len(not_bridges),'are not bridges')\n",
    "# ### Find links that cross bridges, railroads, or creeks/rivers (likely an underpass)\n",
    "# #grab bridges\n",
    "# raw_osm = gpd.read_file(network_filepath/'filtered.gpkg',layer='osm_links')\n",
    "# osm_attr = pd.read_pickle(network_filepath/'osm_attr.pkl')\n",
    "# merged = raw_osm.merge(osm_attr,on='osm_linkid')\n",
    "# bridges = merged.loc[~merged['bridge'].isna(),['osm_linkid','geometry']]\n",
    "# bridges.to_crs(links.crs,inplace=True)\n",
    "# #check if bridge crosses non-bridge\n",
    "# crossing_links = []\n",
    "# for idx, row in bridges.iterrows():\n",
    "#     bridge = row['geometry']\n",
    "#     crosses = not_bridges.loc[not_bridges.crosses(bridge),'linkid'].tolist()\n",
    "#     if len(crosses) > 0:\n",
    "#         crossing_links = crossing_links + crosses\n",
    "# mask = list(set(crossing_links))\n",
    "# not_bridges.loc[mask].reset_index(drop=True).explore()\n",
    "# intersection = gpd.overlay(bridges,not_bridges,keep_geom_type=False)\n",
    "# intersection.explore()\n",
    "# ### Examine cross sections for these\n",
    "# intersection['linkid'].nunique()\n",
    "# len(mask)\n",
    "# import contextily as cx\n",
    "\n",
    "# # use west paces ferry to start with\n",
    "# #linkid = 42901\n",
    "\n",
    "# fig, (ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(12,3))\n",
    "\n",
    "# # for linkid, item in tqdm(interpolated_points_dict.items()):\n",
    "\n",
    "# #     if len(item['bad_grades']) == 0:\n",
    "# #         continue\n",
    "\n",
    "# #     if len(item['distances']) <= 2:\n",
    "# #         continue\n",
    "\n",
    "# linkid = not_bridges.loc[mask,'linkid'].sample(1).item()#intersection['linkid'].sample(1).item()\n",
    "# item = interpolated_points_dict[linkid]\n",
    "\n",
    "# #https://github.com/geopandas/geopandas/issues/2279\n",
    "# #https://stackoverflow.com/questions/8247973/how-do-i-specify-an-arrow-like-linestyle-in-matplotlib\n",
    "\n",
    "# #Extract values\n",
    "# x = np.array(item['distances'])\n",
    "# y = np.array(item['elevations'])\n",
    "\n",
    "# #gives you a boolean mask\n",
    "# bad_up_grades = np.array(item['grade_segments']) >= grade_threshold\n",
    "# bad_down_grades = np.array(item['grade_segments']) <= -grade_threshold\n",
    "\n",
    "# #add an additional value\n",
    "# bad_up_grades = np.hstack([np.array((False)),bad_up_grades])\n",
    "# bad_down_grades = np.hstack([np.array((False)),bad_down_grades])\n",
    "\n",
    "# bad_up_x = x[bad_up_grades]\n",
    "# bad_up_y = y[bad_up_grades]\n",
    "\n",
    "# bad_down_x = x[bad_down_grades]\n",
    "# bad_down_y = y[bad_down_grades]\n",
    "\n",
    "# #First Figure\n",
    "# # Plot the original data and the smoothed curve\n",
    "# ax1.plot(x, y, 'o', label='Original Data')\n",
    "# #ax1.plot(x, new_y, 'x',label='Resampled with Lidar')\n",
    "# ax1.grid(True,linestyle='-.')\n",
    "\n",
    "# # plot bad points\n",
    "# ax1.plot(bad_up_x,bad_up_y,'o',color='green',label=f'Above {grade_threshold}%')\n",
    "# ax1.plot(bad_down_x,bad_down_y,'o',color='red',label=f'Below -{grade_threshold}%')\n",
    "\n",
    "# #ax1.set_title(f'{road_name} (Linkid:{linkid}) Veritcal Cross-Section')\n",
    "# ax1.set_xlabel('Distance (m)')\n",
    "# ax1.set_ylabel('Elevation (m)')\n",
    "# ax1.legend()\n",
    "\n",
    "# #Second and Third figure\n",
    "\n",
    "# #create geos to plot\n",
    "# points = np.array([Point(x,y) for x,y in item['geometry']])\n",
    "# line = LineString(item['geometry'])\n",
    "# minx, miny, maxx, maxy = line.bounds\n",
    "# # line_gdf = gpd.GeoDataFrame({'geometry':line},crs=dem_crs,index=[0])\n",
    "\n",
    "# # use mask to just get bad one\n",
    "# bad_up_grades_points = points[bad_up_grades]\n",
    "# bad_up_grades_points = gpd.GeoDataFrame({'geometry':bad_up_grades_points},geometry='geometry',crs=dem_crs)\n",
    "# bad_up_grades_points['type'] = '> 15%'\n",
    "\n",
    "# bad_down_grades_points = points[bad_down_grades]\n",
    "# bad_down_grades_points = gpd.GeoDataFrame({'geometry':bad_down_grades_points},geometry='geometry',crs=dem_crs)\n",
    "# bad_down_grades_points['type'] = '< -15%'\n",
    "\n",
    "# bad_grades_points = pd.concat([bad_up_grades_points,bad_down_grades_points])\n",
    "\n",
    "\n",
    "# color_dict = {\n",
    "#     '> 15%': 'green',\n",
    "#     '< -15%': 'red',\n",
    "# }\n",
    "# bad_grades_points['color'] = bad_grades_points['type'].map(color_dict)\n",
    "\n",
    "# bad_grades_points.plot(ax=ax2,color=bad_grades_points['color'],zorder=4)\n",
    "# bad_grades_points.plot(ax=ax3,color=bad_grades_points['color'],zorder=4)\n",
    "\n",
    "# #for drawing the link\n",
    "# x = np.array([x for x, y in item['geometry']])\n",
    "# y = np.array([y for x, y in item['geometry']])\n",
    "# # length of line segment\n",
    "# ds=10\n",
    "# # number of line segments per interval\n",
    "# Ns = np.round(np.sqrt( (x[1:]-x[:-1])**2 + (y[1:]-y[:-1])**2 ) / ds).astype(int)\n",
    "# # sub-divide intervals w.r.t. Ns\n",
    "# subdiv = lambda x, Ns=Ns: np.concatenate([ np.linspace(x[ii], x[ii+1], Ns[ii]) for ii, _ in enumerate(x[:-1]) ])\n",
    "# x, y = subdiv(x), subdiv(y)\n",
    "# ax2.quiver(x[:-1], y[:-1], x[1:]-x[:-1], y[1:]-y[:-1], scale_units='xy', angles='xy', scale=1, width=.004, headlength=4, headwidth=4)\n",
    "# ax3.quiver(x[:-1], y[:-1], x[1:]-x[:-1], y[1:]-y[:-1], scale_units='xy', angles='xy', scale=1, width=.004, headlength=4, headwidth=4)\n",
    "\n",
    "# #If you want a legend for map (didnot figure out arrow in the legend)\n",
    "# # from matplotlib.lines import Line2D\n",
    "# # from matplotlib.patches import Arrow\n",
    "# # custom_points = [Line2D([0], [0], marker=\"o\", linestyle=\"none\", markersize=10, color=color) for color in color_dict.values()]\n",
    "# # #custom_line = Arrow(0,0,dx=0.1,dy=0,width=0.1,linewidth=0.5,color='black')\n",
    "# # #custom_points.append()\n",
    "# # leg_points = ax2.legend(custom_points, ['> 15%','< -15%'])#, title = 'Legend', alignment='right')\n",
    "# # ax2.add_artist(leg_points)\n",
    "\n",
    "# #make sure fig is square\n",
    "# padding = 100\n",
    "# x_diff = np.abs(maxx - minx)\n",
    "# y_diff = np.abs(maxy - miny)\n",
    "# diff = (x_diff - y_diff)/2\n",
    "# if diff > 0:\n",
    "#     ax2.set_xlim(minx-padding,maxx+padding)\n",
    "#     ax2.set_ylim(miny-padding-diff,maxy+padding+diff)\n",
    "\n",
    "#     ax3.set_xlim(minx-padding,maxx+padding)\n",
    "#     ax3.set_ylim(miny-padding-diff,maxy+padding+diff)\n",
    "# else:\n",
    "#     ax2.set_xlim(minx-padding-np.abs(diff),maxx+padding+np.abs(diff))\n",
    "#     ax2.set_ylim(miny-padding,maxy+padding)\n",
    "\n",
    "#     ax3.set_xlim(minx-padding-np.abs(diff),maxx+padding+np.abs(diff))\n",
    "#     ax3.set_ylim(miny-padding,maxy+padding)\n",
    "\n",
    "# ax2.set_axis_off()\n",
    "# ax3.set_axis_off()\n",
    "\n",
    "# cx.add_basemap(ax2,zoom=17,source=cx.providers.MapTiler.Satellite(key=maptilerapikey),crs=bad_grades_points.crs,alpha=0.5)\n",
    "# cx.add_basemap(ax3,zoom=16,source=cx.providers.MapTiler.Streets(key=maptilerapikey),crs=bad_grades_points.crs)\n",
    "\n",
    "# #maybe if we wanted a high res version of this later\n",
    "# #https://stackoverflow.com/questions/42483449/mapbox-gl-js-export-map-to-png-or-pdf\n",
    "\n",
    "# name = links.loc[links['linkid']==linkid,'name'].item()\n",
    "# plt.suptitle(f'{name} ({linkid}) vertical profile')\n",
    "# plt.show()\n",
    "\n",
    "# # try: \n",
    "# #     plt.savefig(Path(f'D:/bad_grades/{name}_{linkid}.png'),dpi=300)\n",
    "# # except:\n",
    "# #     'invalid filename'\n",
    "\n",
    "# # #clear the axis\n",
    "# # ax1.cla()\n",
    "# # ax2.cla()\n",
    "# # ax3.cla()\n",
    "# linkid = 28689\n",
    "# item = interpolated_points_dict[linkid]\n",
    "\n",
    "# distances = np.array(item['distances'])\n",
    "# elevations = np.array(item['elevations'])\n",
    "# bad_elevs = item['bad_grades']\n",
    "# #temp_anom = [-0.17, -0.09, -0.11, -0.18, -0.3] # and so on...\n",
    "\n",
    "# # further step could be to identify the high points and assign a lower weight \n",
    "# weights = np.ones_like(elevations)\n",
    "# weights[elevations>311] = 0\n",
    "\n",
    "# whittaker_smoother = WhittakerSmoother(\n",
    "#     lmbda=150, order=2, data_length=len(elevations)#, weights=weights\n",
    "# )\n",
    "\n",
    "# smoothed = whittaker_smoother.smooth(elevations)\n",
    "\n",
    "# plt.plot(distances,elevations)\n",
    "# plt.plot(distances,smoothed)\n",
    "# test1 = pd.Series(elevations).diff()\n",
    "# test1_up = test1[test1>0].sum() / distances.max() * 100\n",
    "# test2_down = test1[test1<0].sum() / distances.max() * 100\n",
    "# print(test1_up,test2_down)\n",
    "# test1 = pd.Series(smoothed).diff()\n",
    "# test1_up = test1[test1>0].sum() / distances.max() * 100\n",
    "# test2_down = test1[test1<0].sum() / distances.max() * 100\n",
    "# print(test1_up,test2_down)\n",
    "# # For these, Lidar probably isn't gonna help\n",
    "# # so we just want to try the spline fit\n",
    "\n",
    "# #identify large abnormalities and exclude stair cases (>25%)\n",
    "# # thresholds = range(6,25+5)\n",
    "# # counts = []\n",
    "# # for threshold in thresholds:\n",
    "# #     abnormal = ((links['max_grade'] > threshold) | (links['min_grade'] > threshold))#& (links['highway'] != 'steps')\n",
    "# #     counts.append(links.loc[abnormal].shape[0])\n",
    "# # np.array(counts)\n",
    "\n",
    "# #before sensitvity, just use the haobing one (15% for local roads)\n",
    "# threshold = 15\n",
    "# abnormal = ((links['max_grade'] > threshold) | (links['min_grade'] > threshold))#& (links['highway'] != 'steps')\n",
    "# abnormal_links = links[abnormal]\n",
    "# print(f\"{abnormal.sum()}/{links.shape[0]} links have at least one point exceeding {threshold}% ({abnormal_links.length.sum().round(0)}/{links.length.sum().round(0)} km)\")\n",
    "# abnormal_links['length_km'] = abnormal_links.length\n",
    "# print(abnormal_links.groupby('highway')['length_km'].sum().round(0).sort_values(ascending=False))\n",
    "\n",
    "# print(abnormal_links['highway'].value_counts().sort_values(ascending=False))\n",
    "# # Method 1\n",
    "# bridge_tif_fps = list((Path('D:/') / 'bridge_tif').glob('*.tif'))\n",
    "# abnormal_links.to_crs(rasterio.open(bridge_tif_fps[0]).crs,inplace=True)\n",
    "# lidar_sampled = dict()\n",
    "\n",
    "# for bridge_tif_fp in tqdm(bridge_tif_fps):\n",
    "#     #open the raster using the link\n",
    "#     bridge_tif = rasterio.open(bridge_tif_fp)\n",
    "\n",
    "#     #find all links that intersect with the current raster\n",
    "#     xmin, ymin, xmax, ymax = bridge_tif.bounds\n",
    "#     bbox = box(xmin,ymin,xmax,ymax)\n",
    "#     intersection = abnormal_links.intersects(bbox)\n",
    "    \n",
    "#     if (intersection == True).any():\n",
    "#         for index in abnormal_links[intersection].index:\n",
    "#             dict_values = interpolated_points_dict.get(index)\n",
    "\n",
    "#             #change the crs\n",
    "#             #open the first one to just get the crs\n",
    "#             geometry = [Point(x,y) for x,y in dict_values['geometry']]\n",
    "#             gdf = gpd.GeoDataFrame({'geometry':geometry},crs=dem_crs)\n",
    "#             gdf.to_crs(bridge_tif.crs,inplace=True)\n",
    "#             geometry = list(zip(gdf.geometry.x,gdf.geometry.y))\n",
    "\n",
    "#             sampled = np.array([val[0] for val in bridge_tif.sample(geometry)])\n",
    "\n",
    "#             #check for nan values in sampled\n",
    "#             #match the nan points to the nearest cell\n",
    "            \n",
    "\n",
    "#             #if at least one non-null value\n",
    "#             #replace elevation if value is higher?\n",
    "#             if np.isnan(sampled).all() == False:\n",
    "#                 lidar_sampled[index] =  np.nanmax([sampled,dict_values['elevations']],axis = 0)\n",
    "\n",
    "# # Method 2\n",
    "# Uses the raw lidar points instead.\n",
    "# #grab bridges\n",
    "\n",
    "# import pickle\n",
    "# with Path('D:/lidar_points.pkl').open('rb') as fh:\n",
    "#     lidar_points = pickle.load(fh)\n",
    "\n",
    "# lidar_points.to_crs(prev_crs,inplace=True)\n",
    "# spatial_index = lidar_points.sindex\n",
    "\n",
    "\n",
    "# lidar_sampled = dict()\n",
    "# incomplete = []\n",
    "\n",
    "# for index in tqdm(abnormal_links.index):\n",
    "#     dict_values = interpolated_points_dict.get(index)\n",
    "\n",
    "#     geometry = [Point(x,y) for x,y in dict_values['geometry']]\n",
    "#     gdf = gpd.GeoDataFrame({'geometry':geometry},crs=dem_crs)\n",
    "#     gdf.to_crs(prev_crs,inplace=True)\n",
    "\n",
    "#     #buffer the data\n",
    "#     buffer_m = 20\n",
    "#     gdf.geometry = gdf.buffer(buffer_m)\n",
    "\n",
    "#     #get the gdf bounding box\n",
    "#     polygon = gdf.geometry.unary_union.convex_hull\n",
    "    \n",
    "#     #use spatial index to only select a small number of points\n",
    "#     possible_matches_index = list(spatial_index.intersection(polygon.bounds))\n",
    "#     possible_matches = lidar_points.iloc[possible_matches_index]\n",
    "    \n",
    "#     #add an index column for the overlay part\n",
    "#     gdf.reset_index(inplace=True)\n",
    "#     precise_matches = gpd.overlay(possible_matches,gdf,how='intersection')\n",
    "\n",
    "#     if len(precise_matches) == 0:\n",
    "#         continue\n",
    "\n",
    "#     if gdf['index'].isin(set(precise_matches['index'].tolist())).all() == False:\n",
    "#         #this is where we might want to increase the buffer on the remaining points\n",
    "        \n",
    "#         #assign nan\n",
    "#         incomplete.append(index)\n",
    "\n",
    "#     new_values = precise_matches.groupby('index')['elevation_m'].mean()\n",
    "#     gdf['new_elevation_m'] = gdf['index'].map(new_values)\n",
    "#     new_values = np.array(gdf['new_elevation_m'])\n",
    "\n",
    "#     lidar_sampled[index] = np.nanmax([new_values,dict_values['elevations']],axis = 0)\n",
    "# lidar_sampled[36]\n",
    "# #interpolated_points_dict[36]\n",
    "# all_rise = {}\n",
    "# all_down = {}\n",
    "# max_rise = {}\n",
    "# min_rise = {}\n",
    "\n",
    "# for key, item in tqdm(lidar_sampled.items(),total=len(lidar_sampled)):     \n",
    "\n",
    "#     elevation_deltas = []\n",
    "    \n",
    "#     #then caluclate the elevation change with the smoothed data\n",
    "#     for x in range(0,len(item)-1):        \n",
    "#         elev1 = item[x]\n",
    "#         elev2 = item[x+1]\n",
    "#         elevation_delta = (elev2-elev1)\n",
    "#         elevation_deltas.append(elevation_delta)\n",
    "\n",
    "#     elevation_deltas = np.asarray(elevation_deltas)\n",
    "#     total_rise = elevation_deltas[elevation_deltas>0].sum()\n",
    "#     total_down = np.absolute(elevation_deltas[elevation_deltas<0].sum()) #take absolute value at this point\n",
    "#     all_rise[key] = total_rise\n",
    "#     all_down[key] = total_down\n",
    "\n",
    "#     if total_rise > 0:\n",
    "#         max_rise[key] = elevation_deltas[elevation_deltas>0].max()\n",
    "#     else:\n",
    "#         max_rise[key] = 0\n",
    "\n",
    "#     if total_down > 0:\n",
    "#         min_rise[key] = np.absolute(elevation_deltas[elevation_deltas<0].min())\n",
    "#     else:\n",
    "#         min_rise[key] = 0\n",
    "\n",
    "# #forward direction\n",
    "# links['new_rise_m'] = pd.Series(all_rise).round(3)\n",
    "# links['new_maxrise_m'] = pd.Series(max_rise).round(3)\n",
    "# links['new_up_grade'] = (links['new_rise_m'] / links.geometry.length * 100).round(1)\n",
    "# links.loc[links.length > interpolate_dist_m,'new_max_grade'] = (links['new_maxrise_m'] / interpolate_dist_m * 100).round(1)\n",
    "# links.loc[links['new_max_grade'].isna(),'new_max_grade'] = (links['new_maxrise_m'] / links.length * 100).round(1)\n",
    "\n",
    "# #reverse direction\n",
    "# links['new_down_m'] = pd.Series(all_down).round(3)\n",
    "# links['new_minrise_m'] = pd.Series(min_rise).round(3)\n",
    "# links['new_down_grade'] = (links['new_down_m'] / links.geometry.length * 100).round(1)\n",
    "# links.loc[links.length > interpolate_dist_m,'new_min_grade'] = (links['new_minrise_m'] / interpolate_dist_m * 100).round(1)\n",
    "# links.loc[links['new_min_grade'].isna(),'new_min_grade'] = (links['new_minrise_m'] / links.length * 100).round(1)\n",
    "\n",
    "# # See where elevation data improved\n",
    "# all_rise = {}\n",
    "# all_down = {}\n",
    "# max_rise = {}\n",
    "# min_rise = {}\n",
    "\n",
    "# for key, item in tqdm(lidar_sampled.items(),total=len(lidar_sampled)):     \n",
    "\n",
    "#     elevation_deltas = []\n",
    "    \n",
    "#     #then caluclate the elevation change with the smoothed data\n",
    "#     for x in range(0,len(item)-1):        \n",
    "#         elev1 = item[x]\n",
    "#         elev2 = item[x+1]\n",
    "#         elevation_delta = (elev2-elev1)\n",
    "#         elevation_deltas.append(elevation_delta)\n",
    "\n",
    "#     elevation_deltas = np.asarray(elevation_deltas)\n",
    "#     total_rise = elevation_deltas[elevation_deltas>0].sum()\n",
    "#     total_down = np.absolute(elevation_deltas[elevation_deltas<0].sum()) #take absolute value at this point\n",
    "#     all_rise[key] = total_rise\n",
    "#     all_down[key] = total_down\n",
    "\n",
    "#     if total_rise > 0:\n",
    "#         max_rise[key] = elevation_deltas[elevation_deltas>0].max()\n",
    "#     else:\n",
    "#         max_rise[key] = 0\n",
    "\n",
    "#     if total_down > 0:\n",
    "#         min_rise[key] = np.absolute(elevation_deltas[elevation_deltas<0].min())\n",
    "#     else:\n",
    "#         min_rise[key] = 0\n",
    "\n",
    "# #forward direction\n",
    "# links['new_rise_m'] = pd.Series(all_rise).round(3)\n",
    "# links['new_maxrise_m'] = pd.Series(max_rise).round(3)\n",
    "# links['new_up_grade'] = (links['new_rise_m'] / links.geometry.length * 100).round(1)\n",
    "# links.loc[links.length > interpolate_dist_m,'new_max_grade'] = (links['new_maxrise_m'] / interpolate_dist_m * 100).round(1)\n",
    "# links.loc[links['new_max_grade'].isna(),'new_max_grade'] = (links['new_maxrise_m'] / links.length * 100).round(1)\n",
    "\n",
    "# #reverse direction\n",
    "# links['new_down_m'] = pd.Series(all_down).round(3)\n",
    "# links['new_minrise_m'] = pd.Series(min_rise).round(3)\n",
    "# links['new_down_grade'] = (links['new_down_m'] / links.geometry.length * 100).round(1)\n",
    "# links.loc[links.length > interpolate_dist_m,'new_min_grade'] = (links['new_minrise_m'] / interpolate_dist_m * 100).round(1)\n",
    "\n",
    "# prev = abnormal_links.shape[0]\n",
    "# fixed = ((links['new_max_grade'] <= threshold) & (links['new_min_grade'] <= threshold))\n",
    "# still_bad = ((links['new_max_grade'] > threshold) & (links['new_min_grade'] > threshold))\n",
    "# print(f'{fixed.sum()} / {prev} abnormal links fixed')\n",
    "# print(f'{still_bad.sum()} are still abnormal')\n",
    "# print(f'{prev-fixed.sum()-still_bad.sum()} did not overlap with a bridge tif')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from scipy.interpolate import splrep, splev\n",
    "\n",
    "# #pick a random one to examine\n",
    "# #linkid = int(abnormal_links.loc[new,'linkid'].sample(1).item())\n",
    "# linkid = links.loc[still_bad,'linkid'].sample(1).item()\n",
    "# #linkid = 53107\n",
    "\n",
    "# #linkid = 28303#44061\n",
    "# item = interpolated_points_dict[linkid]\n",
    "\n",
    "# #title\n",
    "# road_name = links.loc[links.index==linkid,'name'].item()\n",
    "\n",
    "# #ax1 elements\n",
    "# x = np.array(item['distances'])\n",
    "# y = np.array(item['elevations'])\n",
    "# new_y = np.array(lidar_sampled[linkid])\n",
    "\n",
    "# #ax2 elements\n",
    "# grade_x = np.array(range(0,len(x) - 1))\n",
    "# grade_delta = np.diff(y) / 10 * 100\n",
    "# lidar_delta = np.diff(new_y) / 10 * 100\n",
    "\n",
    "# fig, (ax1,ax2) = plt.subplots(ncols=2,figsize=(9,3))\n",
    "\n",
    "# # Plot the original data and the smoothed curve\n",
    "# ax1.plot(x, y, 'o', label='Original Data')\n",
    "# ax1.plot(x, new_y, 'x',label='Resampled with Lidar')\n",
    "# ax1.grid(True,linestyle='-.')\n",
    "\n",
    "# #find min\n",
    "# y_min = np.min([y.min(),new_y.min()])\n",
    "# y_max = np.max([y.max(),new_y.max()])\n",
    "\n",
    "# ax1.set_xlim(0,np.ceil(x.max() / 10) * 10)\n",
    "# ax1.set_ylim(np.floor(y_min/10)*10,np.ceil(y_max/10)*10)\n",
    "# ax1.set_box_aspect(1)\n",
    "\n",
    "# #ax1.set_title(f'{road_name} (Linkid:{linkid}) Veritcal Cross-Section')\n",
    "# ax1.set_xlabel('Distance (m)')\n",
    "# ax1.set_ylabel('Elevation (m)')\n",
    "# ax1.legend()\n",
    "\n",
    "# ax2.plot(grade_x, grade_delta, 'o', label='Original Grade')\n",
    "# ax2.plot(grade_x, lidar_delta, 'x', label='lidar grade')\n",
    "\n",
    "\n",
    "# ax2.set_xlabel('Distance (m)')\n",
    "# ax2.set_ylabel('Grade %')\n",
    "# ax2.set_box_aspect(1)\n",
    "# ax2.legend()\n",
    "\n",
    "# fig.suptitle(f'{road_name} (Linkid:{linkid})')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # Set ticks\n",
    "# #plt.xlim(0,x.max())\n",
    "# #plt.ylim()\n",
    "\n",
    "# # Plot the dropped points as 'x's\n",
    "# #plt.scatter(x_dropped, y_dropped, marker='x', color='red', label='Dropped Points')\n",
    "\n",
    "\n",
    "# #turn item into a dataframe\n",
    "# #df = pd.DataFrame.from_dict(item,orient='columns')\n",
    "\n",
    "# # df['elev_change'] = df['elevations'].diff()\n",
    "# # df['dist_change'] = df['distances'].diff()\n",
    "# # df['change_rate'] = np.abs(df['elev_change'] / df['dist_change'])\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from scipy.interpolate import splrep, splev\n",
    "\n",
    "# #pick a random one to examine\n",
    "# #linkid = int(abnormal_links.loc[new,'linkid'].sample(1).item())\n",
    "# linkid = links.loc[still_bad,'linkid'].sample(1).item()\n",
    "# #linkid = 53107\n",
    "\n",
    "# #linkid = 28303#44061\n",
    "# item = interpolated_points_dict[linkid]\n",
    "\n",
    "# links.loc[links.index==linkid,['up_grade','max_grade','min_grade','new_up_grade','down_grade','new_down_grade']].squeeze()\n",
    "# links.loc[links.index==linkid,['up_grade','max_grade','min_grade','new_up_grade','down_grade','new_down_grade']].squeeze()\n",
    "# links.loc[links.index==linkid,['up_grade','max_grade','min_grade','new_up_grade','down_grade','new_down_grade']].squeeze()\n",
    "# #title\n",
    "# road_name = links.loc[links.index==linkid,'name'].item()\n",
    "\n",
    "# #ax1 elements\n",
    "# x = np.array(item['distances'])\n",
    "# y = np.array(item['elevations'])\n",
    "# new_y = np.array(lidar_sampled[linkid])\n",
    "\n",
    "# #ax2 elements\n",
    "# grade_x = np.array(range(0,len(x) - 1))\n",
    "# grade_delta = np.diff(y) / 10 * 100\n",
    "# lidar_delta = np.diff(new_y) / 10 * 100\n",
    "\n",
    "# fig, (ax1,ax2) = plt.subplots(ncols=2,figsize=(9,3))\n",
    "\n",
    "# # Plot the original data and the smoothed curve\n",
    "# ax1.plot(x, y, 'o', label='Original Data')\n",
    "# ax1.plot(x, new_y, 'x',label='Resampled with Lidar')\n",
    "# ax1.grid(True,linestyle='-.')\n",
    "\n",
    "# #find min\n",
    "# y_min = np.min([y.min(),new_y.min()])\n",
    "# y_max = np.max([y.max(),new_y.max()])\n",
    "\n",
    "# ax1.set_xlim(0,np.ceil(x.max() / 10) * 10)\n",
    "# ax1.set_ylim(np.floor(y_min/10)*10,np.ceil(y_max/10)*10)\n",
    "# ax1.set_box_aspect(1)\n",
    "\n",
    "# #ax1.set_title(f'{road_name} (Linkid:{linkid}) Veritcal Cross-Section')\n",
    "# ax1.set_xlabel('Distance (m)')\n",
    "# ax1.set_ylabel('Elevation (m)')\n",
    "# ax1.legend()\n",
    "\n",
    "# ax2.plot(grade_x, grade_delta, 'o', label='Original Grade')\n",
    "# ax2.plot(grade_x, lidar_delta, 'x', label='lidar grade')\n",
    "\n",
    "\n",
    "# ax2.set_xlabel('Distance (m)')\n",
    "# ax2.set_ylabel('Grade %')\n",
    "# ax2.set_box_aspect(1)\n",
    "# ax2.legend()\n",
    "\n",
    "# fig.suptitle(f'{road_name} (Linkid:{linkid})')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # Set ticks\n",
    "# #plt.xlim(0,x.max())\n",
    "# #plt.ylim()\n",
    "\n",
    "# # Plot the dropped points as 'x's\n",
    "# #plt.scatter(x_dropped, y_dropped, marker='x', color='red', label='Dropped Points')\n",
    "\n",
    "\n",
    "# #turn item into a dataframe\n",
    "# #df = pd.DataFrame.from_dict(item,orient='columns')\n",
    "\n",
    "# # df['elev_change'] = df['elevations'].diff()\n",
    "# # df['dist_change'] = df['distances'].diff()\n",
    "# # df['change_rate'] = np.abs(df['elev_change'] / df['dist_change'])prev = abnormal_links.shape[0]\n",
    "# fixed = ((links['new_max_grade'] <= threshold) & (links['new_min_grade'] <= threshold))\n",
    "# still_bad = ((links['new_max_grade'] > threshold) & (links['new_min_grade'] > threshold))\n",
    "# print(f'{fixed.sum()} / {prev} abnormal links fixed')\n",
    "# print(f'{still_bad.sum()} are still abnormal')\n",
    "# print(f'{prev-fixed.sum()-still_bad.sum()} did not overlap with a bridge tif')\n",
    "# prev = abnormal_links.shape[0]\n",
    "# fixed = ((links['new_max_grade'] <= threshold) & (links['new_min_grade'] <= threshold))\n",
    "# still_bad = ((links['new_max_grade'] > threshold) & (links['new_min_grade'] > threshold))\n",
    "# print(f'{fixed.sum()} / {prev} abnormal links fixed')\n",
    "# print(f'{still_bad.sum()} are still abnormal')\n",
    "# print(f'{prev-fixed.sum()-still_bad.sum()} did not overlap with a bridge tif')links.loc[links['new_min_grade'].isna(),'new_min_grade'] = (links['new_minrise_m'] / links.length * 100).round(1)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from scipy.interpolate import splrep, splev\n",
    "\n",
    "# linkid = 28303#44061\n",
    "# item = interpolated_points_dict[linkid]\n",
    "\n",
    "# x = item['distances']\n",
    "# y = item['elevations']\n",
    "# new_y = new[linkid]\n",
    "\n",
    "# # Plot the original data and the smoothed curve\n",
    "# plt.plot(x, y, 'o', label='Original Data')\n",
    "# plt.plot(x, new_y, 'o',label='Resampled with Lidar')\n",
    "\n",
    "# # Plot the dropped points as 'x's\n",
    "# #plt.scatter(x_dropped, y_dropped, marker='x', color='red', label='Dropped Points')\n",
    "\n",
    "# #plt.title('10th Street Northwest (westward direction, tech to midtown) Veritcal Cross-Section')\n",
    "# plt.xlabel('Distance (m)')\n",
    "# plt.ylabel('Elevation (m)')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# #turn item into a dataframe\n",
    "# #df = pd.DataFrame.from_dict(item,orient='columns')\n",
    "\n",
    "# # df['elev_change'] = df['elevations'].diff()\n",
    "# # df['dist_change'] = df['distances'].diff()\n",
    "# # df['change_rate'] = np.abs(df['elev_change'] / df['dist_change'])\n",
    "# ### Based on Haobing's paper\n",
    "# Spline fitting function\n",
    "# x as linear distance and y as elevation\n",
    "\n",
    "# current issue is that segments are not long enough as is, bridges need to extend forwards/backwards to have more data on the surrounding terrain. In the paper there are 3 km long segments. We can either try to linemerge lines to make them longer or add extensions\n",
    "\n",
    "# this is where the weighting prolly comes in\n",
    "\n",
    "# [Scipy Splrep](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.splrep.html#scipy.interpolate.splrep)\n",
    "\n",
    "\n",
    "# # Assuming x, y, x_dropped, y_dropped, and spl are defined as in your code\n",
    "# x = np.array(df_copy['distances'])\n",
    "# y = np.array(df_copy['elevations'])\n",
    "\n",
    "# df_dropped = df.loc[~df.index.isin(df_copy.index)]\n",
    "\n",
    "# x_dropped = np.array(df_dropped['distances'])\n",
    "# y_dropped = np.array(df_dropped['elevations'])\n",
    "\n",
    "# #spline technique (we're not doing OLS i think because we don't have ground truth data?)\n",
    "# lambda_parameter = 3000 # 500 for local road and 3000 for highways\n",
    "# alpha = 99999990000 #paper used between 1000 and 10000\n",
    "# # i believe higher is smoother\n",
    "# spl = splrep(x,y,s=alpha)\n",
    "# y2 = splev(x,spl)\n",
    "\n",
    "# # Plot the original data and the smoothed curve\n",
    "# plt.plot(x, y, 'o', label='Original Data')\n",
    "# plt.plot(x, y2, label='Smoothed Curve')\n",
    "\n",
    "# # Plot the dropped points as 'x's\n",
    "# plt.scatter(x_dropped, y_dropped, marker='x', color='red', label='Dropped Points')\n",
    "\n",
    "# plt.title('10th Street Northwest (westward direction, tech to midtown) Veritcal Cross-Section')\n",
    "# plt.xlabel('Distance (m)')\n",
    "# plt.ylabel('Elevation (m)')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# links_copy = links.copy()\n",
    "\n",
    "\n",
    "# # Assuming x, y, x_dropped, y_dropped, and spl are defined as in your code\n",
    "# x = np.array(df_copy['distances'])\n",
    "# y = np.array(df_copy['elevations'])\n",
    "\n",
    "# df_dropped = df.loc[~df.index.isin(df_copy.index)]\n",
    "\n",
    "# x_dropped = np.array(df_dropped['distances'])\n",
    "# y_dropped = np.array(df_dropped['elevations'])\n",
    "\n",
    "# #spline technique (we're not doing OLS i think because we don't have ground truth data?)\n",
    "# lambda_parameter = 3000 # 500 for local road and 3000 for highways\n",
    "# alpha = 99999990000 #paper used between 1000 and 10000\n",
    "# # i believe higher is smoother\n",
    "# spl = splrep(x,y,s=alpha)\n",
    "# y2 = splev(x,spl)\n",
    "\n",
    "# # Plot the original data and the smoothed curve\n",
    "# plt.plot(x, y, 'o', label='Original Data')\n",
    "# plt.plot(x, y2, label='Smoothed Curve')\n",
    "\n",
    "# # Plot the dropped points as 'x's\n",
    "# plt.scatter(x_dropped, y_dropped, marker='x', color='red', label='Dropped Points')\n",
    "\n",
    "# plt.title('10th Street Northwest (westward direction, tech to midtown) Veritcal Cross-Section')\n",
    "# plt.xlabel('Distance (m)')\n",
    "# plt.ylabel('Elevation (m)')\n",
    "# plt.legend()\n",
    "\n",
    "# links_copy['tunnel'].value_counts()\n",
    "\n",
    "# vals = ['yes','buidling_passage']\n",
    "# links_copy['tunnel'] = links_copy['tunnel'].isin(vals)\n",
    "# plt.show()\n",
    "\n",
    "# grade_cols = links_copy.columns.tolist()[-8:]\n",
    "# bridge_or_tunnel = (links_copy['tunnel']==True) | (links_copy['bridge']==True)\n",
    "# links_copy.loc[bridge_or_tunnel,['name','geometry']+grade_cols].explore()\n",
    "# links_copy.loc[bridge_or_tunnel,grade_cols] = 0\n",
    "# ## Side profile to show example (10th Street over the connector)\n",
    "# links.columns\n",
    "# linkid = 28303\n",
    "\n",
    "# street_name = links.at[linkid,'name']\n",
    "# distances = interpolated_points_dict[linkid]['distances']\n",
    "# elevations = interpolated_points_dict[linkid]['elevations']\n",
    "# geometries = interpolated_points_dict[linkid]['geometry']\n",
    "\n",
    "# plt.scatter(distances,elevations)\n",
    "# plt.title(street_name)\n",
    "# plt.xlabel('Distance (m)')\n",
    "# plt.ylabel('Elevation (m)')\n",
    "# import folium\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import LineString\n",
    "# import matplotlib.pyplot as plt\n",
    "# import base64\n",
    "# from io import BytesIO\n",
    "\n",
    "# # Replace 'linkid' with your unique identifier for the road segment\n",
    "# linkid = 28303\n",
    "\n",
    "# # Extracting relevant data\n",
    "# street_name = links.at[linkid,'name']\n",
    "# distances = interpolated_points_dict[linkid]['distances']\n",
    "# elevations = interpolated_points_dict[linkid]['elevations']\n",
    "# geometries = interpolated_points_dict[linkid]['geometry']\n",
    "\n",
    "# # Creating a GeoDataFrame for the road segment\n",
    "# road_geometry = LineString(geometries)\n",
    "# road_gdf = gpd.GeoDataFrame(geometry=[road_geometry],crs=src.crs)\n",
    "# road_gdf.to_crs('epsg:4326',inplace=True)\n",
    "\n",
    "# # Create a Folium map centered on the road segment\n",
    "# map_center = [road_gdf.centroid.y, road_gdf.centroid.x]\n",
    "# mymap = folium.Map(location=map_center, zoom_start=15)\n",
    "\n",
    "# # Save the scatter plot as an image\n",
    "# scatter_plot_filename = 'scatter_plot.png'\n",
    "# plt.scatter(distances, elevations, color='red', marker='o', label='Cross-section')\n",
    "# plt.title(street_name)\n",
    "# plt.xlabel('Distance (m)')\n",
    "# plt.ylabel('Elevation (m)')\n",
    "# plt.legend()\n",
    "# plt.savefig(scatter_plot_filename)\n",
    "# plt.close()\n",
    "\n",
    "# # Encode the image to base64\n",
    "# with open(scatter_plot_filename, 'rb') as img_file:\n",
    "#     encoded_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "# # Add the scatter plot image to the map popup\n",
    "# popup_content = \"\"\"\n",
    "#     <h4>{}</h4>\n",
    "#     <img src=\"data:image/png;base64,{}\" alt=\"Scatter Plot\" width=\"400px\">\n",
    "# \"\"\".format(street_name, encoded_image)\n",
    "\n",
    "# popup = folium.Popup(html=popup_content, max_width=500)\n",
    "# folium.GeoJson(road_gdf,popup=popup,lazy=True).add_to(mymap)\n",
    "\n",
    "# # Save the Folium map as an HTML file\n",
    "# #map_filename = 'road_map_with_scatter_plot.html'\n",
    "# #mymap.save(map_filename)\n",
    "\n",
    "# # Display the Folium map\n",
    "# mymap\n",
    "\n",
    "# ## Set to zero for now (we'll update these once the rest of the paper is intergrated)\n",
    "# links.columns\n",
    "# links.columns\n",
    "# # #example point grade calculation for finding rapid changes\n",
    "# # linkid = 40\n",
    "\n",
    "# # point1 = Point(interpolated_points_dict[linkid]['geometry'][0])\n",
    "# # point2 = Point(interpolated_points_dict[linkid]['geometry'][1])\n",
    "# # point1.distance(point2)\n",
    "\n",
    "# # elev1 = interpolated_points_dict[linkid]['elevations'][0]\n",
    "# # elev2 = interpolated_points_dict[linkid]['elevations'][1]\n",
    "\n",
    "# # grade = (elev2-elev1)/(point1.distance(point2))\n",
    "# # grade\n",
    "\n",
    "# # calculates point grade (deprecated)\n",
    "# # item = interpolated_points_dict[linkid]\n",
    "# # grades = []\n",
    "\n",
    "# # for x in range(0,len(item['elevations'])-1):\n",
    "# #     point1 = Point(item['geometry'][x])\n",
    "# #     point2 = Point(item['geometry'][x+1])\n",
    "\n",
    "# #     elev1 = item['elevations'][x]\n",
    "# #     elev2 = item['elevations'][x+1]\n",
    "\n",
    "# #     grade = (elev2-elev1)/(point1.distance(point2))\n",
    "# #     grades.append(grade)\n",
    "\n",
    "# # grades = np.asarray(grades)\n",
    "# # grades[grades>0].mean()\n",
    "\n",
    "# # # calculate average point grade\n",
    "# # # 3 mins\n",
    "\n",
    "# # grades_dict = {}\n",
    "\n",
    "# # for key, item in tqdm(interpolated_points_dict.items(),total=len(interpolated_points_dict)): \n",
    "# #     grades = []\n",
    "\n",
    "\n",
    "# #     for x in range(0,len(item['elevations'])-1):\n",
    "# #         point1 = Point(item['geometry'][x])\n",
    "# #         point2 = Point(item['geometry'][x+1])\n",
    "\n",
    "# #         elev1 = item['elevations'][x]\n",
    "# #         elev2 = item['elevations'][x+1]\n",
    "\n",
    "# #         grade = (elev2-elev1)/(point1.distance(point2))\n",
    "# #         grades.append(grade)\n",
    "\n",
    "# #     grades = np.asarray(grades)\n",
    "# #     grades_dict[key] = grades[grades>0].mean()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
