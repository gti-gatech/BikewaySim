{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elevation Cleaning\n",
    "- Resample bridge deck elevation from labelled point cloud data if points exceed grade change and link is a bridge\n",
    "- For underpass or tunnel links that intersect with bridge links, remove points representing elevated section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated: 4/22/24:\n",
    "Instead of using the network graph version, use the full network version. This means that when finally assigning the elevation stats, we'll need to interpolate using the dictionary results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.plot import show\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import box, mapping\n",
    "from shapely.ops import Point\n",
    "from tqdm import tqdm\n",
    "import pyproj\n",
    "import math\n",
    "from shapely.ops import LineString\n",
    "import pickle\n",
    "import contextily as cx\n",
    "\n",
    "from whittaker_eilers import WhittakerSmoother\n",
    "\n",
    "import src.elevation_tools as elevation_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = json.load((Path.cwd().parent / 'config.json').open('rb'))\n",
    "network_fp = Path(config['project_directory']) / 'OSM_Download'\n",
    "export_fp = Path(config['project_directory']) / 'Network'\n",
    "if network_fp.exists() == False:\n",
    "    network_fp.mkdir()\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import non-network version of osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import network\n",
    "network_filepath= Path(config['project_directory']) / 'Network'\n",
    "links = gpd.read_file(Path(config['project_directory']+f\"/OSM_Download/osm_{config['geofabrik_year']}.gpkg\"),layer='raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproject network to DEM crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_crs = links.crs\n",
    "tiff_links = list((Path(config['usgs']) / 'dem_files').glob('*.tif'))\n",
    "\n",
    "#open the first one to just get the crs\n",
    "src = rasterio.open(tiff_links[0])\n",
    "dem_crs = src.crs\n",
    "src.close()\n",
    "\n",
    "links.to_crs(dem_crs,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import sampled elevation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for storing the interpolated points with sampled elevation data\n",
    "with (network_filepath/'elevation.pkl').open('rb') as fh:\n",
    "    interpolated_points_dict = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local road that I know has steep grades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkid = 340365816\n",
    "grade_threshold = 15\n",
    "elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Northside Drive as \"tertiary\" road with a small segment above 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkid = 352003174\n",
    "grade_threshold = 15\n",
    "elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkid = 44097075\n",
    "grade_threshold = 30\n",
    "elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeltLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkid = 226119768\n",
    "grade_threshold = 15\n",
    "elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkid = 741964053\n",
    "grade_threshold = 30\n",
    "elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Circular golf course loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkid = 1087991070\n",
    "elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Knockout\n",
    "- Calculate segment grade changes using the sampling distance\n",
    "- Define a segment grade threshold by OSM highway type (e.g., motorway, local, secondary, etc.)\n",
    "    - Highways/Interstates > 8%\n",
    "    - Most local roads > 15%\n",
    "    - Few local roads > 25%\n",
    "- Knockout all grade changes that exceed threshold\n",
    "- Repeat until no grade changes are above threshold\n",
    "    - If this removes too many points \n",
    "- Spline fit on the remaining data for the interpolation step\n",
    "\n",
    "Hongyu Method:\n",
    "- Find first grade change (ascent or descent) exceeding threshold\n",
    "- Search 30m, 100m, or 150m after and find the last opposite grade change exceeding threshold\n",
    "- Remove first to last point\n",
    "- Infill with the spline fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, set elevation points to nan for large grade changes (> 30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(elevation_tools)\n",
    "import src.elevation_tools as elevation_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_threshold = 30\n",
    "\n",
    "#indentifies which links exceed threshold\n",
    "exceeds_1 = elevation_tools.exceeds_threshold(links['osmid'].tolist(),interpolated_points_dict,large_threshold)\n",
    "\n",
    "#exceeds_threshold[['id','highway','tunnel','bridge','geometry']].explore(popup=True)\n",
    "\n",
    "#knockout points\n",
    "for linkid in tqdm(exceeds_1):\n",
    "    item = interpolated_points_dict.get(linkid,0)\n",
    "    elevation_tools.point_knockout(item,grade_threshold)\n",
    "    interpolated_points_dict[linkid] = item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set different grade thresholds by the assigned lables\n",
    "Lists are collectively exhaustive of all the highway tags in the study area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the label field\n",
    "links['label'] = None\n",
    "\n",
    "#tunnel\n",
    "links.loc[links['tunnel'].notna() & links['label'].isna(),'label'] = 'tunnel'\n",
    "\n",
    "#bridge\n",
    "links.loc[links['bridge'].notna() & links['label'].isna(),'label'] = 'bridge'\n",
    "\n",
    "#motorway/major arterials\n",
    "motorway = ['motorway','motorway_link',\n",
    "            'trunk', 'trunk_link',\n",
    "            'primary','primary_link',\n",
    "            'secondary','secondary_link',\n",
    "            'raceway', 'proposed']\n",
    "links.loc[links['highway'].isin(motorway) & links['label'].isna(),'label'] = 'motorway'\n",
    "\n",
    "#local/service roads\n",
    "local = ['tertiary','tertiary_link','residential','service', 'unclassified','living_street']\n",
    "links.loc[links['highway'].isin(local) & links['label'].isna(),'label'] = 'local'\n",
    "\n",
    "#pedestrian paths/steps may not follow grade thresholds\n",
    "ped = ['path','footway','track','pedestrian','cycleway','platform']\n",
    "links.loc[links['highway'].isin(ped) & links['label'].isna(),'label'] = 'ped'\n",
    "\n",
    "#ways where grade doesn't matter\n",
    "exclude = ['steps','construction','disused', 'corridor','services']\n",
    "links.loc[links['highway'].isin(exclude) & links['label'].isna(),'label'] = 'exclude'\n",
    "\n",
    "# links[links['label'].isna()]\n",
    "# links['highway'].nunique()#.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knockout points on motorways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorway_threshold = 8\n",
    "motorway_ids = links.loc[links['label']=='motorway','osmid'].tolist()\n",
    "\n",
    "#indentifies which links exceed threshold\n",
    "exceeds_2 = elevation_tools.exceeds_threshold(motorway_ids,interpolated_points_dict,motorway_threshold)\n",
    "\n",
    "#exceeds_threshold[['id','highway','tunnel','bridge','geometry']].explore(popup=True)\n",
    "\n",
    "#knockout points\n",
    "for linkid in tqdm(exceeds_2):\n",
    "    item = interpolated_points_dict.get(linkid,0)\n",
    "    elevation_tools.point_knockout(item,motorway_threshold)\n",
    "    interpolated_points_dict[linkid] = item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knockout points on local roads and other facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_threshold = 20\n",
    "local_ids = links.loc[links['label']!='highway','osmid'].tolist()\n",
    "\n",
    "#indentifies which links exceed threshold\n",
    "exceeds_2 = elevation_tools.exceeds_threshold(local_ids,interpolated_points_dict,local_threshold)\n",
    "\n",
    "#exceeds_threshold[['id','highway','tunnel','bridge','geometry']].explore(popup=True)\n",
    "\n",
    "#knockout points\n",
    "for linkid in tqdm(exceeds_2):\n",
    "    item = interpolated_points_dict.get(linkid,0)\n",
    "    elevation_tools.point_knockout(item,local_threshold)\n",
    "    interpolated_points_dict[linkid] = item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample bridge decks (use lidar here)\n",
    "Need to fix this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import lidar processed lidar points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(config['usgs']+'/lidar_points.pkl').open('rb') as fh:\n",
    "    lidar_points = pickle.load(fh)\n",
    "lidar_points.to_crs(dem_crs,inplace=True)\n",
    "spatial_index = lidar_points.sindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_found = []\n",
    "\n",
    "for linkid in tqdm(links.loc[links['label']=='bridge','osmid'].tolist()):\n",
    "    \n",
    "    item = interpolated_points_dict.get(linkid)\n",
    "\n",
    "    geometry = [Point(x,y) for x,y in item['geometry']]\n",
    "    gdf = gpd.GeoDataFrame({'geometry':geometry},crs=dem_crs)\n",
    "\n",
    "    #buffer the data\n",
    "    buffer_m = 20\n",
    "    gdf.geometry = gdf.buffer(buffer_m)\n",
    "\n",
    "    #get the gdf bounding box\n",
    "    polygon = gdf.geometry.unary_union.convex_hull\n",
    "    \n",
    "    #use spatial index to only select a small number of points\n",
    "    possible_matches_index = list(spatial_index.intersection(polygon.bounds))\n",
    "    possible_matches = lidar_points.iloc[possible_matches_index]\n",
    "    \n",
    "    #add an index column for the overlay part\n",
    "    gdf.reset_index(inplace=True)\n",
    "    precise_matches = gpd.overlay(possible_matches,gdf,how='intersection')\n",
    "\n",
    "    #if no matches found take no action\n",
    "    if len(precise_matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        lidar_found.append(linkid)\n",
    "\n",
    "    #take average of all nearby values\n",
    "    new_values = precise_matches.groupby('index')['elevation_m'].mean()\n",
    "    gdf['new_elevation_m'] = gdf['index'].map(new_values)\n",
    "    new_values = np.array(gdf['new_elevation_m'])\n",
    "\n",
    "    no_lidar = np.isnan(new_values).sum()\n",
    "\n",
    "    #use nanmax\n",
    "    new_values = np.nanmax([new_values,item['elevations']],axis = 0)\n",
    "\n",
    "    #output = elevation_tools.elevation_stats(item['distances'],new_values,grade_threshold)\n",
    "\n",
    "    #replace existing values\n",
    "    interpolated_points_dict[linkid]['elevations'] = new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lidar_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lidar_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spline Fit\n",
    "For all the links, fit a spline for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spline fit\n",
    "import numpy as np\n",
    "from scipy.interpolate import splrep, splev, BSpline\n",
    "\n",
    "too_short = [ ]\n",
    "\n",
    "for key, item in tqdm(interpolated_points_dict.items()):\n",
    "    df = pd.DataFrame({'distance':item['distances'],'elevation':item['elevations']})\n",
    "\n",
    "    #remove na values\n",
    "    df = df[df.notna().all(axis=1)]\n",
    "\n",
    "    #in this case, just do linear interpolation between the two values\n",
    "    if df.shape[0] <= 3:\n",
    "        too_short.append(key)\n",
    "        continue\n",
    "\n",
    "    #fit a spline\n",
    "    spline = splrep(df['distance'], df['elevation'], s=0.5)\n",
    "\n",
    "    #add spline to dict\n",
    "    interpolated_points_dict[key]['spline'] = spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9245034 goes under a railraod track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add a check here\n",
    "# import random\n",
    "\n",
    "# osmid = 751119047#random.choice(list(interpolated_points_dict.keys()))\n",
    "# item = interpolated_points_dict[osmid]\n",
    "\n",
    "# spline = item.get('spline',0)\n",
    "# xs = item.get('distances',0)\n",
    "# elevations = item.get('elevations',0)\n",
    "\n",
    "# #get new elevation values\n",
    "# new_xs = np.arange(0,xs[-1],1)\n",
    "# new_elevations = splev(new_xs, spline)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(xs,elevations,'-')\n",
    "# ax.plot(new_xs,new_elevations,'-.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (export_fp / \"spline_fit_elevation.pkl\").open('wb') as fh:\n",
    "    pickle.dump(interpolated_points_dict,fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move on to step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Find underpasses/tunnels and knockout elevated segments\n",
    "- Should have a distinct section that's high above everything else\n",
    "- Need to also bring in railroads\n",
    "- Can also be tagged as tunnels\n",
    "# #grab ones that are not bridges and don't exceed the threshold\n",
    "# not_bridges = exceeds_threshold.loc[exceeds_threshold['bridge'].isna(),['id','geometry']]\n",
    "# print(len(not_bridges),'of thse are not tagged as bridges')\n",
    "# bridges = links.loc[~links['bridge'].isna(),['id','geometry']]\n",
    "Check if bridge crosses non-bridge (won't include where a link connects to a bridge)\n",
    "\n",
    "# crossing_links = []\n",
    "# for idx, row in bridges.iterrows():\n",
    "#     bridge = row['geometry']\n",
    "#     crosses = not_bridges.loc[not_bridges.crosses(bridge),'id'].tolist()\n",
    "#     if len(crosses) > 0:\n",
    "#         crossing_links = crossing_links + crosses\n",
    "# mask = list(set(crossing_links))\n",
    "\n",
    "## Smooth with [Whittaker-Eilers Method](https://towardsdatascience.com/the-perfect-way-to-smooth-your-noisy-data-4f3fe6b44440)\n",
    "\n",
    "Need fitted function for interpolation\n",
    "# for linkid, item in interpolated_points_dict.items():\n",
    "    \n",
    "#     distances = np.array(item['distances'])\n",
    "    \n",
    "#     if linkid in lidar_found:\n",
    "#         elevations = np.array(item['lidar'])\n",
    "#     else: \n",
    "#         elevations = np.array(item['elevations'])\n",
    "\n",
    "#     whittaker_smoother = WhittakerSmoother(\n",
    "#     lmbda=150, order=2, data_length=len(elevations)\n",
    "#     )\n",
    "\n",
    "#     smoothed = whittaker_smoother.smooth(elevations)\n",
    "\n",
    "#     output = elevation_tools.elevation_stats(distances,smoothed,grade_threshold)\n",
    "\n",
    "#     #assign new entry in the dictionary\n",
    "#     interpolated_points_dict[linkid].update({\n",
    "#         'smoothed': smoothed,\n",
    "#         'smoothed_ascent': output['ascent'],\n",
    "#         'smoothed_descent': output['descent'], \n",
    "#         'smoothed_ascent_grade': output['ascent_grade'],\n",
    "#         'smoothed_descent_grade': output['descent_grade'],\n",
    "#         'smoothed_bad_ascent_grades': output['bad_ascent_grades'],\n",
    "#         'smoothed_bad_descent_grades': output['bad_descent_grades'],\n",
    "#         'smoothed_segment_grades': output['segment_grades']\n",
    "#     })\n",
    "\n",
    "# #for storing the interpolated points with sampled elevation data\n",
    "# with (export_fp/'smoothed_elevation.pkl').open('wb') as fh:\n",
    "#     pickle.dump(interpolated_points_dict,fh)\n",
    "## Measuring Grade\n",
    "Length of grade + grade matters. Grade is change in elevation over a length of road. \n",
    "- Grade can be averaged over the entire link (seperating up and down)\n",
    "- Grade can be averaged over the just the section of up or down (expected grade going uphill)\n",
    "- Grade can be categorized (>3%, >6,% >10,etc) and the length in each category could be calculated\n",
    "\n",
    "If we're just concerned about the impact on travel time then an average value is probably fine, but if we're more concerned about a preference then categorizing and finding the lengths of each is probably more useful.\n",
    "\n",
    "In route choice literature, they're either look at the average grade of the entire route (Hood 2011, Prato 2018) or break it into categories (proportion of the route 2-4% grade) (Broach 2012). Since we're estimating link level impedances, we can be flexible and avoid taking averages if desired.\n",
    "\n",
    "Broach:\n",
    "- 2-4%\n",
    "- 4-6%\n",
    "- more than 6% -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
