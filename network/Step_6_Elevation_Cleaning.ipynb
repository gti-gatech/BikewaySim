{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elevation Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.plot import show\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import box, mapping\n",
    "from shapely.ops import Point\n",
    "from tqdm import tqdm\n",
    "import pyproj\n",
    "import math\n",
    "from shapely.ops import LineString\n",
    "import pickle\n",
    "import contextily as cx\n",
    "\n",
    "# from whittaker_eilers import WhittakerSmoother\n",
    "\n",
    "import src.elevation_tools as elevation_tools\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,str(Path.cwd().parent))\n",
    "import file_structure_setup\n",
    "config = file_structure_setup.filepaths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import non-network version of osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import network\n",
    "links = gpd.read_file(config['osmdwnld_fp'] / f\"osm_{config['geofabrik_year']}.gpkg\",layer='raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproject network to DEM crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_urls = elevation_tools.get_dem_urls(links)\n",
    "dem_crs = rasterio.open(dem_urls[0]).crs\n",
    "links.to_crs(dem_crs,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import sampled elevation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for storing the interpolated points with sampled elevation data\n",
    "with (config['network_fp']/'elevation.pkl').open('rb') as fh:\n",
    "    interpolated_points_dict = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local road that I know has steep grades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 340365816\n",
    "# grade_threshold = 15\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Northside Drive as \"tertiary\" road with a small segment above 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 352003174\n",
    "# grade_threshold = 15\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 44097075\n",
    "# grade_threshold = 30\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeltLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 226119768\n",
    "# grade_threshold = 15\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 741964053\n",
    "# grade_threshold = 30\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Circular golf course loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 1087991070\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Knockout\n",
    "- Calculate segment grade changes using the sampling distance\n",
    "- Define a segment grade threshold by OSM highway type (e.g., motorway, local, secondary, etc.)\n",
    "    - Highways/Interstates > 8%\n",
    "    - Most local roads > 15%\n",
    "    - Few local roads > 25%\n",
    "- Knockout all grade changes that exceed threshold\n",
    "- Repeat until no grade changes are above threshold\n",
    "    - If this removes too many points \n",
    "- Spline fit on the remaining data for the interpolation step\n",
    "\n",
    "Hongyu Method:\n",
    "- Find first grade change (ascent or descent) exceeding threshold\n",
    "- Search 30m, 100m, or 150m after and find the last opposite grade change exceeding threshold\n",
    "- Remove first to last point\n",
    "- Infill with the spline fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Grade Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_thresholds = {\n",
    "    'tunnel': 8,\n",
    "    'bridge': 8,\n",
    "    'motorway': 8,\n",
    "    'local': 20,\n",
    "    'ped': 30,\n",
    "    'exclude': 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the label field\n",
    "links['label'] = None\n",
    "\n",
    "#tunnel\n",
    "links.loc[links['tunnel'].notna() & links['label'].isna(),'label'] = 'tunnel'\n",
    "\n",
    "#bridge\n",
    "links.loc[links['bridge'].notna() & links['label'].isna(),'label'] = 'bridge'\n",
    "\n",
    "#motorway/major arterials\n",
    "motorway = ['motorway','motorway_link',\n",
    "            'trunk', 'trunk_link',\n",
    "            'primary','primary_link',\n",
    "            'secondary','secondary_link',\n",
    "            'raceway', 'proposed']\n",
    "links.loc[links['highway'].isin(motorway) & links['label'].isna(),'label'] = 'motorway'\n",
    "\n",
    "#local/service roads\n",
    "local = ['tertiary','tertiary_link','residential','service', 'unclassified','living_street']\n",
    "links.loc[links['highway'].isin(local) & links['label'].isna(),'label'] = 'local'\n",
    "\n",
    "#pedestrian paths/steps may not follow grade thresholds\n",
    "ped = ['path','footway','track','pedestrian','cycleway','platform']\n",
    "links.loc[links['highway'].isin(ped) & links['label'].isna(),'label'] = 'ped'\n",
    "\n",
    "#label everything else as exclude or place a high value\n",
    "links.loc[links['label'].isna(),'label'] = 'exclude'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 / 60908 tunnel links exceed the threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 66.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 / 60908 bridge links exceed the threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 97.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 / 60908 motorway links exceed the threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 187.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 / 60908 local links exceed the threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 167.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 60908 ped links exceed the threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 60908 exclude links exceed the threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for label, grade_threshold in grade_thresholds.items():\n",
    "    #identify links with grades exceeding the threshold\n",
    "    labelled_links = links.loc[links['label']==label,'osmid'].tolist()\n",
    "    exceeds = elevation_tools.exceeds_threshold(labelled_links,interpolated_points_dict,grade_threshold)\n",
    "    print(len(exceeds),'/',len(interpolated_points_dict),label,'links exceed the threshold')\n",
    "    #for the links that exceed the threshold, do point knockout\n",
    "    for linkid in tqdm(exceeds):\n",
    "        item = interpolated_points_dict.get(linkid,0)\n",
    "        #TODO have a before after measure to jusdge quality\n",
    "        #before = len(item['elevations'])\n",
    "        elevation_tools.point_knockout(item,grade_threshold)\n",
    "        interpolated_points_dict[linkid] = item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spline Fit\n",
    "For all the links, fit a spline for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60908/60908 [00:51<00:00, 1175.11it/s]\n"
     ]
    }
   ],
   "source": [
    "#spline fit\n",
    "import numpy as np\n",
    "from scipy.interpolate import splrep, splev, BSpline\n",
    "\n",
    "too_short = [ ]\n",
    "\n",
    "for key, item in tqdm(interpolated_points_dict.items()):\n",
    "    df = pd.DataFrame({'distance':item['distances'],'elevation':item['elevations']})\n",
    "\n",
    "    #remove na values\n",
    "    df = df[df.notna().all(axis=1)]\n",
    "\n",
    "    #in this case, just do linear interpolation between the two values\n",
    "    if df.shape[0] <= 3:\n",
    "        too_short.append(key)\n",
    "        continue\n",
    "\n",
    "    #fit a spline\n",
    "    spline = splrep(df['distance'], df['elevation'], s=0.5)\n",
    "\n",
    "    #add spline to dict\n",
    "    interpolated_points_dict[key]['spline'] = spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9245034 goes under a railraod track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add a check here\n",
    "# import random\n",
    "\n",
    "# osmid = 751119047#random.choice(list(interpolated_points_dict.keys()))\n",
    "# item = interpolated_points_dict[osmid]\n",
    "\n",
    "# spline = item.get('spline',0)\n",
    "# xs = item.get('distances',0)\n",
    "# elevations = item.get('elevations',0)\n",
    "\n",
    "# #get new elevation values\n",
    "# new_xs = np.arange(0,xs[-1],1)\n",
    "# new_elevations = splev(new_xs, spline)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(xs,elevations,'-')\n",
    "# ax.plot(new_xs,new_elevations,'-.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['network_fp'] / \"spline_fit_elevation.pkl\").open('wb') as fh:\n",
    "    pickle.dump(interpolated_points_dict,fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move on to step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Find underpasses/tunnels and knockout elevated segments\n",
    "- Should have a distinct section that's high above everything else\n",
    "- Need to also bring in railroads\n",
    "- Can also be tagged as tunnels\n",
    "# #grab ones that are not bridges and don't exceed the threshold\n",
    "# not_bridges = exceeds_threshold.loc[exceeds_threshold['bridge'].isna(),['id','geometry']]\n",
    "# print(len(not_bridges),'of thse are not tagged as bridges')\n",
    "# bridges = links.loc[~links['bridge'].isna(),['id','geometry']]\n",
    "Check if bridge crosses non-bridge (won't include where a link connects to a bridge)\n",
    "\n",
    "# crossing_links = []\n",
    "# for idx, row in bridges.iterrows():\n",
    "#     bridge = row['geometry']\n",
    "#     crosses = not_bridges.loc[not_bridges.crosses(bridge),'id'].tolist()\n",
    "#     if len(crosses) > 0:\n",
    "#         crossing_links = crossing_links + crosses\n",
    "# mask = list(set(crossing_links))\n",
    "\n",
    "## Smooth with [Whittaker-Eilers Method](https://towardsdatascience.com/the-perfect-way-to-smooth-your-noisy-data-4f3fe6b44440)\n",
    "\n",
    "Need fitted function for interpolation\n",
    "# for linkid, item in interpolated_points_dict.items():\n",
    "    \n",
    "#     distances = np.array(item['distances'])\n",
    "    \n",
    "#     if linkid in lidar_found:\n",
    "#         elevations = np.array(item['lidar'])\n",
    "#     else: \n",
    "#         elevations = np.array(item['elevations'])\n",
    "\n",
    "#     whittaker_smoother = WhittakerSmoother(\n",
    "#     lmbda=150, order=2, data_length=len(elevations)\n",
    "#     )\n",
    "\n",
    "#     smoothed = whittaker_smoother.smooth(elevations)\n",
    "\n",
    "#     output = elevation_tools.elevation_stats(distances,smoothed,grade_threshold)\n",
    "\n",
    "#     #assign new entry in the dictionary\n",
    "#     interpolated_points_dict[linkid].update({\n",
    "#         'smoothed': smoothed,\n",
    "#         'smoothed_ascent': output['ascent'],\n",
    "#         'smoothed_descent': output['descent'], \n",
    "#         'smoothed_ascent_grade': output['ascent_grade'],\n",
    "#         'smoothed_descent_grade': output['descent_grade'],\n",
    "#         'smoothed_bad_ascent_grades': output['bad_ascent_grades'],\n",
    "#         'smoothed_bad_descent_grades': output['bad_descent_grades'],\n",
    "#         'smoothed_segment_grades': output['segment_grades']\n",
    "#     })\n",
    "\n",
    "# #for storing the interpolated points with sampled elevation data\n",
    "# with (export_fp/'smoothed_elevation.pkl').open('wb') as fh:\n",
    "#     pickle.dump(interpolated_points_dict,fh)\n",
    "## Measuring Grade\n",
    "Length of grade + grade matters. Grade is change in elevation over a length of road. \n",
    "- Grade can be averaged over the entire link (seperating up and down)\n",
    "- Grade can be averaged over the just the section of up or down (expected grade going uphill)\n",
    "- Grade can be categorized (>3%, >6,% >10,etc) and the length in each category could be calculated\n",
    "\n",
    "If we're just concerned about the impact on travel time then an average value is probably fine, but if we're more concerned about a preference then categorizing and finding the lengths of each is probably more useful.\n",
    "\n",
    "In route choice literature, they're either look at the average grade of the entire route (Hood 2011, Prato 2018) or break it into categories (proportion of the route 2-4% grade) (Broach 2012). Since we're estimating link level impedances, we can be flexible and avoid taking averages if desired.\n",
    "\n",
    "Broach:\n",
    "- 2-4%\n",
    "- 4-6%\n",
    "- more than 6% -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
