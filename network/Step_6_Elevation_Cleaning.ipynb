{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elevation Cleaning and Assignment to Links\n",
    "Future features if there was time:\n",
    "- Add in more topology checks to identify bridges\n",
    "    - see if road crosses tunnel/river/railroad/etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import box, mapping\n",
    "from shapely.ops import Point\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from bikewaysim.paths import config\n",
    "from bikewaysim.network import elevation_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import non-network version of osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import network\n",
    "links = gpd.read_file(config['osmdwnld_fp'] / f\"osm.gpkg\",layer='raw')\n",
    "\n",
    "#reproject network to DEM crs\n",
    "with (config['network_fp']/'dem_crs.txt').open('r') as fh:\n",
    "    dem_crs = fh.read()\n",
    "links.to_crs(dem_crs,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import sampled elevation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for storing the interpolated points with sampled elevation data\n",
    "with (config['network_fp']/'elevation_w_lidar.pkl').open('rb') as fh:\n",
    "    interpolated_points_dict = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove links if no elevation data found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some below zero elevations near the airport\n",
    "error = []\n",
    "for linkid, item in interpolated_points_dict.items():\n",
    "    if (item['elevations'] < 0).any():\n",
    "        error.append(linkid)\n",
    "len(error)\n",
    "# links[links['osmid'].isin(error)].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_points_dict = {key:item for key,item in interpolated_points_dict.items() if key not in error}\n",
    "links = links[links['osmid'].isin(interpolated_points_dict.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_elev = np.max(np.array([item['elevations'].max() for key, item in interpolated_points_dict.items()]))\n",
    "min_elev = np.min(np.array([item['elevations'].min() for key, item in interpolated_points_dict.items()]))\n",
    "print('Max Elevation:',max_elev,'m','Min Elevation:',min_elev,'m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many of the low elevation points near rivers\n",
    "# min_elevs = [key for key, item in interpolated_points_dict.items() if any(item['elevations'] < 250) ]\n",
    "# links[links['osmid'].isin(min_elevs)].explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 79424672\n",
    "# grade_threshold = 10\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 42106698\n",
    "# grade_threshold = 20\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 26800206\n",
    "# grade_threshold = 20\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolated_points_dict[linkid] = elevation_tools.point_knockout(interpolated_points_dict[linkid],8)\n",
    "# interpolated_points_dict[linkid]['elevations']\n",
    "# linkid = 26800206\n",
    "# grade_threshold = 8\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)\n",
    "# x = interpolated_points_dict[linkid]['distances']\n",
    "# y = interpolated_points_dict[linkid]['elevations']\n",
    "# test = elevation_tools.elevation_stats(x,y,80)\n",
    "# test.keys()\n",
    "# test['descent_grade']\n",
    "# test['bad_ascent_grades']\n",
    "# test['bad_descent_grades']\n",
    "# test['distance_deltas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Local road that I know has steep grades: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 340365816\n",
    "# grade_threshold = 15\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True,lidar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Northside Drive as \"tertiary\" road with a small segment above 15% -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 352003174\n",
    "# grade_threshold = 15\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True,lidar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Unpaved trail -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 1087991070\n",
    "# grade_threshold = 30\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BeltLine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 226119768\n",
    "# grade_threshold = 15\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 741964053\n",
    "# grade_threshold = 30\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Circular golf course loop -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 1087991070\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkid = 569529892\n",
    "# grade_threshold = 4\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True,lidar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace dem elevation values with lidar where available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, item in interpolated_points_dict.items():\n",
    "    if 'lidar' in item.keys():\n",
    "        lidar_elev = item['lidar']\n",
    "        dem_elev = item['elevations']\n",
    "        new_elev = [a if not np.isnan(a) else b for a, b in zip(lidar_elev,dem_elev)]\n",
    "        interpolated_points_dict[key]['elevations'] = np.array(new_elev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Knockout\n",
    "- Calculate segment grades (rise/10m or rise/length if link length was less than 10m)\n",
    "- Define a segment grade threshold by OSM highway type (e.g., motorway, local, secondary, etc.)\n",
    "    - Highways/Interstates > 8%\n",
    "    - Most local roads > 15%\n",
    "    - Few local roads > 25%\n",
    "- Knockout elevations where the associated grade changes exceeds the threshold\n",
    "- Repeat until no grade changes are above the set threshold\n",
    "    - If threshold is too low this will remove too many points\n",
    "    - Just start and end will be used \n",
    "- Spline fit on the remaining data for the interpolation step\n",
    "\n",
    "How it differs from Hongyu's Method:\n",
    "- Find first grade change (ascent or descent) exceeding threshold\n",
    "- Search 30m, 100m, or 150m after and find the last opposite grade change exceeding threshold\n",
    "- Remove first to last point\n",
    "- Infill with the spline fit\n",
    "- Our segments are too short for this but this approach could be explored in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Grade Thresholds\n",
    "Don't set too high of a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_thresholds = {\n",
    "    'tunnel': 8,\n",
    "    'bridge': 8,\n",
    "    'roads': 8,\n",
    "    'local': 20,\n",
    "    'bikeped': 20,\n",
    "    'everything_else': 25\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the label field\n",
    "links['label'] = None\n",
    "\n",
    "#tunnel\n",
    "links.loc[links['tunnel'].notna() & links['label'].isna(),'label'] = 'tunnel'\n",
    "\n",
    "#bridge\n",
    "links.loc[links['bridge'].notna() & links['label'].isna(),'label'] = 'bridge'\n",
    "\n",
    "#motorway/major arterials\n",
    "motorway = ['motorway','motorway_link',\n",
    "            'trunk', 'trunk_link',\n",
    "            'primary','primary_link',\n",
    "            'secondary','secondary_link',\n",
    "            'raceway', 'proposed','tertiary','tertiary_link','service', 'unclassified','living_street']\n",
    "links.loc[links['highway'].isin(motorway) & links['label'].isna(),'label'] = 'roads'\n",
    "\n",
    "#local/service roads\n",
    "local = ['residential']\n",
    "links.loc[links['highway'].isin(local) & links['label'].isna(),'label'] = 'local'\n",
    "\n",
    "#pedestrian paths/steps may not follow grade thresholds\n",
    "bikeped = ['path','footway','pedestrian','cycleway']\n",
    "links.loc[links['highway'].isin(bikeped) & links['label'].isna(),'label'] = 'bikeped'\n",
    "\n",
    "#label everything else as exclude or place a high value\n",
    "links.loc[links['label'].isna(),'label'] = 'everything_else'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(elevation_tools)\n",
    "for label, grade_threshold in grade_thresholds.items():\n",
    "    #identify links with grades exceeding the threshold\n",
    "    labelled_links = links.loc[links['label']==label,'osmid'].tolist()\n",
    "    exceeds = elevation_tools.exceeds_threshold(labelled_links,interpolated_points_dict,grade_threshold)\n",
    "    print(len(exceeds),'/',len(interpolated_points_dict),label,'links exceed the threshold')\n",
    "    #for the links that exceed the threshold, do point knockout\n",
    "    for linkid in tqdm(exceeds):\n",
    "        item = interpolated_points_dict.get(linkid,0)\n",
    "        item = elevation_tools.point_knockout(item,grade_threshold)\n",
    "        interpolated_points_dict[linkid] = item\n",
    "check = [key for key, item in interpolated_points_dict.items() if np.isnan(item['elevations']).any()]\n",
    "print(len(check),'links had at least one point knocked out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check some of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "linkid = random.choice(check)\n",
    "print(linkid)\n",
    "print(interpolated_points_dict[linkid]['elevations'])\n",
    "grade_threshold = 10\n",
    "elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export into QGIS to examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = {linkid:elevation_tools.simple_elevation_stats(item['distances'],item['elevations']) for linkid, item in interpolated_points_dict.items()}\n",
    "export = pd.DataFrame.from_dict(export,orient='index')\n",
    "\n",
    "df = pd.merge(links,export,left_on='osmid',right_index=True)\n",
    "df['ascent_ft'] = df['ascent_m'] * 3.28084\n",
    "df['descent_ft'] = df['descent_m'] * 3.28084\n",
    "\n",
    "df['max_grade'] = np.max(np.abs(df[['ascent_grade_%','descent_grade_%']].values),axis=1)\n",
    "gdf = gpd.GeoDataFrame(df,crs=dem_crs)\n",
    "gdf.to_crs(config['projected_crs_epsg'],inplace=True)\n",
    "gdf.to_file(Path.home()/'Downloads/scratch.gpkg',layer='raw_grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf[(gdf['ascent_m']>100) | (gdf['descent_m']>100)].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.loc[gdf['max_grade']>20].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spline Fit\n",
    "For all the links, fit a spline for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #spline fit\n",
    "# import numpy as np\n",
    "# from scipy.interpolate import splrep, splev, BSpline\n",
    "\n",
    "# key = random.choice(list(interpolated_points_dict.keys()))\n",
    "# item = interpolated_points_dict[key]\n",
    "\n",
    "# too_short = [ ]\n",
    "\n",
    "# df = pd.DataFrame({'distance':item['distances'],'elevation':item['elevations']})\n",
    "\n",
    "# #remove na values\n",
    "# df = df[df.notna().all(axis=1)]\n",
    "\n",
    "# #in this case, just do linear interpolation between the two values\n",
    "# # if df.shape[0] <= 3:\n",
    "# #     too_short.append(key)\n",
    "# #     continue\n",
    "\n",
    "# #fit a spline\n",
    "# spline = splrep(df['distance'], df['elevation'], s=0.5)\n",
    "\n",
    "# #add spline to dict\n",
    "# # interpolated_points_dict[key]['spline'] = spline\n",
    "\n",
    "# #TODO add this feature\n",
    "# #get smoothed elevations\n",
    "# #get new elevation values\n",
    "# # new_xs = np.arange(0,xs[-1],1)\n",
    "\n",
    "# new_elevations = splev(item['distances'], spline)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(df['distance'],df['elevation'],'-')\n",
    "# ax.plot(item['distances'],new_elevations,'-.')\n",
    "# ax.set_ylim(min_elev,max_elev)\n",
    "\n",
    "# interpolated_points_dict[key]['smoothed'] = new_elevations\n",
    "\n",
    "# grade_threshold = 15\n",
    "# linkid = key\n",
    "# elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True,lidar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spline fit\n",
    "import numpy as np\n",
    "from scipy.interpolate import splrep, splev, BSpline\n",
    "\n",
    "too_short = [ ]\n",
    "\n",
    "for key, item in tqdm(interpolated_points_dict.items()):\n",
    "    df = pd.DataFrame({'distance':item['distances'],'elevation':item['elevations']})\n",
    "\n",
    "    #remove na values\n",
    "    df = df[df.notna().all(axis=1)]\n",
    "\n",
    "    #in this case, just do linear interpolation between the two values\n",
    "    if df.shape[0] <= 3:\n",
    "        too_short.append(key)\n",
    "        continue\n",
    "\n",
    "    #fit a spline\n",
    "    spline = splrep(df['distance'], df['elevation'], s=0.5)\n",
    "\n",
    "    #add spline to dict\n",
    "    interpolated_points_dict[key]['spline'] = spline\n",
    "\n",
    "    #TODO add this feature\n",
    "    #get smoothed elevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(too_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_points_dict[too_short[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links[links['osmid'].isin(too_short)].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['network_fp'] / \"spline_fit_elevation.pkl\").open('wb') as fh:\n",
    "    pickle.dump(interpolated_points_dict,fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move on to step 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Find underpasses/tunnels and knockout elevated segments\n",
    "# - Should have a distinct section that's high above everything else\n",
    "# - Need to also bring in railroads\n",
    "# - Can also be tagged as tunnels\n",
    "# # #grab ones that are not bridges and don't exceed the threshold\n",
    "# # not_bridges = exceeds_threshold.loc[exceeds_threshold['bridge'].isna(),['id','geometry']]\n",
    "# # print(len(not_bridges),'of thse are not tagged as bridges')\n",
    "# # bridges = links.loc[~links['bridge'].isna(),['id','geometry']]\n",
    "# Check if bridge crosses non-bridge (won't include where a link connects to a bridge)\n",
    "\n",
    "# # crossing_links = []\n",
    "# # for idx, row in bridges.iterrows():\n",
    "# #     bridge = row['geometry']\n",
    "# #     crosses = not_bridges.loc[not_bridges.crosses(bridge),'id'].tolist()\n",
    "# #     if len(crosses) > 0:\n",
    "# #         crossing_links = crossing_links + crosses\n",
    "# # mask = list(set(crossing_links))\n",
    "\n",
    "# ## Smooth with [Whittaker-Eilers Method](https://towardsdatascience.com/the-perfect-way-to-smooth-your-noisy-data-4f3fe6b44440)\n",
    "\n",
    "# Need fitted function for interpolation\n",
    "# # for linkid, item in interpolated_points_dict.items():\n",
    "    \n",
    "# #     distances = np.array(item['distances'])\n",
    "    \n",
    "# #     if linkid in lidar_found:\n",
    "# #         elevations = np.array(item['lidar'])\n",
    "# #     else: \n",
    "# #         elevations = np.array(item['elevations'])\n",
    "\n",
    "# #     whittaker_smoother = WhittakerSmoother(\n",
    "# #     lmbda=150, order=2, data_length=len(elevations)\n",
    "# #     )\n",
    "\n",
    "# #     smoothed = whittaker_smoother.smooth(elevations)\n",
    "\n",
    "# #     output = elevation_tools.elevation_stats(distances,smoothed,grade_threshold)\n",
    "\n",
    "# #     #assign new entry in the dictionary\n",
    "# #     interpolated_points_dict[linkid].update({\n",
    "# #         'smoothed': smoothed,\n",
    "# #         'smoothed_ascent': output['ascent'],\n",
    "# #         'smoothed_descent': output['descent'], \n",
    "# #         'smoothed_ascent_grade': output['ascent_grade'],\n",
    "# #         'smoothed_descent_grade': output['descent_grade'],\n",
    "# #         'smoothed_bad_ascent_grades': output['bad_ascent_grades'],\n",
    "# #         'smoothed_bad_descent_grades': output['bad_descent_grades'],\n",
    "# #         'smoothed_segment_grades': output['segment_grades']\n",
    "# #     })\n",
    "\n",
    "# # #for storing the interpolated points with sampled elevation data\n",
    "# # with (export_fp/'smoothed_elevation.pkl').open('wb') as fh:\n",
    "# #     pickle.dump(interpolated_points_dict,fh)\n",
    "# ## Measuring Grade\n",
    "# Length of grade + grade matters. Grade is change in elevation over a length of road. \n",
    "# - Grade can be averaged over the entire link (seperating up and down)\n",
    "# - Grade can be averaged over the just the section of up or down (expected grade going uphill)\n",
    "# - Grade can be categorized (>3%, >6,% >10,etc) and the length in each category could be calculated\n",
    "\n",
    "# If we're just concerned about the impact on travel time then an average value is probably fine, but if we're more concerned about a preference then categorizing and finding the lengths of each is probably more useful.\n",
    "\n",
    "# In route choice literature, they're either look at the average grade of the entire route (Hood 2011, Prato 2018) or break it into categories (proportion of the route 2-4% grade) (Broach 2012). Since we're estimating link level impedances, we can be flexible and avoid taking averages if desired.\n",
    "\n",
    "# Broach:\n",
    "# - 2-4%\n",
    "# - 4-6%\n",
    "# - more than 6%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
