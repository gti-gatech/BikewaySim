{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Prepare REVISED 4/08/24\n",
    "In this step, we add signals, bicycle facilities (with dates), and elevation attributes (avg up/down slop), remove links where cyclists are not allowed, and create a psuedo graph edge list for modeling turns.\n",
    "\n",
    "Further refinement of the attributes for impedance calibration pruposes should be done in the `impedance_calibration` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import src.modeling_turns as modeling_turns\n",
    "import src.add_attributes as add_attributes\n",
    "import src.prepare_network as prepare_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = json.load((Path.cwd().parent / 'config.json').open('rb'))\n",
    "network_fp = Path(config['project_directory']) / \"Network\"\n",
    "osmdwnld_fp = Path(config['project_directory']) / \"OSM_Download\"\n",
    "elevation_fp = Path(config['project_directory']) / \"Elevation\"\n",
    "cyclinginfra_fp = Path(config['project_directory']) / \"Cycling_Infra_Dating\"\n",
    "export_fp = Path(config['project_directory']) / \"Calibration\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import network links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = gpd.read_file(network_fp/'networks.gpkg',layer='osm_links')\n",
    "nodes = gpd.read_file(network_fp/'networks.gpkg',layer='osm_nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and add attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add attributes back (especially the oneway column)\n",
    "osm_attrs = gpd.read_file(osmdwnld_fp / f\"osm_{config['geofabrik_year']}.gpkg\",layer='raw',ignore_geometry=True)\n",
    "links = pd.merge(links,osm_attrs,left_on='osmid',right_on='osmid')\n",
    "del osm_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn oneway into boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "links['oneway'] = links['oneway'].isin(['yes','-1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add bicycle infrastructure and dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO temporary version while i get the dates added\n",
    "cycling_infra_dates = gpd.read_file(cyclinginfra_fp/'osm_cycleways_w_dates.gpkg',layer='test_dates',ignore_geometry=True)\n",
    "links = pd.merge(links,cycling_infra_dates[['osmid','facility_fwd','facility_rev','year']],on='osmid',how='left')\n",
    "del cycling_infra_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Bike Ottawa LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lts_paths = (Path.home() / \"Documents/GitHub/stressmodel/data\").glob(\"*.json\")\n",
    "lts_list = []\n",
    "for lts_path in lts_paths:\n",
    "    lts_edges = gpd.read_file(lts_path,ignore_geometry=True)\n",
    "    lts_edges['osmid'] = lts_edges['id'].str.split('/').apply(lambda x: x[1]).astype(int)\n",
    "    lts = int(lts_path.name.split('_')[1].split('.')[0])\n",
    "    lts_edges['lts'] = lts\n",
    "    lts_list.append(lts_edges[['osmid','lts']])\n",
    "lts_df = pd.concat(lts_list)\n",
    "links = pd.merge(links,lts_df,on='osmid',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add elevation data\n",
    "Assign the correct direction, then flip signs for reverse links later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation = gpd.read_file(network_fp/'elevation.gpkg',layer='elevation',ignore_geometry=True)\n",
    "ascent_columns = [col for col in elevation.columns if \"ascent\" in col]\n",
    "descent_columns = [col for col in elevation.columns if \"descent\" in col]\n",
    "links = pd.merge(links,elevation[['osm_linkid','reverse_geometry']+ascent_columns+descent_columns],on='osm_linkid',how='left')\n",
    "del elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPDATE: fixed the reverse links bug so no longer need to flip the geometry here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #first take the absolute value then flip the elevations to match the geometry\n",
    "# #TODO do this in the elevation script instead\n",
    "# links.loc[:,ascent_columns+descent_columns] = links.loc[:,ascent_columns+descent_columns].abs()\n",
    "# links.loc[links['reverse_geometry']==True,ascent_columns+descent_columns] = links.loc[links['reverse_geometry']==True,descent_columns+ascent_columns].values\n",
    "\n",
    "# #have to do this with bike facilities too\n",
    "# #TODO, at some point figure this part out during network filtering\n",
    "# #ONEWAY links do not have this issue\n",
    "# links.loc[links['reverse_geometry']==True,['facility_fwd','facility_rev']] = links.loc[links['reverse_geometry']==True,['facility_rev','facility_fwd']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actually reverse the underlying geometry so it doesn't mess up the turns\n",
    "#links.loc[links['reverse_geometry']==True,'geometry'] = links['geometry'].apply(lambda geom: geom.reverse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.rename(columns={'osm_A':'A','osm_B':'B','osm_linkid':'linkid'},inplace=True)\n",
    "nodes.rename(columns={'osm_N':'N'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate link lengths\n",
    "links['length_ft'] = links.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove non-routable links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "links['link_type'].unique()\n",
    "keep_type = ['road', 'service', 'parking_and_driveways', 'pedestrian', 'bike', 'sidewalk_or_crossing']\n",
    "links = links[links['link_type'].isin(keep_type)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove isolated links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before connected components: Links 179981 Nodes 150882\n",
      "After connected components: Links 178485 Nodes 135741\n"
     ]
    }
   ],
   "source": [
    "links, nodes = prepare_network.largest_comp_and_simplify(links,nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create reverse links and turn dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO change this to not create the turn graph (just make it an extra optional step)\n",
    "## Create turn graph dataframe\n",
    "directed_links, turns_df = modeling_turns.create_pseudo_dual_graph(links,'A','B','linkid','oneway')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add signals (do later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source_A', 'source_B', 'target_A', 'target_B', 'source_linkid',\n",
       "       'source_reverse_link', 'source_azimuth', 'target_linkid',\n",
       "       'target_reverse_link', 'target_azimuth', 'azimuth_change', 'turn_type',\n",
       "       'signalized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_signals = pd.read_parquet(network_fp/'osm_signals.parquet')\n",
    "osm_signals.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "turns_df = pd.merge(turns_df,osm_signals[['source_linkid','source_reverse_link','target_linkid','target_reverse_link','signalized']],on=['source_linkid','source_reverse_link','target_linkid','target_reverse_link'],how='left')\n",
    "turns_df.loc[turns_df['signalized'].isna(),'signalized'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding stressful intersections\n",
    "# import pandas as pd\n",
    "# highway_order = {\n",
    "#     'trunk': 0,\n",
    "#     'trunk_link': 1,\n",
    "#     'primary': 2,\n",
    "#     'primary_link': 3,\n",
    "#     'secondary': 4,\n",
    "#     'secondary_link': 5,\n",
    "#     'tertiary': 6,\n",
    "#     'tertiary_link': 7,\n",
    "#     'unclassified': 8,\n",
    "#     'residential': 9\n",
    "# }\n",
    "# highway_order = pd.Series(highway_order)\n",
    "# highway_order = highway_order.reset_index()\n",
    "# highway_order.columns = ['highway','order']\n",
    "# #add highway ranking based on the above\n",
    "# pseudo_df['target_highway_order'] = pseudo_df['target_highway'].map(highway_order.set_index('highway')['order'])\n",
    "# pseudo_df['source_highway_order'] = pseudo_df['source_highway'].map(highway_order.set_index('highway')['order'])\n",
    "# #remove straight and uturn\n",
    "# cond1 = pseudo_df['turn_type'].isin(['left','right'])\n",
    "# #only road to road for now\n",
    "# cond2 = (pseudo_df['source_link_type'] == 'road') & (pseudo_df['target_link_type'] == 'road')\n",
    "# cross_streets = pseudo_df[cond1 & cond2]\n",
    "\n",
    "# #use groupby to find the max target_highway order\n",
    "# cross_streets = cross_streets.groupby(['source_linkid','source_A','source_B'])['target_highway_order'].min()\n",
    "# cross_streets.name = 'cross_street'\n",
    "\n",
    "# #add to main df\n",
    "# pseudo_df = pd.merge(pseudo_df,cross_streets,left_on=['source_linkid','source_A','source_B'],right_index=True,how='left')\n",
    "\n",
    "# #change numbers back to normal\n",
    "# pseudo_df['cross_street_order'] = pseudo_df['cross_street']\n",
    "# pseudo_df['cross_street'] = pseudo_df['cross_street'].map(highway_order.set_index('order')['highway'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add directional attributes and flip as needed\n",
    "directed_links = pd.merge(directed_links,links[['linkid','facility_fwd','facility_rev']+ascent_columns+descent_columns],on='linkid')\n",
    "directed_links.loc[directed_links['reverse_link']==True,ascent_columns+descent_columns] = directed_links.loc[directed_links['reverse_link']==True,descent_columns+ascent_columns].values\n",
    "directed_links.loc[directed_links['reverse_link']==True,['facility_fwd','facility_rev']] = directed_links.loc[directed_links['reverse_link']==True,['facility_rev','facility_fwd']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "turns_df.to_parquet(network_fp/'turns_df.parquet')\n",
    "directed_links.to_parquet(network_fp/'directed_edges.parquet')\n",
    "links.to_file(network_fp/'final_network.gpkg',layer='edges')\n",
    "nodes.to_file(network_fp/'final_network.gpkg',layer='nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional add geo data to turns and export for examination\n",
    "from shapely.ops import MultiLineString\n",
    "geo_dict = dict(zip(links['linkid'],links['geometry']))\n",
    "turns_df['source_geo'] = turns_df['source_linkid'].map(geo_dict)\n",
    "turns_df['target_geo'] = turns_df['target_linkid'].map(geo_dict)\n",
    "turns_df['geometry'] = turns_df.apply(lambda row: MultiLineString([row['source_geo'],row['target_geo']]),axis=1)\n",
    "turns_df.drop(columns=['source_geo','target_geo'],inplace=True)\n",
    "turns_gdf = gpd.GeoDataFrame(turns_df,crs=links.crs)\n",
    "turns_gdf.drop(columns=['source','target']).to_file(network_fp/'final_network.gpkg',layer='turns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO serialize the attributes to add as needed?\n",
    "# with (network_fp / 'edges_with_attributes.pkl').open('wb') as fh:\n",
    "#     pickle.dump(links,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with (network_fp / 'edges.pkl').open('wb') as fh:\n",
    "#     pickle.dump(links,fh,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with (network_fp / 'nodes.pkl').open('wb') as fh:\n",
    "#     pickle.dump(nodes,fh,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with (network_fp / 'directed_edges.pkl').open('wb') as fh:\n",
    "#     pickle.dump(edges,fh,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with (network_fp / 'turn_df.pkl').open('wb') as fh:\n",
    "#     pickle.dump(turn_df,fh,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated past here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add attributes back and then flip elevation/bicyccle attributes\n",
    "#do so i don't have to re-flip everytime i import? could potentially save memory though\n",
    "#TODO it would still be smarter to store as a dict or something\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.merge(edges,links.drop(columns=['A','B']),on='linkid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if reverse_geo == true then ascent should be descent and vice versa\n",
    "# loops have reverse_geometry is np.nana\n",
    "# assume that all elevation columns will be paired by what is after ascent/descent\n",
    "elevation_columns = ['ascent_m', 'descent_m', 'ascent_grade_%','descent_grade_%']\n",
    "# Remove elements containing \"ascent\" or \"descent\"\n",
    "cleaned_columns = [col for col in elevation_columns if \"ascent\" not in col and \"descent\" not in col]    \n",
    "# Remove duplicates by converting the list to a set and back to a list\n",
    "cleaned_columns = list(set(cleaned_columns))\n",
    "\n",
    "for cleaned_column in cleaned_columns:\n",
    "    #swap if reverse geometry == true\n",
    "    links.loc[links['reverse_geometry']==True,ascent_columns+descent_columns] = links.loc[links['reverse_geometry']==True,descent_columns+ascent_columns]\n",
    "    \n",
    "    \n",
    "    df_edges[] = np.where(df_edges['reverse_link'], df_edges[elev_columns[1]].abs(), df_edges[elev_columns[0]])\n",
    "    #drop the down version?\n",
    "    df_edges.drop(columns=elev_columns[1],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if reverse_geo == true then ascent should be descent and vice versa\n",
    "# loops have reverse_geometry is np.nan\n",
    "# assume that all elevation columns will be paired by what is after ascent/descent\n",
    "elevation_columns = ['ascent_m', 'descent_m', 'ascent_grade_%','descent_grade_%']\n",
    "# Remove elements containing \"ascent\" or \"descent\"\n",
    "cleaned_columns = [col for col in elevation_columns if \"ascent\" not in col and \"descent\" not in col]    \n",
    "# Remove duplicates by converting the list to a set and back to a list\n",
    "cleaned_columns = list(set(cleaned_columns))\n",
    "\n",
    "for cleaned_column in cleaned_columns:\n",
    "    #swap\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_edges[] = np.where(df_edges['reverse_link'], df_edges[elev_columns[1]].abs(), df_edges[elev_columns[0]])\n",
    "    #drop the down version?\n",
    "    df_edges.drop(columns=elev_columns[1],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.rename(columns={'osm_A':'A','osm_B':'B','osm_linkid':'linkid'},inplace=True)\n",
    "nodes.rename(columns={'osm_N':'N'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create turn graph dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges, turn_df = modeling_turns.create_pseudo_dual_graph(links,'A','B','linkid','oneway')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flip attributes if needed (elevation, bicycle facilities)\n",
    "Turns should be good as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add geo (needed for map matching part)\n",
    "df_edges = df_edges.merge(links.drop(columns=['A','B']),on=['linkid'])\n",
    "df_edges = gpd.GeoDataFrame(df_edges,geometry='geometry',crs=links.crs)\n",
    "df_edges = df_edges.loc[:,~df_edges.columns.duplicated()].copy()\n",
    "df_edges.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just export the df_edges?\n",
    "df_edges.to_file(export_fp/'Map_Matching/matching.gpkg',layer='edges')\n",
    "nodes.to_file(export_fp/'Map_Matching/matching.gpkg',layer='nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add geo to the turns too\n",
    "from shapely.ops import MultiLineString\n",
    "pseudo_df = pseudo_df.merge(links[['linkid','geometry']],left_on='source_linkid',right_on='linkid')\n",
    "pseudo_df = pseudo_df.merge(links[['linkid','geometry']],left_on='target_linkid',right_on='linkid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = pseudo_df.apply(lambda row: MultiLineString([row['geometry_x'],row['geometry_y']]),axis=1)\n",
    "pseudo_df.drop(columns=['geometry_x','geometry_y','linkid_x','linkid_y'],inplace=True)\n",
    "pseudo_df = gpd.GeoDataFrame(pseudo_df,geometry=geometry,crs=links.crs)\n",
    "\n",
    "# pseudo_edges = pseudo_edges.loc[:,~pseudo_edges.columns.duplicated()].copy()\n",
    "# pseudo_edges.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_df['source'] = pseudo_df['source'].astype(str)\n",
    "pseudo_df['target'] = pseudo_df['target'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_df.to_file(export_fp/'Map_Matching/matching.gpkg',layer='turns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle the graph\n",
    "with (export_fp / 'Map_Matching/turn_G.pkl').open('wb') as fh:\n",
    "    pickle.dump(pseudo_G,fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Come back to below later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Prepare\n",
    "This notebook prepares the final routing network.\n",
    "\n",
    "1. Import the desired routing network\n",
    "1. Add attributes\n",
    "1. Add reconciled attributes\n",
    "1. Add signals\n",
    "1. Add elevation\n",
    "\n",
    "Then the network will be turned into a directed network graph complete with an edge list representing the directed edges and another one representing turns. Some attribute values are reversed to account for direction (e.g., elevation, signals)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data from previous notebooks and merge them. Merge here so updates can be done at each step without having to repeat everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_filepath = Path.home() / \"Documents/BikewaySimData/Projects/gdot/networks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered data\n",
    "links = gpd.read_file(network_filepath/'filtered.gpkg',layer='osm_links')\n",
    "nodes = gpd.read_file(network_filepath/'filtered.gpkg',layer='osm_nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add osm data\n",
    "links = add_attributes.add_osm_attr(links,network_filepath / 'osm_attr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename\n",
    "links.rename(columns={'osm_A':'A','osm_B':'B','osm_linkid':'linkid'},inplace=True)\n",
    "nodes.rename(columns={'osm_N':'N'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconciled data\n",
    "reconciled = gpd.read_file(network_filepath/'reconciled.gpkg',layer='links',ignore_geometry=True)\n",
    "#[col for col in reconciled.columns if col not in links.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['osm_linkid','speedlimit_range_mph','lanes_per_direction']\n",
    "links = links.merge(reconciled[cols_to_keep],on='osm_linkid',how='left')\n",
    "del reconciled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename\n",
    "links.rename(columns={'osm_A':'A','osm_B':'B','osm_linkid':'linkid'},inplace=True)\n",
    "nodes.rename(columns={'osm_N':'N'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#signals added\n",
    "links_w_signals = gpd.read_file(network_filepath/'signals_added.gpkg',layer='links',ignore_geometry=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_w_signals = gpd.read_file(network_filepath/'signals_added.gpkg',layer='nodes',ignore_geometry=True)\n",
    "nodes_w_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO change linkid to osm_linkid later\n",
    "cols_to_keep = ['linkid','signal_A','signal_B']\n",
    "links = links.merge(links_w_signals[cols_to_keep],on='linkid',how='left')\n",
    "##del nodes_w_signals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elevation added\n",
    "links_w_elevation = gpd.read_file(network_filepath/'elevation_added.gpkg',ignore_geometry=True)\n",
    "links_w_elevation.columns\n",
    "links_w_elevation.rename(columns={\n",
    "    'a_s_c_e_n_t___m':'ascent_m',\n",
    "    'd_e_s_c_e_n_t___m':'descent_m',\n",
    "    'a_s_c_e_n_t___g_r_a_d_e':'ascent_grade',\n",
    "    'd_e_s_c_e_n_t___g_r_a_d_e':'descent_grade',\n",
    "}, inplace =True)\n",
    "cols_to_keep = ['linkid','ascent_m','descent_m','ascent_grade','descent_grade','(0,2]_descent',\n",
    "       '(2,4]_descent', '(4,6]_descent', '(6,10]_descent', '(10,15]_descent',\n",
    "       '(15,inf]_descent', '(0,2]_ascent', '(2,4]_ascent', '(4,6]_ascent',\n",
    "       '(6,10]_ascent', '(10,15]_ascent', '(15,inf]_ascent']\n",
    "links = links.merge(links_w_elevation[cols_to_keep],on='linkid')\n",
    "del links_w_elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = Path.home() / \"Documents/BikewaySimData/Projects/gdot\"\n",
    "edges = gpd.read_file(fp/'networks/elevation_added.gpkg',layer=\"links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use geometry one last time\n",
    "edges['length_ft'] = edges.length\n",
    "\n",
    "#turn bridge and tunnel to boolean values\n",
    "edges['tunnel'] = edges['tunnel'].notna()\n",
    "edges['bridge'] = edges['bridge'].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn bike facil into one column\n",
    "edges['bike_facility_type'] = np.nan\n",
    "edges.loc[(edges['mu'] == 1) & (edges['bike_facility_type'].isna()),'bike_facility_type'] = 'shared-use path'\n",
    "edges.loc[(edges['pbl'] == 1) & (edges['bike_facility_type'].isna()),'bike_facility_type'] = 'protected bike lane'\n",
    "edges.loc[(edges['bl'] == 1) & (edges['bike_facility_type'].isna()),'bike_facility_type'] = 'bike lane'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges, pseudo_df, pseudo_G = modeling_turns.create_pseudo_dual_graph(edges,'A','B','linkid','oneway',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add desired attributes from links to df_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_edges = df_edges.merge(edges[['linkid','geometry']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_cols = ['linkid', 'osmid', 'link_type', 'name', 'oneway','length_ft']\n",
    "\n",
    "#anything that's an instance or would be better as a count value (but not a turn)\n",
    "event_cols = ['bridge','tunnel']\n",
    "\n",
    "#anything that's for the duration of the entire link and has categories\n",
    "category_cols = ['link_type','highway','speedlimit_range_mph',\n",
    "               'lanes_per_direction','bike_facility_type']\n",
    "\n",
    "#reverse in tuple form (these need to be flipped if going the other direction)\n",
    "rev_columns = [('ascent_m','descent_m'),\n",
    "               ('ascent_grade','descent_grade'),\n",
    "               ('(0,2]_ascent','(0,2]_descent'),\n",
    "               ('(2,4]_ascent','(2,4]_descent'),\n",
    "               ('(4,6]_ascent','(4,6]_descent'),\n",
    "               ('(6,10]_ascent','(6,10]_descent'),\n",
    "               ('(10,15]_ascent','(10,15]_descent'),\n",
    "               ('(15,inf]_ascent','(15,inf]_descent')]\n",
    "\n",
    "from itertools import chain\n",
    "keep_cols = basic_cols + event_cols + category_cols + list(chain(*rev_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attrs = ['linkid', 'osmid', 'link_type', 'name', 'highway',\n",
    "#        'bridge', 'tunnel', 'bl', 'pbl', 'mu','speedlimit_range_mph',\n",
    "#        'lanes_per_direction', 'up_grade', 'down_grade', 'length_ft',\n",
    "#        'vehicle_separation','geometry']\n",
    "df_edges = df_edges.merge(edges[keep_cols],on='linkid',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with grade\n",
    "Need to flip sign of grade for reverse links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_up_down_tuples(lst):\n",
    "#     result = []\n",
    "#     current_tuple = []\n",
    "\n",
    "#     for item in lst:\n",
    "#         if 'ascent' in item or 'descent' in item:\n",
    "#             current_tuple.append(item)\n",
    "#             if len(current_tuple) == 2:\n",
    "#                 result.append(tuple(current_tuple))\n",
    "#                 current_tuple = []\n",
    "\n",
    "#     return result\n",
    "\n",
    "# rev_columns = ['ascent_m','descent_m','ascent_grade','descent_grade',\n",
    "#                '(0,2]_down', '(2,4]_down', '(4,6]_down',\n",
    "#                '(6,10]_down', '(10,15]_down','(15,inf]_down',\n",
    "#                '(0,2]_up', '(2,4]_up', '(4,6]_up', '(6,10]_up',\n",
    "#                '(10,15]_up', '(15,inf]_up'\n",
    "#                ]\n",
    "\n",
    "# combined_tuples = combine_up_down_tuples(rev_columns)\n",
    "\n",
    "for elev_columns in rev_columns:\n",
    "    df_edges[elev_columns[0]] = np.where(df_edges['reverse_link'], df_edges[elev_columns[1]].abs(), df_edges[elev_columns[0]])\n",
    "    #drop the down version?\n",
    "    df_edges.drop(columns=elev_columns[1],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turns and Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add additional attributes needed for processing\n",
    "source_links = edges[['linkid','osmid','link_type','name','highway']]\n",
    "target_links = edges[['linkid','osmid','link_type','name','highway']]\n",
    "source_links.columns = 'source_' + source_links.columns\n",
    "target_links.columns = 'target_' + target_links.columns\n",
    "pseudo_df = pseudo_df.merge(source_links,on='source_linkid',how='left')\n",
    "pseudo_df = pseudo_df.merge(target_links,on='target_linkid',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn Restrictions\n",
    "Two types in OSM (represented as OSM relations):\n",
    "- No (blank) turns\n",
    "- Only this turn allowed\n",
    "\n",
    "For chosen we don't need to consider turn restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn_restrictions = pd.read_csv(fp.parent/'osm_turn_restrictions.csv')\n",
    "# pseudo_df = pseudo_df.merge(turn_restrictions,left_on=['source_osmid','target_osmid'],right_on=['from_way_id','to_way_id'],how='left')\n",
    "# road_cond = (pseudo_df['source_link_type'] == 'road') & (pseudo_df['target_link_type'] == 'road')\n",
    "# no_restr = pseudo_df['type'] == 'no'\n",
    "# only_restr = pseudo_df['type'] == 'only'\n",
    "\n",
    "# #add a remove column\n",
    "# pseudo_df['remove'] = False\n",
    "\n",
    "# #remove the no turns\n",
    "# pseudo_df.loc[road_cond & no_restr,'remove'] = True\n",
    "\n",
    "# #for only, find all instances road_cond + from source and set to True\n",
    "# sources = set(turn_restrictions.loc[turn_restrictions['type']=='only','from_way_id'].tolist())\n",
    "# pseudo_df.loc[road_cond & pseudo_df['source_osmid'].isin(sources) & pseudo_df['type'].isna(),'remove'] = True\n",
    "\n",
    "# #Remove these turns and drop the added columns\n",
    "# print((pseudo_df['remove']==True).sum(),'turns removed')\n",
    "# pseudo_df = pseudo_df[pseudo_df['remove']==False]\n",
    "# pseudo_df.drop(columns=['relation_id', 'restriction', 'from_way_id',\n",
    "#        'to_way_id', 'type', 'remove'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with signals\n",
    "Perform two merges and use the source/target reverse link columns to determine which signal ID to keep.\n",
    "- For the source link, use signal_B if reverse == False else signal_A\n",
    "- For the target link, use signal_A if reverse == False else signal_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = pseudo_df[['source_linkid','source_reverse_link']].merge(edges,left_on='source_linkid',right_on='linkid',how='left')\n",
    "pseudo_df['source_signal'] = np.where(source['source_reverse_link'], source['signal_A'], source['signal_B'])\n",
    "\n",
    "target = pseudo_df[['target_linkid','target_reverse_link']].merge(edges,left_on='target_linkid',right_on='linkid',how='left')\n",
    "pseudo_df['target_signal'] = np.where(target['target_reverse_link']==False, target['signal_B'], target['signal_A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying signalized/unsignalized turns\n",
    "- Only look at roads for now\n",
    "- Filter to left/right turns per source linkid per direction\n",
    "- Take the highest road classification and assign it as the cross street road classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "highway_order = {\n",
    "    'trunk': 0,\n",
    "    'trunk_link': 1,\n",
    "    'primary': 2,\n",
    "    'primary_link': 3,\n",
    "    'secondary': 4,\n",
    "    'secondary_link': 5,\n",
    "    'tertiary': 6,\n",
    "    'tertiary_link': 7,\n",
    "    'unclassified': 8,\n",
    "    'residential': 9\n",
    "}\n",
    "highway_order = pd.Series(highway_order)\n",
    "highway_order = highway_order.reset_index()\n",
    "highway_order.columns = ['highway','order']\n",
    "#add highway ranking based on the above\n",
    "pseudo_df['target_highway_order'] = pseudo_df['target_highway'].map(highway_order.set_index('highway')['order'])\n",
    "pseudo_df['source_highway_order'] = pseudo_df['source_highway'].map(highway_order.set_index('highway')['order'])\n",
    "#remove straight and uturn\n",
    "cond1 = pseudo_df['turn_type'].isin(['left','right'])\n",
    "#only road to road for now\n",
    "cond2 = (pseudo_df['source_link_type'] == 'road') & (pseudo_df['target_link_type'] == 'road')\n",
    "cross_streets = pseudo_df[cond1 & cond2]\n",
    "\n",
    "#use groupby to find the max target_highway order\n",
    "cross_streets = cross_streets.groupby(['source_linkid','source_A','source_B'])['target_highway_order'].min()\n",
    "cross_streets.name = 'cross_street'\n",
    "\n",
    "#add to main df\n",
    "pseudo_df = pd.merge(pseudo_df,cross_streets,left_on=['source_linkid','source_A','source_B'],right_index=True,how='left')\n",
    "\n",
    "#change numbers back to normal\n",
    "pseudo_df['cross_street_order'] = pseudo_df['cross_street']\n",
    "pseudo_df['cross_street'] = pseudo_df['cross_street'].map(highway_order.set_index('order')['highway'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Add OSM crossing into this logic\n",
    "    - Signals\n",
    "        - Wait on this until we have the route attributes code done\n",
    "        - Add crossings in signalization\n",
    "        - Majority of crossings are nodes not ways\n",
    "        - Cycleway crossings typically dealt the same way\n",
    "        - If meeting nodes are both crossings and within the traffic signal buffer, they're signalized crossings\n",
    "            - Or if both connecting links are crossings/connect to the road etc\n",
    "        - Way attributes\n",
    "            - Footway = crossing\n",
    "            - Highway = footway\n",
    "        - Node attributes\n",
    "            - Crossing = * (traffic signals/marked/etc)\n",
    "            - Highway = crossing\n",
    "        - Link attributes\n",
    "            - Some links are labeled as crossings but this is not as consistent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signalized = pseudo_df['source_signal'] == pseudo_df['target_signal']\n",
    "left_or_straight =  pseudo_df['turn_type'].isin(['left','straight'])\n",
    "both_road = (pseudo_df['source_link_type'] == 'road') & (pseudo_df['target_link_type'] == 'road')\n",
    "cross_street = pseudo_df['cross_street_order'] <= 5\n",
    "\n",
    "#signalized\n",
    "pseudo_df.loc[signalized & both_road,'signalized'] = True\n",
    "pseudo_df.loc[pseudo_df['signalized'].isna(),'signalized'] = False\n",
    "# pseudo_df.loc[signalized & left_or_straight & both_road,'signalized_left_straight'] = True\n",
    "# pseudo_df.loc[pseudo_df['signalized_left_straight'].isna(),'signalized_left_straight'] = False\n",
    "\n",
    "pseudo_df.loc[-signalized & both_road & cross_street,'unsignalized'] = True\n",
    "pseudo_df.loc[pseudo_df['unsignalized'].isna(),'unsignalized'] = False\n",
    "\n",
    "#clean up\n",
    "rem =  ['source_osmid', 'source_link_type', 'source_name',\n",
    "       'source_highway', 'target_osmid', 'target_link_type', 'target_name',\n",
    "       'target_highway', 'source_signal', 'target_signal',\n",
    "       'target_highway_order', 'source_highway_order', 'cross_street',\n",
    "       'cross_street_order']\n",
    "pseudo_df.drop(columns=rem,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export for impedance calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_edges = gpd.GeoDataFrame(df_edges,crs='epsg:2240')\n",
    "df_edges.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (fp.parent / 'chosen.pkl').open('wb') as fh:\n",
    "    export = (df_edges,pseudo_df,pseudo_G)\n",
    "    pickle.dump(export,fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add geometry to examine results in QGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add geo\n",
    "link_geo = dict(zip(links['linkid'],links['geometry']))\n",
    "pseudo_df['src_geo'] = pseudo_df['source_linkid'].map(link_geo)\n",
    "pseudo_df['trgt_geo'] = pseudo_df['target_linkid'].map(link_geo)\n",
    "pseudo_df['geometry'] = pseudo_df[['src_geo','trgt_geo']].apply(lambda row: MultiLineString([row['src_geo'],row['trgt_geo']]),axis=1)\n",
    "\n",
    "pseudo_df.drop(columns=['src_geo','trgt_geo'],inplace=True)\n",
    "pseudo_df = gpd.GeoDataFrame(pseudo_df,crs=links.crs)\n",
    "\n",
    "pseudo_df['source'] = pseudo_df['source'].astype(str)\n",
    "pseudo_df['target'] = pseudo_df['target'].astype(str)\n",
    "\n",
    "#check results (may need a smaller road network to test on)\n",
    "pseudo_df.to_file(Path.home()/'Downloads/testing.gpkg',layer='cross_streets')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
