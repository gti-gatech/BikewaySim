{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Elevation Data from USGS DEM to OSM with Resample for Bridge Decks from USGS Lidar\n",
    "---\n",
    "Method based on [Liu et al. 2018](https://doi.org/10.1016/j.trc.2018.05.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import contextily as cx\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.plot import show\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import box, mapping\n",
    "from shapely.ops import Point\n",
    "from tqdm import tqdm\n",
    "import pyproj\n",
    "import math\n",
    "import json\n",
    "from shapely.ops import LineString\n",
    "import pickle\n",
    "\n",
    "from bikewaysim.paths import config\n",
    "from bikewaysim.network import elevation_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Golden Gate Bridge Example\n",
    "This is just to give an idea of the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xml.etree.ElementTree as ET\n",
    "# import requests\n",
    "# from shapely.ops import LineString\n",
    "\n",
    "# # Load XML from a URL (assuming it's already fetched by requests)\n",
    "# # If you're reading it directly from a file or string, use open() or string directly.\n",
    "# url = \"https://www.openstreetmap.org/api/0.6/way/595194543/full\"\n",
    "# response = requests.get(url)\n",
    "\n",
    "# # Parse the XML content\n",
    "# root = ET.fromstring(response.content)\n",
    "\n",
    "# # Dictionary to store node coordinates keyed by their ID\n",
    "# node_coords = {}\n",
    "\n",
    "# # First, extract all node coordinates into a dictionary\n",
    "# for node in root.findall(\"node\"):\n",
    "#     node_id = node.attrib['id']\n",
    "#     lat = float(node.attrib['lat'])\n",
    "#     lon = float(node.attrib['lon'])\n",
    "#     node_coords[node_id] = (lon, lat)\n",
    "\n",
    "# # Extract the nodes referred by the 'way' element\n",
    "# way = root.find(\"way\")  # Assuming you only have one 'way' element\n",
    "# ordered_coords = []\n",
    "\n",
    "# # Extract 'nd' elements which contain references to the node IDs\n",
    "# for nd in way.findall(\"nd\"):\n",
    "#     ref = nd.attrib['ref']\n",
    "#     if ref in node_coords:\n",
    "#         # Append the (lat, lon) tuple to the ordered list\n",
    "#         ordered_coords.append(node_coords[ref])\n",
    "\n",
    "# # Turn into linestring and then geodataframe\n",
    "# gg_bridge = gpd.GeoDataFrame({'osmid':595194543,'name':'GG Bridge','geometry':LineString(ordered_coords)},index=[0],crs='epsg:4326')\n",
    "# gg_bridge.set_index('osmid',inplace=True)\n",
    "\n",
    "# from importlib import reload\n",
    "# reload(elevation_tools)\n",
    "# gg_dem_urls,responses = elevation_tools.get_dem_urls(gg_bridge)\n",
    "# print(len(gg_dem_urls),'TIFF files for the provided network')\n",
    "# print(\"Names:\",set([gg_dem_url.split('/')[-3] for gg_dem_url in gg_dem_urls]))\n",
    "\n",
    "# # we only want CA_SanFrancisco_B23\n",
    "# # TODO make a function that plots the bounding boxes of the different products to make it easier to select one\n",
    "# gg_dem_urls = [gg_dem_url for gg_dem_url in gg_dem_urls if 'CA_SanFrancisco' in gg_dem_url ]\n",
    "\n",
    "# # project network to same CRS as DEM\n",
    "# gg_dem_crs = rasterio.open(gg_dem_urls[0]).crs\n",
    "# gg_bridge.to_crs(gg_dem_crs,inplace=True)\n",
    "\n",
    "# #create interpolated points\n",
    "# gg_interpolated_dist_m = 10\n",
    "# gg_interpolated_points_dict = elevation_tools.interpolate_points(gg_bridge,gg_interpolated_dist_m)\n",
    "\n",
    "# #sample values from DEM\n",
    "# for gg_dem_url in tqdm(gg_dem_urls):\n",
    "#     elevation_tools.sample_elevation(gg_dem_url,gg_bridge,gg_interpolated_points_dict)\n",
    "\n",
    "# #get lidar\n",
    "# gg_bridge.to_crs('epsg:4326',inplace=True)\n",
    "# gg_bridge_lidar_urls = elevation_tools.get_lidar_urls(gg_bridge)#[\"https://rockyweb.usgs.gov/vdelivery/Datasets/Staged/Elevation/LPC/Projects/CA_West_Coast_LiDAR_2016_B16/CA_WestCoastElNinoUTM10_2016/LAZ/USGS_LPC_CA_West_Coast_LiDAR_2016_B16_10SEG54454185.laz\"]\n",
    "# print(len(gg_bridge_lidar_urls),'LAZ files for the provided network')\n",
    "# print(\"Names:\",set([lidar_url.split('/')[-3] for lidar_url in gg_bridge_lidar_urls]))\n",
    "\n",
    "# gg_bridge_lidar_urls = [gg_bridge_lidar_url for gg_bridge_lidar_url in gg_bridge_lidar_urls if 'CA_SanFrancisco' in gg_bridge_lidar_url ]\n",
    "# gg_bridge_deck = elevation_tools.get_bridge_decks(gg_bridge_lidar_urls)\n",
    "\n",
    "# gg_bridge_deck.to_crs(gg_dem_crs,inplace=True)\n",
    "\n",
    "# #resample from lidar\n",
    "# elevation_tools.sample_lidar([595194543],gg_interpolated_points_dict,gg_bridge_deck,gg_dem_crs)\n",
    "\n",
    "# gg_bridge.reset_index(inplace=True)\n",
    "# gg_bridge.to_crs(gg_dem_crs,inplace=True)\n",
    "\n",
    "# linkid = 595194543\n",
    "# grade_threshold = 20\n",
    "# elevation_tools.visualize_one(linkid,gg_bridge,gg_dem_crs,gg_interpolated_points_dict,grade_threshold,config['maptilerapikey'],lidar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = gpd.read_file(config['osmdwnld_fp'] / \"osm.gpkg\",layer=\"raw\")\n",
    "links.set_index('osmid',inplace=True)\n",
    "links.to_crs('epsg:4326',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get DEM TIFF download links from [USGS.gov](https://apps.nationalmap.gov/downloader/#/?z=4&y=37.99999999999999&x=-95&basemap=usgs_topo&datasets=&layerIds=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add compression to reduce the file size or make it so that it reads the data live instead of downloading it\n",
    "#TODO change the retrieval method to bounding box instead of unary union\n",
    "dem_urls, response = elevation_tools.get_dem_urls(links)\n",
    "print(len(dem_urls),'TIFF files for the provided network')\n",
    "print(\"Names:\",set([dem_url.split('/')[-3] for dem_url in dem_urls]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choose which product to use from the names (usually pick the one that was uploaded last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Georgia, it'd be GA_Statewide\n",
    "dem_urls = [url for url in dem_urls if 'GA_Statewide' in url]\n",
    "#if you want to download the full DEM files run this\n",
    "#elevation_tools.download_dem(dem_urls,config['usgs_fp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Network to Same CRS as TIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_crs = rasterio.open(dem_urls[0]).crs\n",
    "links.to_crs(dem_crs,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same dem as text\n",
    "with (config['network_fp'] / 'dem_crs.txt').open('w') as file:\n",
    "    file.write(dem_crs.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Settings (Default is 10m)\n",
    "If a link is shorter than this, only the start and end point will be used. Last point will always be included in the profile (e.g., link with length 11 m will be sampled at 0, 10, and 11 meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_dist_m = 10\n",
    "print((links.length < interpolate_dist_m).sum(),'links of',links.shape[0] ,'total were less than',interpolate_dist_m,'meters')\n",
    "print(links[links.length<interpolate_dist_m].length.sum().round(0),'of',links.length.sum().round(0),'meters',round(links[links.length<interpolate_dist_m].length.sum() / links.length.sum() * 100,0),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_points_dict = elevation_tools.interpolate_points(links,interpolate_dist_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample values from DEM (17-25 minutes for study area)\n",
    "Replace the nan values on the interpolated points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dem_url in tqdm(dem_urls):\n",
    "    elevation_tools.sample_elevation(dem_url,links,interpolated_points_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export sampled interpolated points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for storing the interpolated points with sampled elevation data\n",
    "import pickle\n",
    "with (config['network_fp']/'elevation.pkl').open('wb') as fh:\n",
    "    pickle.dump(interpolated_points_dict,fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bridge Deck Elevations from USGS LiDAR\n",
    "Install [laspy and lazrs](https://laspy.readthedocs.io/en/latest/installation.html). This step takes a while (~2 hours) because of a wait and retry timer for the API requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Broken for now. Returns some of the LAZ files from USGS but not all of them. The National Map Viewer Data Download still works though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# reload(elevation_tools)\n",
    "# #comment out after running\n",
    "# links.to_crs('epsg:4326',inplace=True)\n",
    "# lidar_urls = elevation_tools.get_lidar_urls(links)\n",
    "# print(len(lidar_urls),'LAZ files for the provided network')\n",
    "# print(\"Names:\",set([lidar_url.split('/')[-3] for lidar_url in lidar_urls]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bridge_decks = elevation_tools.get_bridge_decks(lidar_urls)\n",
    "# with (config['network_fp']/'bridge_deck_elevations.pkl').open('wb') as fh:\n",
    "#     pickle.dump(bridge_decks,fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Bridges from LiDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['network_fp']/'elevation.pkl').open('rb') as fh:\n",
    "    interpolated_points_dict = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['usgs_fp']/'lidar_points.pkl').open('rb') as fh:\n",
    "    bridge_decks = pickle.load(fh)\n",
    "bridge_decks.to_crs(dem_crs,inplace=True)\n",
    "# # with (config['network_fp']/'bridge_deck_elevations.pkl').open('rb') as fh:\n",
    "#     bridge_decks = pickle.load(fh)\n",
    "# bridge_decks.to_crs(dem_crs,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge_linkids = links[links['bridge'].notna()].index.tolist()\n",
    "elevation_tools.sample_lidar(bridge_linkids,interpolated_points_dict,bridge_decks,dem_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for storing the interpolated points with sampled elevation data\n",
    "import pickle\n",
    "with (config['network_fp']/'elevation_w_lidar.pkl').open('wb') as fh:\n",
    "    pickle.dump(interpolated_points_dict,fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect LiDAR results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_lidar = {key:item['lidar'] for key,item in interpolated_points_dict.items() if 'lidar' in item.keys()}\n",
    "with_lidar = [key for key,item in with_lidar.items() if np.isnan(item).all() == False]\n",
    "links.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from importlib import reload\n",
    "reload(elevation_tools)\n",
    "#pick some examples to visaulize here\n",
    "# linkid = random.choice(with_lidar) # \n",
    "linkid = 38976112\n",
    "grade_threshold = 20\n",
    "elevation_tools.visualize(links,dem_crs,interpolated_points_dict,[linkid],grade_threshold,None,config['maptilerapikey'],one_off=True,lidar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move on to Step 6 (Below code is test code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating grade from sampled elevation\n",
    "# #for storing the interpolated points with sampled elevation data\n",
    "# import pickle\n",
    "# with (network_filepath.parent/'elevation.pkl').open('rb') as fh:\n",
    "#     interpolated_points_dict = pickle.load(fh)\n",
    "# Calculate average up-grade and down-grade for each link using the interpolated points with sampled elevations\n",
    "# # all_rise = {}\n",
    "# # all_down = {}\n",
    "# # max_rise = {}\n",
    "# # min_rise = {}\n",
    "# grade_threshold = 15\n",
    "\n",
    "# for key, item in tqdm(interpolated_points_dict.items(),total=len(interpolated_points_dict)):     \n",
    "\n",
    "#     #find the total distance to get average grade\n",
    "#     total_distance = np.array(item['distances']).max()\n",
    "    \n",
    "#     #caluclate the elevation change between points and add to list\n",
    "#     elevation_deltas = []\n",
    "#     grades = []\n",
    "#     bad_grades = []\n",
    "#     for x in range(0,len(item['elevations'])-1):        \n",
    "#         elev1 = item['elevations'][x]\n",
    "#         elev2 = item['elevations'][x+1]\n",
    "#         dist1 = item['distances'][x]\n",
    "#         dist2 = item['distances'][x+1]\n",
    "#         elevation_delta = (elev2-elev1)\n",
    "#         elevation_deltas.append(elevation_delta)\n",
    "#         distance_delta = (dist2-dist1)\n",
    "\n",
    "#         segment_grade = elevation_delta / distance_delta * 100\n",
    "#         grades.append(segment_grade)\n",
    "\n",
    "#         #flag potentially bad elevation points\n",
    "#         if np.abs(segment_grade) >= grade_threshold:\n",
    "#             bad_grades.append(x)\n",
    "\n",
    "#     #get total up and down\n",
    "#     elevation_deltas = np.asarray(elevation_deltas)\n",
    "#     up = elevation_deltas[elevation_deltas>0].sum()\n",
    "#     down = np.absolute(elevation_deltas[elevation_deltas<0].sum()) #take absolute value at this point\n",
    "\n",
    "#     #get average up and down grade\n",
    "#     up_grade = up / total_distance * 100\n",
    "#     down_grade = down / total_distance * 100\n",
    "    \n",
    "#     interpolated_points_dict[key].update(\n",
    "#         {\n",
    "#             'up': up,\n",
    "#             'down': down, \n",
    "#             'up_grade': up_grade,\n",
    "#             'down_grade': down_grade,\n",
    "#             'elevation_deltas': elevation_deltas,\n",
    "#             'grade_segments': grades,\n",
    "#             'bad_grades': bad_grades\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "# # Tasks\n",
    "# - We have a good way to plot/examine\n",
    "# - We just need to clean the rest of the data\n",
    "# - We have taken out underpasses, so now we should look at bridges and sample lidar stuff and then examine the results\n",
    "# - Once that is done, smooth everything and caclulate the different variables\n",
    "# # Plotting\n",
    "# We want to better visualize the problem and identify the different types of situations:\n",
    "# - Vertical profile with bad grades highlighed\n",
    "#     - Different colors for up/down (arrows?)\n",
    "#     - Ignore vertical exageration for now\n",
    "# - Grade profile just showing the grades with bad ones highlighted (maybe even a dual axis graph)\n",
    "# - Static map of where the link is + bad grades highlighted\n",
    "# - Really, we want to export all the bad figures with a link to the osm way in the title\n",
    "\n",
    "\n",
    "# # use west paces ferry to start with\n",
    "# #linkid = 42901\n",
    "\n",
    "# fig, (ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(12,3))\n",
    "\n",
    "# # for linkid, item in tqdm(interpolated_points_dict.items()):\n",
    "\n",
    "# #     if len(item['bad_grades']) == 0:\n",
    "# #         continue\n",
    "\n",
    "# #     if len(item['distances']) <= 2:\n",
    "# #         continue\n",
    "\n",
    "# cond = True\n",
    "# while cond == True:\n",
    "#     linkid = links['linkid'].sample(1).item()\n",
    "#     item = interpolated_points_dict[linkid]\n",
    "#     if (len(item['bad_grades']) > 0) & (len(item['distances']) > 2):\n",
    "#         cond = False\n",
    "    \n",
    "\n",
    "# #https://github.com/geopandas/geopandas/issues/2279\n",
    "# #https://stackoverflow.com/questions/8247973/how-do-i-specify-an-arrow-like-linestyle-in-matplotlib\n",
    "\n",
    "# #Extract values\n",
    "# x = np.array(item['distances'])\n",
    "# y = np.array(item['elevations'])\n",
    "\n",
    "# #gives you a boolean mask\n",
    "# bad_up_grades = np.array(item['grade_segments']) >= grade_threshold\n",
    "# bad_down_grades = np.array(item['grade_segments']) <= -grade_threshold\n",
    "\n",
    "# #add an additional value\n",
    "# bad_up_grades = np.hstack([np.array((False)),bad_up_grades])\n",
    "# bad_down_grades = np.hstack([np.array((False)),bad_down_grades])\n",
    "\n",
    "# bad_up_x = x[bad_up_grades]\n",
    "# bad_up_y = y[bad_up_grades]\n",
    "\n",
    "# bad_down_x = x[bad_down_grades]\n",
    "# bad_down_y = y[bad_down_grades]\n",
    "\n",
    "# #First Figure\n",
    "# # Plot the original data and the smoothed curve\n",
    "# ax1.plot(x, y, 'o', label='Original Data')\n",
    "# #ax1.plot(x, new_y, 'x',label='Resampled with Lidar')\n",
    "# ax1.grid(True,linestyle='-.')\n",
    "\n",
    "# # plot bad points\n",
    "# ax1.plot(bad_up_x,bad_up_y,'o',color='green',label=f'Above {grade_threshold}%')\n",
    "# ax1.plot(bad_down_x,bad_down_y,'o',color='red',label=f'Below -{grade_threshold}%')\n",
    "\n",
    "# #ax1.set_title(f'{road_name} (Linkid:{linkid}) Veritcal Cross-Section')\n",
    "# ax1.set_xlabel('Distance (m)')\n",
    "# ax1.set_ylabel('Elevation (m)')\n",
    "# ax1.legend()\n",
    "\n",
    "# #Second and Third figure\n",
    "\n",
    "# #create geos to plot\n",
    "# points = np.array([Point(x,y) for x,y in item['geometry']])\n",
    "# line = LineString(item['geometry'])\n",
    "# minx, miny, maxx, maxy = line.bounds\n",
    "# # line_gdf = gpd.GeoDataFrame({'geometry':line},crs=dem_crs,index=[0])\n",
    "\n",
    "# # use mask to just get bad one\n",
    "# bad_up_grades_points = points[bad_up_grades]\n",
    "# bad_up_grades_points = gpd.GeoDataFrame({'geometry':bad_up_grades_points},geometry='geometry',crs=dem_crs)\n",
    "# bad_up_grades_points['type'] = '> 15%'\n",
    "\n",
    "# bad_down_grades_points = points[bad_down_grades]\n",
    "# bad_down_grades_points = gpd.GeoDataFrame({'geometry':bad_down_grades_points},geometry='geometry',crs=dem_crs)\n",
    "# bad_down_grades_points['type'] = '< -15%'\n",
    "\n",
    "# bad_grades_points = pd.concat([bad_up_grades_points,bad_down_grades_points])\n",
    "\n",
    "\n",
    "# color_dict = {\n",
    "#     '> 15%': 'green',\n",
    "#     '< -15%': 'red',\n",
    "# }\n",
    "# bad_grades_points['color'] = bad_grades_points['type'].map(color_dict)\n",
    "\n",
    "# bad_grades_points.plot(ax=ax2,color=bad_grades_points['color'],zorder=4)\n",
    "# bad_grades_points.plot(ax=ax3,color=bad_grades_points['color'],zorder=4)\n",
    "\n",
    "# #for drawing the link\n",
    "# x = np.array([x for x, y in item['geometry']])\n",
    "# y = np.array([y for x, y in item['geometry']])\n",
    "# # length of line segment\n",
    "# ds=10\n",
    "# # number of line segments per interval\n",
    "# Ns = np.round(np.sqrt( (x[1:]-x[:-1])**2 + (y[1:]-y[:-1])**2 ) / ds).astype(int)\n",
    "# # sub-divide intervals w.r.t. Ns\n",
    "# subdiv = lambda x, Ns=Ns: np.concatenate([ np.linspace(x[ii], x[ii+1], Ns[ii]) for ii, _ in enumerate(x[:-1]) ])\n",
    "# x, y = subdiv(x), subdiv(y)\n",
    "# ax2.quiver(x[:-1], y[:-1], x[1:]-x[:-1], y[1:]-y[:-1], scale_units='xy', angles='xy', scale=1, width=.004, headlength=4, headwidth=4)\n",
    "# ax3.quiver(x[:-1], y[:-1], x[1:]-x[:-1], y[1:]-y[:-1], scale_units='xy', angles='xy', scale=1, width=.004, headlength=4, headwidth=4)\n",
    "\n",
    "# #If you want a legend for map (didnot figure out arrow in the legend)\n",
    "# # from matplotlib.lines import Line2D\n",
    "# # from matplotlib.patches import Arrow\n",
    "# # custom_points = [Line2D([0], [0], marker=\"o\", linestyle=\"none\", markersize=10, color=color) for color in color_dict.values()]\n",
    "# # #custom_line = Arrow(0,0,dx=0.1,dy=0,width=0.1,linewidth=0.5,color='black')\n",
    "# # #custom_points.append()\n",
    "# # leg_points = ax2.legend(custom_points, ['> 15%','< -15%'])#, title = 'Legend', alignment='right')\n",
    "# # ax2.add_artist(leg_points)\n",
    "\n",
    "# #make sure fig is square\n",
    "# padding = 100\n",
    "# x_diff = np.abs(maxx - minx)\n",
    "# y_diff = np.abs(maxy - miny)\n",
    "# diff = (x_diff - y_diff)/2\n",
    "# if diff > 0:\n",
    "#     ax2.set_xlim(minx-padding,maxx+padding)\n",
    "#     ax2.set_ylim(miny-padding-diff,maxy+padding+diff)\n",
    "\n",
    "#     ax3.set_xlim(minx-padding,maxx+padding)\n",
    "#     ax3.set_ylim(miny-padding-diff,maxy+padding+diff)\n",
    "# else:\n",
    "#     ax2.set_xlim(minx-padding-np.abs(diff),maxx+padding+np.abs(diff))\n",
    "#     ax2.set_ylim(miny-padding,maxy+padding)\n",
    "\n",
    "#     ax3.set_xlim(minx-padding-np.abs(diff),maxx+padding+np.abs(diff))\n",
    "#     ax3.set_ylim(miny-padding,maxy+padding)\n",
    "\n",
    "# ax2.set_axis_off()\n",
    "# ax3.set_axis_off()\n",
    "\n",
    "# cx.add_basemap(ax2,zoom=17,source=cx.providers.MapTiler.Satellite(key=maptilerapikey),crs=bad_grades_points.crs,alpha=0.5)\n",
    "# cx.add_basemap(ax3,zoom=16,source=cx.providers.MapTiler.Streets(key=maptilerapikey),crs=bad_grades_points.crs)\n",
    "\n",
    "# #maybe if we wanted a high res version of this later\n",
    "# #https://stackoverflow.com/questions/42483449/mapbox-gl-js-export-map-to-png-or-pdf\n",
    "\n",
    "# name = links.loc[links['linkid']==linkid,'name'].item()\n",
    "# plt.suptitle(f'{name} ({linkid}) vertical profile')\n",
    "# plt.show()\n",
    "\n",
    "# # try: \n",
    "# #     plt.savefig(Path(f'D:/bad_grades/{name}_{linkid}.png'),dpi=300)\n",
    "# # except:\n",
    "# #     'invalid filename'\n",
    "\n",
    "# # #clear the axis\n",
    "# # ax1.cla()\n",
    "# # ax2.cla()\n",
    "# # ax3.cla()\n",
    "\n",
    "# from scipy.interpolate import splrep, splev\n",
    "\n",
    "# #pick a random one to examine\n",
    "# #linkid = int(abnormal_links.loc[new,'linkid'].sample(1).item())\n",
    "# linkid = links.loc[still_bad,'linkid'].sample(1).item()\n",
    "# #linkid = 53107\n",
    "\n",
    "# #linkid = 28303#44061\n",
    "# item = interpolated_points_dict[linkid]\n",
    "\n",
    "# #title\n",
    "# road_name = links.loc[links.index==linkid,'name'].item()\n",
    "\n",
    "# #ax1 elements\n",
    "# x = np.array(item['distances'])\n",
    "# y = np.array(item['elevations'])\n",
    "# new_y = np.array(lidar_sampled[linkid])\n",
    "\n",
    "# #ax2 elements\n",
    "# grade_x = np.array(range(0,len(x) - 1))\n",
    "# grade_delta = np.diff(y) / 10 * 100\n",
    "# lidar_delta = np.diff(new_y) / 10 * 100\n",
    "\n",
    "# fig, (ax1,ax2) = plt.subplots(ncols=2,figsize=(9,3))\n",
    "\n",
    "# # Plot the original data and the smoothed curve\n",
    "# ax1.plot(x, y, 'o', label='Original Data')\n",
    "# ax1.plot(x, new_y, 'x',label='Resampled with Lidar')\n",
    "# ax1.grid(True,linestyle='-.')\n",
    "\n",
    "# #find min\n",
    "# y_min = np.min([y.min(),new_y.min()])\n",
    "# y_max = np.max([y.max(),new_y.max()])\n",
    "\n",
    "# ax1.set_xlim(0,np.ceil(x.max() / 10) * 10)\n",
    "# ax1.set_ylim(np.floor(y_min/10)*10,np.ceil(y_max/10)*10)\n",
    "# ax1.set_box_aspect(1)\n",
    "\n",
    "# #ax1.set_title(f'{road_name} (Linkid:{linkid}) Veritcal Cross-Section')\n",
    "# ax1.set_xlabel('Distance (m)')\n",
    "# ax1.set_ylabel('Elevation (m)')\n",
    "# ax1.legend()\n",
    "\n",
    "# ax2.plot(grade_x, grade_delta, 'o', label='Original Grade')\n",
    "# ax2.plot(grade_x, lidar_delta, 'x', label='lidar grade')\n",
    "\n",
    "\n",
    "# ax2.set_xlabel('Distance (m)')\n",
    "# ax2.set_ylabel('Grade %')\n",
    "# ax2.set_box_aspect(1)\n",
    "# ax2.legend()\n",
    "\n",
    "# fig.suptitle(f'{road_name} (Linkid:{linkid})')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # Set ticks\n",
    "# #plt.xlim(0,x.max())\n",
    "# #plt.ylim()\n",
    "\n",
    "# # Plot the dropped points as 'x's\n",
    "# #plt.scatter(x_dropped, y_dropped, marker='x', color='red', label='Dropped Points')\n",
    "\n",
    "\n",
    "# #turn item into a dataframe\n",
    "# #df = pd.DataFrame.from_dict(item,orient='columns')\n",
    "\n",
    "# # df['elev_change'] = df['elevations'].diff()\n",
    "# # df['dist_change'] = df['distances'].diff()\n",
    "# # df['change_rate'] = np.abs(df['elev_change'] / df['dist_change'])\n",
    "\n",
    "#     total_down =  \n",
    "#     all_rise[key] = total_rise\n",
    "#     all_down[key] = total_down\n",
    "\n",
    "#     if total_rise > 0:\n",
    "#         max_rise[key] = elevation_deltas[elevation_deltas>0].max()\n",
    "#     else:\n",
    "#         max_rise[key] = 0\n",
    "\n",
    "#     if total_down > 0:\n",
    "#         min_rise[key] = np.absolute(elevation_deltas[elevation_deltas<0].min())\n",
    "#     else:\n",
    "#         min_rise[key] = 0\n",
    "\n",
    "# #forward direction\n",
    "# links['rise_m'] = pd.Series(all_rise).round(3)\n",
    "# links['maxrise_m'] = pd.Series(max_rise).round(3)\n",
    "# links['up_grade'] = (links['rise_m'] / links.geometry.length * 100).round(1)\n",
    "# links.loc[links.length > interpolate_dist_m,'max_grade'] = (links['maxrise_m'] / interpolate_dist_m * 100).round(1)\n",
    "# links.loc[links['max_grade'].isna(),'max_grade'] = (links['maxrise_m'] / links.length * 100).round(1)\n",
    "\n",
    "# #reverse direction\n",
    "# links['down_m'] = pd.Series(all_down).round(3)\n",
    "# links['minrise_m'] = pd.Series(min_rise).round(3)\n",
    "# links['down_grade'] = (links['down_m'] / links.geometry.length * 100).round(1)\n",
    "# links.loc[links.length > interpolate_dist_m,'min_grade'] = (links['minrise_m'] / interpolate_dist_m * 100).round(1)\n",
    "# links.loc[links['min_grade'].isna(),'min_grade'] = (links['minrise_m'] / links.length * 100).round(1)\n",
    "# #reproject data and export\n",
    "# #links.to_crs('epsg:2240',inplace=True)\n",
    "# #links.to_crs(prev_crs).reset_index(drop=True).to_file(project_dir/'reconciled_network.gpkg',layer='links_w_signals_elevation')\n",
    "# # Sample bridge elevations using LIDAR data\n",
    "# Bridges and tunnels will have inaccurate grades becuase they don't follow the terrain. DEMs represent the lowest point possible so that they show the underlying natural terrain. Raw LIDAR data will have points representing builidngs, trees, and other things on the earth's surface.\n",
    "\n",
    "# The download lidar script will download .LAZ files for the study area (LIDAR data is also available from the USGS site). These are then converted into TIF files using the lidar notebook. Once the TIF files have been made, the following code blocks sample new data that replaces previous elevation samples if the new sampled elevation is greater than the existing.\n",
    "\n",
    "# A new column is created for the link geodataframe to examine where the newly sampled points increased/decreased the accuracy of the elevation data. The assumption is that more accurate data should reduce the number of extreme grade changes and by extension the average grade. It may not do that in all cases, so the next step will be to smooth the data.\n",
    "# linkids = []\n",
    "# for key, item in interpolated_points_dict.items():\n",
    "#     if len(item['bad_grades']) > 0:\n",
    "#         linkids.append(key)\n",
    "# print(len(linkids),'exceed the threshold')\n",
    "# exceeds_threshold = links.loc[linkids]\n",
    "# #grab ones that are not bridges and don't exceed the threshold\n",
    "# not_bridges = exceeds_threshold.loc[exceeds_threshold['bridge'].isna() ,['linkid','geometry']]\n",
    "# print(len(not_bridges),'are not bridges')\n",
    "# ### Find links that cross bridges, railroads, or creeks/rivers (likely an underpass)\n",
    "# #grab bridges\n",
    "# raw_osm = gpd.read_file(network_filepath/'filtered.gpkg',layer='osm_links')\n",
    "# osm_attr = pd.read_pickle(network_filepath/'osm_attr.pkl')\n",
    "# merged = raw_osm.merge(osm_attr,on='osm_linkid')\n",
    "# bridges = merged.loc[~merged['bridge'].isna(),['osm_linkid','geometry']]\n",
    "# bridges.to_crs(links.crs,inplace=True)\n",
    "# #check if bridge crosses non-bridge\n",
    "# crossing_links = []\n",
    "# for idx, row in bridges.iterrows():\n",
    "#     bridge = row['geometry']\n",
    "#     crosses = not_bridges.loc[not_bridges.crosses(bridge),'linkid'].tolist()\n",
    "#     if len(crosses) > 0:\n",
    "#         crossing_links = crossing_links + crosses\n",
    "# mask = list(set(crossing_links))\n",
    "# not_bridges.loc[mask].reset_index(drop=True).explore()\n",
    "# intersection = gpd.overlay(bridges,not_bridges,keep_geom_type=False)\n",
    "# intersection.explore()\n",
    "# ### Examine cross sections for these\n",
    "# intersection['linkid'].nunique()\n",
    "# len(mask)\n",
    "# import contextily as cx\n",
    "\n",
    "# # use west paces ferry to start with\n",
    "# #linkid = 42901\n",
    "\n",
    "# fig, (ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(12,3))\n",
    "\n",
    "# # for linkid, item in tqdm(interpolated_points_dict.items()):\n",
    "\n",
    "# #     if len(item['bad_grades']) == 0:\n",
    "# #         continue\n",
    "\n",
    "# #     if len(item['distances']) <= 2:\n",
    "# #         continue\n",
    "\n",
    "# linkid = not_bridges.loc[mask,'linkid'].sample(1).item()#intersection['linkid'].sample(1).item()\n",
    "# item = interpolated_points_dict[linkid]\n",
    "\n",
    "# #https://github.com/geopandas/geopandas/issues/2279\n",
    "# #https://stackoverflow.com/questions/8247973/how-do-i-specify-an-arrow-like-linestyle-in-matplotlib\n",
    "\n",
    "# #Extract values\n",
    "# x = np.array(item['distances'])\n",
    "# y = np.array(item['elevations'])\n",
    "\n",
    "# #gives you a boolean mask\n",
    "# bad_up_grades = np.array(item['grade_segments']) >= grade_threshold\n",
    "# bad_down_grades = np.array(item['grade_segments']) <= -grade_threshold\n",
    "\n",
    "# #add an additional value\n",
    "# bad_up_grades = np.hstack([np.array((False)),bad_up_grades])\n",
    "# bad_down_grades = np.hstack([np.array((False)),bad_down_grades])\n",
    "\n",
    "# bad_up_x = x[bad_up_grades]\n",
    "# bad_up_y = y[bad_up_grades]\n",
    "\n",
    "# bad_down_x = x[bad_down_grades]\n",
    "# bad_down_y = y[bad_down_grades]\n",
    "\n",
    "# #First Figure\n",
    "# # Plot the original data and the smoothed curve\n",
    "# ax1.plot(x, y, 'o', label='Original Data')\n",
    "# #ax1.plot(x, new_y, 'x',label='Resampled with Lidar')\n",
    "# ax1.grid(True,linestyle='-.')\n",
    "\n",
    "# # plot bad points\n",
    "# ax1.plot(bad_up_x,bad_up_y,'o',color='green',label=f'Above {grade_threshold}%')\n",
    "# ax1.plot(bad_down_x,bad_down_y,'o',color='red',label=f'Below -{grade_threshold}%')\n",
    "\n",
    "# #ax1.set_title(f'{road_name} (Linkid:{linkid}) Veritcal Cross-Section')\n",
    "# ax1.set_xlabel('Distance (m)')\n",
    "# ax1.set_ylabel('Elevation (m)')\n",
    "# ax1.legend()\n",
    "\n",
    "# #Second and Third figure\n",
    "\n",
    "# #create geos to plot\n",
    "# points = np.array([Point(x,y) for x,y in item['geometry']])\n",
    "# line = LineString(item['geometry'])\n",
    "# minx, miny, maxx, maxy = line.bounds\n",
    "# # line_gdf = gpd.GeoDataFrame({'geometry':line},crs=dem_crs,index=[0])\n",
    "\n",
    "# # use mask to just get bad one\n",
    "# bad_up_grades_points = points[bad_up_grades]\n",
    "# bad_up_grades_points = gpd.GeoDataFrame({'geometry':bad_up_grades_points},geometry='geometry',crs=dem_crs)\n",
    "# bad_up_grades_points['type'] = '> 15%'\n",
    "\n",
    "# bad_down_grades_points = points[bad_down_grades]\n",
    "# bad_down_grades_points = gpd.GeoDataFrame({'geometry':bad_down_grades_points},geometry='geometry',crs=dem_crs)\n",
    "# bad_down_grades_points['type'] = '< -15%'\n",
    "\n",
    "# bad_grades_points = pd.concat([bad_up_grades_points,bad_down_grades_points])\n",
    "\n",
    "\n",
    "# color_dict = {\n",
    "#     '> 15%': 'green',\n",
    "#     '< -15%': 'red',\n",
    "# }\n",
    "# bad_grades_points['color'] = bad_grades_points['type'].map(color_dict)\n",
    "\n",
    "# bad_grades_points.plot(ax=ax2,color=bad_grades_points['color'],zorder=4)\n",
    "# bad_grades_points.plot(ax=ax3,color=bad_grades_points['color'],zorder=4)\n",
    "\n",
    "# #for drawing the link\n",
    "# x = np.array([x for x, y in item['geometry']])\n",
    "# y = np.array([y for x, y in item['geometry']])\n",
    "# # length of line segment\n",
    "# ds=10\n",
    "# # number of line segments per interval\n",
    "# Ns = np.round(np.sqrt( (x[1:]-x[:-1])**2 + (y[1:]-y[:-1])**2 ) / ds).astype(int)\n",
    "# # sub-divide intervals w.r.t. Ns\n",
    "# subdiv = lambda x, Ns=Ns: np.concatenate([ np.linspace(x[ii], x[ii+1], Ns[ii]) for ii, _ in enumerate(x[:-1]) ])\n",
    "# x, y = subdiv(x), subdiv(y)\n",
    "# ax2.quiver(x[:-1], y[:-1], x[1:]-x[:-1], y[1:]-y[:-1], scale_units='xy', angles='xy', scale=1, width=.004, headlength=4, headwidth=4)\n",
    "# ax3.quiver(x[:-1], y[:-1], x[1:]-x[:-1], y[1:]-y[:-1], scale_units='xy', angles='xy', scale=1, width=.004, headlength=4, headwidth=4)\n",
    "\n",
    "# #If you want a legend for map (didnot figure out arrow in the legend)\n",
    "# # from matplotlib.lines import Line2D\n",
    "# # from matplotlib.patches import Arrow\n",
    "# # custom_points = [Line2D([0], [0], marker=\"o\", linestyle=\"none\", markersize=10, color=color) for color in color_dict.values()]\n",
    "# # #custom_line = Arrow(0,0,dx=0.1,dy=0,width=0.1,linewidth=0.5,color='black')\n",
    "# # #custom_points.append()\n",
    "# # leg_points = ax2.legend(custom_points, ['> 15%','< -15%'])#, title = 'Legend', alignment='right')\n",
    "# # ax2.add_artist(leg_points)\n",
    "\n",
    "# #make sure fig is square\n",
    "# padding = 100\n",
    "# x_diff = np.abs(maxx - minx)\n",
    "# y_diff = np.abs(maxy - miny)\n",
    "# diff = (x_diff - y_diff)/2\n",
    "# if diff > 0:\n",
    "#     ax2.set_xlim(minx-padding,maxx+padding)\n",
    "#     ax2.set_ylim(miny-padding-diff,maxy+padding+diff)\n",
    "\n",
    "#     ax3.set_xlim(minx-padding,maxx+padding)\n",
    "#     ax3.set_ylim(miny-padding-diff,maxy+padding+diff)\n",
    "# else:\n",
    "#     ax2.set_xlim(minx-padding-np.abs(diff),maxx+padding+np.abs(diff))\n",
    "#     ax2.set_ylim(miny-padding,maxy+padding)\n",
    "\n",
    "#     ax3.set_xlim(minx-padding-np.abs(diff),maxx+padding+np.abs(diff))\n",
    "#     ax3.set_ylim(miny-padding,maxy+padding)\n",
    "\n",
    "# ax2.set_axis_off()\n",
    "# ax3.set_axis_off()\n",
    "\n",
    "# cx.add_basemap(ax2,zoom=17,source=cx.providers.MapTiler.Satellite(key=maptilerapikey),crs=bad_grades_points.crs,alpha=0.5)\n",
    "# cx.add_basemap(ax3,zoom=16,source=cx.providers.MapTiler.Streets(key=maptilerapikey),crs=bad_grades_points.crs)\n",
    "\n",
    "# #maybe if we wanted a high res version of this later\n",
    "# #https://stackoverflow.com/questions/42483449/mapbox-gl-js-export-map-to-png-or-pdf\n",
    "\n",
    "# name = links.loc[links['linkid']==linkid,'name'].item()\n",
    "# plt.suptitle(f'{name} ({linkid}) vertical profile')\n",
    "# plt.show()\n",
    "\n",
    "# # try: \n",
    "# #     plt.savefig(Path(f'D:/bad_grades/{name}_{linkid}.png'),dpi=300)\n",
    "# # except:\n",
    "# #     'invalid filename'\n",
    "\n",
    "# # #clear the axis\n",
    "# # ax1.cla()\n",
    "# # ax2.cla()\n",
    "# # ax3.cla()\n",
    "# linkid = 28689\n",
    "# item = interpolated_points_dict[linkid]\n",
    "\n",
    "# distances = np.array(item['distances'])\n",
    "# elevations = np.array(item['elevations'])\n",
    "# bad_elevs = item['bad_grades']\n",
    "# #temp_anom = [-0.17, -0.09, -0.11, -0.18, -0.3] # and so on...\n",
    "\n",
    "# # further step could be to identify the high points and assign a lower weight \n",
    "# weights = np.ones_like(elevations)\n",
    "# weights[elevations>311] = 0\n",
    "\n",
    "# whittaker_smoother = WhittakerSmoother(\n",
    "#     lmbda=150, order=2, data_length=len(elevations)#, weights=weights\n",
    "# )\n",
    "\n",
    "# smoothed = whittaker_smoother.smooth(elevations)\n",
    "\n",
    "# plt.plot(distances,elevations)\n",
    "# plt.plot(distances,smoothed)\n",
    "# test1 = pd.Series(elevations).diff()\n",
    "# test1_up = test1[test1>0].sum() / distances.max() * 100\n",
    "# test2_down = test1[test1<0].sum() / distances.max() * 100\n",
    "# print(test1_up,test2_down)\n",
    "# test1 = pd.Series(smoothed).diff()\n",
    "# test1_up = test1[test1>0].sum() / distances.max() * 100\n",
    "# test2_down = test1[test1<0].sum() / distances.max() * 100\n",
    "# print(test1_up,test2_down)\n",
    "# # For these, Lidar probably isn't gonna help\n",
    "# # so we just want to try the spline fit\n",
    "\n",
    "# #identify large abnormalities and exclude stair cases (>25%)\n",
    "# # thresholds = range(6,25+5)\n",
    "# # counts = []\n",
    "# # for threshold in thresholds:\n",
    "# #     abnormal = ((links['max_grade'] > threshold) | (links['min_grade'] > threshold))#& (links['highway'] != 'steps')\n",
    "# #     counts.append(links.loc[abnormal].shape[0])\n",
    "# # np.array(counts)\n",
    "\n",
    "# #before sensitvity, just use the haobing one (15% for local roads)\n",
    "# threshold = 15\n",
    "# abnormal = ((links['max_grade'] > threshold) | (links['min_grade'] > threshold))#& (links['highway'] != 'steps')\n",
    "# abnormal_links = links[abnormal]\n",
    "# print(f\"{abnormal.sum()}/{links.shape[0]} links have at least one point exceeding {threshold}% ({abnormal_links.length.sum().round(0)}/{links.length.sum().round(0)} km)\")\n",
    "# abnormal_links['length_km'] = abnormal_links.length\n",
    "# print(abnormal_links.groupby('highway')['length_km'].sum().round(0).sort_values(ascending=False))\n",
    "\n",
    "# print(abnormal_links['highway'].value_counts().sort_values(ascending=False))\n",
    "# # Method 1\n",
    "# bridge_tif_fps = list((Path('D:/') / 'bridge_tif').glob('*.tif'))\n",
    "# abnormal_links.to_crs(rasterio.open(bridge_tif_fps[0]).crs,inplace=True)\n",
    "# lidar_sampled = dict()\n",
    "\n",
    "# for bridge_tif_fp in tqdm(bridge_tif_fps):\n",
    "#     #open the raster using the link\n",
    "#     bridge_tif = rasterio.open(bridge_tif_fp)\n",
    "\n",
    "#     #find all links that intersect with the current raster\n",
    "#     xmin, ymin, xmax, ymax = bridge_tif.bounds\n",
    "#     bbox = box(xmin,ymin,xmax,ymax)\n",
    "#     intersection = abnormal_links.intersects(bbox)\n",
    "    \n",
    "#     if (intersection == True).any():\n",
    "#         for index in abnormal_links[intersection].index:\n",
    "#             dict_values = interpolated_points_dict.get(index)\n",
    "\n",
    "#             #change the crs\n",
    "#             #open the first one to just get the crs\n",
    "#             geometry = [Point(x,y) for x,y in dict_values['geometry']]\n",
    "#             gdf = gpd.GeoDataFrame({'geometry':geometry},crs=dem_crs)\n",
    "#             gdf.to_crs(bridge_tif.crs,inplace=True)\n",
    "#             geometry = list(zip(gdf.geometry.x,gdf.geometry.y))\n",
    "\n",
    "#             sampled = np.array([val[0] for val in bridge_tif.sample(geometry)])\n",
    "\n",
    "#             #check for nan values in sampled\n",
    "#             #match the nan points to the nearest cell\n",
    "            \n",
    "\n",
    "#             #if at least one non-null value\n",
    "#             #replace elevation if value is higher?\n",
    "#             if np.isnan(sampled).all() == False:\n",
    "#                 lidar_sampled[index] =  np.nanmax([sampled,dict_values['elevations']],axis = 0)\n",
    "\n",
    "# # Method 2\n",
    "# Uses the raw lidar points instead.\n",
    "# #grab bridges\n",
    "\n",
    "# import pickle\n",
    "# with Path('D:/lidar_points.pkl').open('rb') as fh:\n",
    "#     lidar_points = pickle.load(fh)\n",
    "\n",
    "# lidar_points.to_crs(prev_crs,inplace=True)\n",
    "# spatial_index = lidar_points.sindex\n",
    "\n",
    "\n",
    "# lidar_sampled = dict()\n",
    "# incomplete = []\n",
    "\n",
    "# for index in tqdm(abnormal_links.index):\n",
    "#     dict_values = interpolated_points_dict.get(index)\n",
    "\n",
    "#     geometry = [Point(x,y) for x,y in dict_values['geometry']]\n",
    "#     gdf = gpd.GeoDataFrame({'geometry':geometry},crs=dem_crs)\n",
    "#     gdf.to_crs(prev_crs,inplace=True)\n",
    "\n",
    "#     #buffer the data\n",
    "#     buffer_m = 20\n",
    "#     gdf.geometry = gdf.buffer(buffer_m)\n",
    "\n",
    "#     #get the gdf bounding box\n",
    "#     polygon = gdf.geometry.unary_union.convex_hull\n",
    "    \n",
    "#     #use spatial index to only select a small number of points\n",
    "#     possible_matches_index = list(spatial_index.intersection(polygon.bounds))\n",
    "#     possible_matches = lidar_points.iloc[possible_matches_index]\n",
    "    \n",
    "#     #add an index column for the overlay part\n",
    "#     gdf.reset_index(inplace=True)\n",
    "#     precise_matches = gpd.overlay(possible_matches,gdf,how='intersection')\n",
    "\n",
    "#     if len(precise_matches) == 0:\n",
    "#         continue\n",
    "\n",
    "#     if gdf['index'].isin(set(precise_matches['index'].tolist())).all() == False:\n",
    "#         #this is where we might want to increase the buffer on the remaining points\n",
    "        \n",
    "#         #assign nan\n",
    "#         incomplete.append(index)\n",
    "\n",
    "#     new_values = precise_matches.groupby('index')['elevation_m'].mean()\n",
    "#     gdf['new_elevation_m'] = gdf['index'].map(new_values)\n",
    "#     new_values = np.array(gdf['new_elevation_m'])\n",
    "\n",
    "#     lidar_sampled[index] = np.nanmax([new_values,dict_values['elevations']],axis = 0)\n",
    "# lidar_sampled[36]\n",
    "# #interpolated_points_dict[36]\n",
    "# all_rise = {}\n",
    "# all_down = {}\n",
    "# max_rise = {}\n",
    "# min_rise = {}\n",
    "\n",
    "# for key, item in tqdm(lidar_sampled.items(),total=len(lidar_sampled)):     \n",
    "\n",
    "#     elevation_deltas = []\n",
    "    \n",
    "#     #then caluclate the elevation change with the smoothed data\n",
    "#     for x in range(0,len(item)-1):        \n",
    "#         elev1 = item[x]\n",
    "#         elev2 = item[x+1]\n",
    "#         elevation_delta = (elev2-elev1)\n",
    "#         elevation_deltas.append(elevation_delta)\n",
    "\n",
    "#     elevation_deltas = np.asarray(elevation_deltas)\n",
    "#     total_rise = elevation_deltas[elevation_deltas>0].sum()\n",
    "#     total_down = np.absolute(elevation_deltas[elevation_deltas<0].sum()) #take absolute value at this point\n",
    "#     all_rise[key] = total_rise\n",
    "#     all_down[key] = total_down\n",
    "\n",
    "#     if total_rise > 0:\n",
    "#         max_rise[key] = elevation_deltas[elevation_deltas>0].max()\n",
    "#     else:\n",
    "#         max_rise[key] = 0\n",
    "\n",
    "#     if total_down > 0:\n",
    "#         min_rise[key] = np.absolute(elevation_deltas[elevation_deltas<0].min())\n",
    "#     else:\n",
    "#         min_rise[key] = 0\n",
    "\n",
    "# #forward direction\n",
    "# links['new_rise_m'] = pd.Series(all_rise).round(3)\n",
    "# links['new_maxrise_m'] = pd.Series(max_rise).round(3)\n",
    "# links['new_up_grade'] = (links['new_rise_m'] / links.geometry.length * 100).round(1)\n",
    "# links.loc[links.length > interpolate_dist_m,'new_max_grade'] = (links['new_maxrise_m'] / interpolate_dist_m * 100).round(1)\n",
    "# links.loc[links['new_max_grade'].isna(),'new_max_grade'] = (links['new_maxrise_m'] / links.length * 100).round(1)\n",
    "\n",
    "# #reverse direction\n",
    "# links['new_down_m'] = pd.Series(all_down).round(3)\n",
    "# links['new_minrise_m'] = pd.Series(min_rise).round(3)\n",
    "# links['new_down_grade'] = (links['new_down_m'] / links.geometry.length * 100).round(1)\n",
    "# links.loc[links.length > interpolate_dist_m,'new_min_grade'] = (links['new_minrise_m'] / interpolate_dist_m * 100).round(1)\n",
    "# links.loc[links['new_min_grade'].isna(),'new_min_grade'] = (links['new_minrise_m'] / links.length * 100).round(1)\n",
    "\n",
    "# # See where elevation data improved\n",
    "# all_rise = {}\n",
    "# all_down = {}\n",
    "# max_rise = {}\n",
    "# min_rise = {}\n",
    "\n",
    "# for key, item in tqdm(lidar_sampled.items(),total=len(lidar_sampled)):     \n",
    "\n",
    "#     elevation_deltas = []\n",
    "    \n",
    "#     #then caluclate the elevation change with the smoothed data\n",
    "#     for x in range(0,len(item)-1):        \n",
    "#         elev1 = item[x]\n",
    "#         elev2 = item[x+1]\n",
    "#         elevation_delta = (elev2-elev1)\n",
    "#         elevation_deltas.append(elevation_delta)\n",
    "\n",
    "#     elevation_deltas = np.asarray(elevation_deltas)\n",
    "#     total_rise = elevation_deltas[elevation_deltas>0].sum()\n",
    "#     total_down = np.absolute(elevation_deltas[elevation_deltas<0].sum()) #take absolute value at this point\n",
    "#     all_rise[key] = total_rise\n",
    "#     all_down[key] = total_down\n",
    "\n",
    "#     if total_rise > 0:\n",
    "#         max_rise[key] = elevation_deltas[elevation_deltas>0].max()\n",
    "#     else:\n",
    "#         max_rise[key] = 0\n",
    "\n",
    "#     if total_down > 0:\n",
    "#         min_rise[key] = np.absolute(elevation_deltas[elevation_deltas<0].min())\n",
    "#     else:\n",
    "#         min_rise[key] = 0\n",
    "\n",
    "# #forward direction\n",
    "# links['new_rise_m'] = pd.Series(all_rise).round(3)\n",
    "# links['new_maxrise_m'] = pd.Series(max_rise).round(3)\n",
    "# links['new_up_grade'] = (links['new_rise_m'] / links.geometry.length * 100).round(1)\n",
    "# links.loc[links.length > interpolate_dist_m,'new_max_grade'] = (links['new_maxrise_m'] / interpolate_dist_m * 100).round(1)\n",
    "# links.loc[links['new_max_grade'].isna(),'new_max_grade'] = (links['new_maxrise_m'] / links.length * 100).round(1)\n",
    "\n",
    "# #reverse direction\n",
    "# links['new_down_m'] = pd.Series(all_down).round(3)\n",
    "# links['new_minrise_m'] = pd.Series(min_rise).round(3)\n",
    "# links['new_down_grade'] = (links['new_down_m'] / links.geometry.length * 100).round(1)\n",
    "# links.loc[links.length > interpolate_dist_m,'new_min_grade'] = (links['new_minrise_m'] / interpolate_dist_m * 100).round(1)\n",
    "\n",
    "# prev = abnormal_links.shape[0]\n",
    "# fixed = ((links['new_max_grade'] <= threshold) & (links['new_min_grade'] <= threshold))\n",
    "# still_bad = ((links['new_max_grade'] > threshold) & (links['new_min_grade'] > threshold))\n",
    "# print(f'{fixed.sum()} / {prev} abnormal links fixed')\n",
    "# print(f'{still_bad.sum()} are still abnormal')\n",
    "# print(f'{prev-fixed.sum()-still_bad.sum()} did not overlap with a bridge tif')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from scipy.interpolate import splrep, splev\n",
    "\n",
    "# #pick a random one to examine\n",
    "# #linkid = int(abnormal_links.loc[new,'linkid'].sample(1).item())\n",
    "# linkid = links.loc[still_bad,'linkid'].sample(1).item()\n",
    "# #linkid = 53107\n",
    "\n",
    "# #linkid = 28303#44061\n",
    "# item = interpolated_points_dict[linkid]\n",
    "\n",
    "# #title\n",
    "# road_name = links.loc[links.index==linkid,'name'].item()\n",
    "\n",
    "# #ax1 elements\n",
    "# x = np.array(item['distances'])\n",
    "# y = np.array(item['elevations'])\n",
    "# new_y = np.array(lidar_sampled[linkid])\n",
    "\n",
    "# #ax2 elements\n",
    "# grade_x = np.array(range(0,len(x) - 1))\n",
    "# grade_delta = np.diff(y) / 10 * 100\n",
    "# lidar_delta = np.diff(new_y) / 10 * 100\n",
    "\n",
    "# fig, (ax1,ax2) = plt.subplots(ncols=2,figsize=(9,3))\n",
    "\n",
    "# # Plot the original data and the smoothed curve\n",
    "# ax1.plot(x, y, 'o', label='Original Data')\n",
    "# ax1.plot(x, new_y, 'x',label='Resampled with Lidar')\n",
    "# ax1.grid(True,linestyle='-.')\n",
    "\n",
    "# #find min\n",
    "# y_min = np.min([y.min(),new_y.min()])\n",
    "# y_max = np.max([y.max(),new_y.max()])\n",
    "\n",
    "# ax1.set_xlim(0,np.ceil(x.max() / 10) * 10)\n",
    "# ax1.set_ylim(np.floor(y_min/10)*10,np.ceil(y_max/10)*10)\n",
    "# ax1.set_box_aspect(1)\n",
    "\n",
    "# #ax1.set_title(f'{road_name} (Linkid:{linkid}) Veritcal Cross-Section')\n",
    "# ax1.set_xlabel('Distance (m)')\n",
    "# ax1.set_ylabel('Elevation (m)')\n",
    "# ax1.legend()\n",
    "\n",
    "# ax2.plot(grade_x, grade_delta, 'o', label='Original Grade')\n",
    "# ax2.plot(grade_x, lidar_delta, 'x', label='lidar grade')\n",
    "\n",
    "\n",
    "# ax2.set_xlabel('Distance (m)')\n",
    "# ax2.set_ylabel('Grade %')\n",
    "# ax2.set_box_aspect(1)\n",
    "# ax2.legend()\n",
    "\n",
    "# fig.suptitle(f'{road_name} (Linkid:{linkid})')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # Set ticks\n",
    "# #plt.xlim(0,x.max())\n",
    "# #plt.ylim()\n",
    "\n",
    "# # Plot the dropped points as 'x's\n",
    "# #plt.scatter(x_dropped, y_dropped, marker='x', color='red', label='Dropped Points')\n",
    "\n",
    "\n",
    "# #turn item into a dataframe\n",
    "# #df = pd.DataFrame.from_dict(item,orient='columns')\n",
    "\n",
    "# # df['elev_change'] = df['elevations'].diff()\n",
    "# # df['dist_change'] = df['distances'].diff()\n",
    "# # df['change_rate'] = np.abs(df['elev_change'] / df['dist_change'])\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from scipy.interpolate import splrep, splev\n",
    "\n",
    "# #pick a random one to examine\n",
    "# #linkid = int(abnormal_links.loc[new,'linkid'].sample(1).item())\n",
    "# linkid = links.loc[still_bad,'linkid'].sample(1).item()\n",
    "# #linkid = 53107\n",
    "\n",
    "# #linkid = 28303#44061\n",
    "# item = interpolated_points_dict[linkid]\n",
    "\n",
    "# links.loc[links.index==linkid,['up_grade','max_grade','min_grade','new_up_grade','down_grade','new_down_grade']].squeeze()\n",
    "# links.loc[links.index==linkid,['up_grade','max_grade','min_grade','new_up_grade','down_grade','new_down_grade']].squeeze()\n",
    "# links.loc[links.index==linkid,['up_grade','max_grade','min_grade','new_up_grade','down_grade','new_down_grade']].squeeze()\n",
    "# #title\n",
    "# road_name = links.loc[links.index==linkid,'name'].item()\n",
    "\n",
    "# #ax1 elements\n",
    "# x = np.array(item['distances'])\n",
    "# y = np.array(item['elevations'])\n",
    "# new_y = np.array(lidar_sampled[linkid])\n",
    "\n",
    "# #ax2 elements\n",
    "# grade_x = np.array(range(0,len(x) - 1))\n",
    "# grade_delta = np.diff(y) / 10 * 100\n",
    "# lidar_delta = np.diff(new_y) / 10 * 100\n",
    "\n",
    "# fig, (ax1,ax2) = plt.subplots(ncols=2,figsize=(9,3))\n",
    "\n",
    "# # Plot the original data and the smoothed curve\n",
    "# ax1.plot(x, y, 'o', label='Original Data')\n",
    "# ax1.plot(x, new_y, 'x',label='Resampled with Lidar')\n",
    "# ax1.grid(True,linestyle='-.')\n",
    "\n",
    "# #find min\n",
    "# y_min = np.min([y.min(),new_y.min()])\n",
    "# y_max = np.max([y.max(),new_y.max()])\n",
    "\n",
    "# ax1.set_xlim(0,np.ceil(x.max() / 10) * 10)\n",
    "# ax1.set_ylim(np.floor(y_min/10)*10,np.ceil(y_max/10)*10)\n",
    "# ax1.set_box_aspect(1)\n",
    "\n",
    "# #ax1.set_title(f'{road_name} (Linkid:{linkid}) Veritcal Cross-Section')\n",
    "# ax1.set_xlabel('Distance (m)')\n",
    "# ax1.set_ylabel('Elevation (m)')\n",
    "# ax1.legend()\n",
    "\n",
    "# ax2.plot(grade_x, grade_delta, 'o', label='Original Grade')\n",
    "# ax2.plot(grade_x, lidar_delta, 'x', label='lidar grade')\n",
    "\n",
    "\n",
    "# ax2.set_xlabel('Distance (m)')\n",
    "# ax2.set_ylabel('Grade %')\n",
    "# ax2.set_box_aspect(1)\n",
    "# ax2.legend()\n",
    "\n",
    "# fig.suptitle(f'{road_name} (Linkid:{linkid})')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # Set ticks\n",
    "# #plt.xlim(0,x.max())\n",
    "# #plt.ylim()\n",
    "\n",
    "# # Plot the dropped points as 'x's\n",
    "# #plt.scatter(x_dropped, y_dropped, marker='x', color='red', label='Dropped Points')\n",
    "\n",
    "\n",
    "# #turn item into a dataframe\n",
    "# #df = pd.DataFrame.from_dict(item,orient='columns')\n",
    "\n",
    "# # df['elev_change'] = df['elevations'].diff()\n",
    "# # df['dist_change'] = df['distances'].diff()\n",
    "# # df['change_rate'] = np.abs(df['elev_change'] / df['dist_change'])prev = abnormal_links.shape[0]\n",
    "# fixed = ((links['new_max_grade'] <= threshold) & (links['new_min_grade'] <= threshold))\n",
    "# still_bad = ((links['new_max_grade'] > threshold) & (links['new_min_grade'] > threshold))\n",
    "# print(f'{fixed.sum()} / {prev} abnormal links fixed')\n",
    "# print(f'{still_bad.sum()} are still abnormal')\n",
    "# print(f'{prev-fixed.sum()-still_bad.sum()} did not overlap with a bridge tif')\n",
    "# prev = abnormal_links.shape[0]\n",
    "# fixed = ((links['new_max_grade'] <= threshold) & (links['new_min_grade'] <= threshold))\n",
    "# still_bad = ((links['new_max_grade'] > threshold) & (links['new_min_grade'] > threshold))\n",
    "# print(f'{fixed.sum()} / {prev} abnormal links fixed')\n",
    "# print(f'{still_bad.sum()} are still abnormal')\n",
    "# print(f'{prev-fixed.sum()-still_bad.sum()} did not overlap with a bridge tif')links.loc[links['new_min_grade'].isna(),'new_min_grade'] = (links['new_minrise_m'] / links.length * 100).round(1)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from scipy.interpolate import splrep, splev\n",
    "\n",
    "# linkid = 28303#44061\n",
    "# item = interpolated_points_dict[linkid]\n",
    "\n",
    "# x = item['distances']\n",
    "# y = item['elevations']\n",
    "# new_y = new[linkid]\n",
    "\n",
    "# # Plot the original data and the smoothed curve\n",
    "# plt.plot(x, y, 'o', label='Original Data')\n",
    "# plt.plot(x, new_y, 'o',label='Resampled with Lidar')\n",
    "\n",
    "# # Plot the dropped points as 'x's\n",
    "# #plt.scatter(x_dropped, y_dropped, marker='x', color='red', label='Dropped Points')\n",
    "\n",
    "# #plt.title('10th Street Northwest (westward direction, tech to midtown) Veritcal Cross-Section')\n",
    "# plt.xlabel('Distance (m)')\n",
    "# plt.ylabel('Elevation (m)')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# #turn item into a dataframe\n",
    "# #df = pd.DataFrame.from_dict(item,orient='columns')\n",
    "\n",
    "# # df['elev_change'] = df['elevations'].diff()\n",
    "# # df['dist_change'] = df['distances'].diff()\n",
    "# # df['change_rate'] = np.abs(df['elev_change'] / df['dist_change'])\n",
    "# ### Based on Haobing's paper\n",
    "# Spline fitting function\n",
    "# x as linear distance and y as elevation\n",
    "\n",
    "# current issue is that segments are not long enough as is, bridges need to extend forwards/backwards to have more data on the surrounding terrain. In the paper there are 3 km long segments. We can either try to linemerge lines to make them longer or add extensions\n",
    "\n",
    "# this is where the weighting prolly comes in\n",
    "\n",
    "# [Scipy Splrep](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.splrep.html#scipy.interpolate.splrep)\n",
    "\n",
    "\n",
    "# # Assuming x, y, x_dropped, y_dropped, and spl are defined as in your code\n",
    "# x = np.array(df_copy['distances'])\n",
    "# y = np.array(df_copy['elevations'])\n",
    "\n",
    "# df_dropped = df.loc[~df.index.isin(df_copy.index)]\n",
    "\n",
    "# x_dropped = np.array(df_dropped['distances'])\n",
    "# y_dropped = np.array(df_dropped['elevations'])\n",
    "\n",
    "# #spline technique (we're not doing OLS i think because we don't have ground truth data?)\n",
    "# lambda_parameter = 3000 # 500 for local road and 3000 for highways\n",
    "# alpha = 99999990000 #paper used between 1000 and 10000\n",
    "# # i believe higher is smoother\n",
    "# spl = splrep(x,y,s=alpha)\n",
    "# y2 = splev(x,spl)\n",
    "\n",
    "# # Plot the original data and the smoothed curve\n",
    "# plt.plot(x, y, 'o', label='Original Data')\n",
    "# plt.plot(x, y2, label='Smoothed Curve')\n",
    "\n",
    "# # Plot the dropped points as 'x's\n",
    "# plt.scatter(x_dropped, y_dropped, marker='x', color='red', label='Dropped Points')\n",
    "\n",
    "# plt.title('10th Street Northwest (westward direction, tech to midtown) Veritcal Cross-Section')\n",
    "# plt.xlabel('Distance (m)')\n",
    "# plt.ylabel('Elevation (m)')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# links_copy = links.copy()\n",
    "\n",
    "\n",
    "# # Assuming x, y, x_dropped, y_dropped, and spl are defined as in your code\n",
    "# x = np.array(df_copy['distances'])\n",
    "# y = np.array(df_copy['elevations'])\n",
    "\n",
    "# df_dropped = df.loc[~df.index.isin(df_copy.index)]\n",
    "\n",
    "# x_dropped = np.array(df_dropped['distances'])\n",
    "# y_dropped = np.array(df_dropped['elevations'])\n",
    "\n",
    "# #spline technique (we're not doing OLS i think because we don't have ground truth data?)\n",
    "# lambda_parameter = 3000 # 500 for local road and 3000 for highways\n",
    "# alpha = 99999990000 #paper used between 1000 and 10000\n",
    "# # i believe higher is smoother\n",
    "# spl = splrep(x,y,s=alpha)\n",
    "# y2 = splev(x,spl)\n",
    "\n",
    "# # Plot the original data and the smoothed curve\n",
    "# plt.plot(x, y, 'o', label='Original Data')\n",
    "# plt.plot(x, y2, label='Smoothed Curve')\n",
    "\n",
    "# # Plot the dropped points as 'x's\n",
    "# plt.scatter(x_dropped, y_dropped, marker='x', color='red', label='Dropped Points')\n",
    "\n",
    "# plt.title('10th Street Northwest (westward direction, tech to midtown) Veritcal Cross-Section')\n",
    "# plt.xlabel('Distance (m)')\n",
    "# plt.ylabel('Elevation (m)')\n",
    "# plt.legend()\n",
    "\n",
    "# links_copy['tunnel'].value_counts()\n",
    "\n",
    "# vals = ['yes','buidling_passage']\n",
    "# links_copy['tunnel'] = links_copy['tunnel'].isin(vals)\n",
    "# plt.show()\n",
    "\n",
    "# grade_cols = links_copy.columns.tolist()[-8:]\n",
    "# bridge_or_tunnel = (links_copy['tunnel']==True) | (links_copy['bridge']==True)\n",
    "# links_copy.loc[bridge_or_tunnel,['name','geometry']+grade_cols].explore()\n",
    "# links_copy.loc[bridge_or_tunnel,grade_cols] = 0\n",
    "# ## Side profile to show example (10th Street over the connector)\n",
    "# links.columns\n",
    "# linkid = 28303\n",
    "\n",
    "# street_name = links.at[linkid,'name']\n",
    "# distances = interpolated_points_dict[linkid]['distances']\n",
    "# elevations = interpolated_points_dict[linkid]['elevations']\n",
    "# geometries = interpolated_points_dict[linkid]['geometry']\n",
    "\n",
    "# plt.scatter(distances,elevations)\n",
    "# plt.title(street_name)\n",
    "# plt.xlabel('Distance (m)')\n",
    "# plt.ylabel('Elevation (m)')\n",
    "# import folium\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import LineString\n",
    "# import matplotlib.pyplot as plt\n",
    "# import base64\n",
    "# from io import BytesIO\n",
    "\n",
    "# # Replace 'linkid' with your unique identifier for the road segment\n",
    "# linkid = 28303\n",
    "\n",
    "# # Extracting relevant data\n",
    "# street_name = links.at[linkid,'name']\n",
    "# distances = interpolated_points_dict[linkid]['distances']\n",
    "# elevations = interpolated_points_dict[linkid]['elevations']\n",
    "# geometries = interpolated_points_dict[linkid]['geometry']\n",
    "\n",
    "# # Creating a GeoDataFrame for the road segment\n",
    "# road_geometry = LineString(geometries)\n",
    "# road_gdf = gpd.GeoDataFrame(geometry=[road_geometry],crs=src.crs)\n",
    "# road_gdf.to_crs('epsg:4326',inplace=True)\n",
    "\n",
    "# # Create a Folium map centered on the road segment\n",
    "# map_center = [road_gdf.centroid.y, road_gdf.centroid.x]\n",
    "# mymap = folium.Map(location=map_center, zoom_start=15)\n",
    "\n",
    "# # Save the scatter plot as an image\n",
    "# scatter_plot_filename = 'scatter_plot.png'\n",
    "# plt.scatter(distances, elevations, color='red', marker='o', label='Cross-section')\n",
    "# plt.title(street_name)\n",
    "# plt.xlabel('Distance (m)')\n",
    "# plt.ylabel('Elevation (m)')\n",
    "# plt.legend()\n",
    "# plt.savefig(scatter_plot_filename)\n",
    "# plt.close()\n",
    "\n",
    "# # Encode the image to base64\n",
    "# with open(scatter_plot_filename, 'rb') as img_file:\n",
    "#     encoded_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "# # Add the scatter plot image to the map popup\n",
    "# popup_content = \"\"\"\n",
    "#     <h4>{}</h4>\n",
    "#     <img src=\"data:image/png;base64,{}\" alt=\"Scatter Plot\" width=\"400px\">\n",
    "# \"\"\".format(street_name, encoded_image)\n",
    "\n",
    "# popup = folium.Popup(html=popup_content, max_width=500)\n",
    "# folium.GeoJson(road_gdf,popup=popup,lazy=True).add_to(mymap)\n",
    "\n",
    "# # Save the Folium map as an HTML file\n",
    "# #map_filename = 'road_map_with_scatter_plot.html'\n",
    "# #mymap.save(map_filename)\n",
    "\n",
    "# # Display the Folium map\n",
    "# mymap\n",
    "\n",
    "# ## Set to zero for now (we'll update these once the rest of the paper is intergrated)\n",
    "# links.columns\n",
    "# links.columns\n",
    "# # #example point grade calculation for finding rapid changes\n",
    "# # linkid = 40\n",
    "\n",
    "# # point1 = Point(interpolated_points_dict[linkid]['geometry'][0])\n",
    "# # point2 = Point(interpolated_points_dict[linkid]['geometry'][1])\n",
    "# # point1.distance(point2)\n",
    "\n",
    "# # elev1 = interpolated_points_dict[linkid]['elevations'][0]\n",
    "# # elev2 = interpolated_points_dict[linkid]['elevations'][1]\n",
    "\n",
    "# # grade = (elev2-elev1)/(point1.distance(point2))\n",
    "# # grade\n",
    "\n",
    "# # calculates point grade (deprecated)\n",
    "# # item = interpolated_points_dict[linkid]\n",
    "# # grades = []\n",
    "\n",
    "# # for x in range(0,len(item['elevations'])-1):\n",
    "# #     point1 = Point(item['geometry'][x])\n",
    "# #     point2 = Point(item['geometry'][x+1])\n",
    "\n",
    "# #     elev1 = item['elevations'][x]\n",
    "# #     elev2 = item['elevations'][x+1]\n",
    "\n",
    "# #     grade = (elev2-elev1)/(point1.distance(point2))\n",
    "# #     grades.append(grade)\n",
    "\n",
    "# # grades = np.asarray(grades)\n",
    "# # grades[grades>0].mean()\n",
    "\n",
    "# # # calculate average point grade\n",
    "# # # 3 mins\n",
    "\n",
    "# # grades_dict = {}\n",
    "\n",
    "# # for key, item in tqdm(interpolated_points_dict.items(),total=len(interpolated_points_dict)): \n",
    "# #     grades = []\n",
    "\n",
    "\n",
    "# #     for x in range(0,len(item['elevations'])-1):\n",
    "# #         point1 = Point(item['geometry'][x])\n",
    "# #         point2 = Point(item['geometry'][x+1])\n",
    "\n",
    "# #         elev1 = item['elevations'][x]\n",
    "# #         elev2 = item['elevations'][x+1]\n",
    "\n",
    "# #         grade = (elev2-elev1)/(point1.distance(point2))\n",
    "# #         grades.append(grade)\n",
    "\n",
    "# #     grades = np.asarray(grades)\n",
    "# #     grades_dict[key] = grades[grades>0].mean()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
