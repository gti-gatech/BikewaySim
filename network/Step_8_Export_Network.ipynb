{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Prepare REVISED 4/08/24\n",
    "In this step, we add signals, bicycle facilities (with dates), and elevation attributes (avg up/down slop), remove links where cyclists are not allowed, and create a psuedo graph edge list for modeling turns.\n",
    "\n",
    "Further refinement of the attributes for impedance calibration pruposes should be done in the `impedance_calibration` module. This includes reconciling the different attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import src.modeling_turns as modeling_turns\n",
    "import src.add_attributes as add_attributes\n",
    "import src.prepare_network as prepare_network\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,str(Path.cwd().parent))\n",
    "import file_structure_setup\n",
    "config = file_structure_setup.filepaths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import network links and add attributes back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = gpd.read_file(config['network_fp'] /'networks.gpkg',layer='osm_links')\n",
    "og_cols = links.columns\n",
    "nodes = gpd.read_file(config['network_fp'] / 'networks.gpkg',layer='osm_nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #temporary\n",
    "# #also give raceways this\n",
    "# links.loc[links['highway']=='raceway','link_type'] = 'no_access_or_private'\n",
    "# links.loc[links['highway'].isin(['construction','proposed']),'link_type'] = 'construction or proposed'\n",
    "# links[og_cols].to_file(config['network_fp'] /'networks.gpkg',layer='osm_links')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197082 links 6877.0 miles 150882 nodes\n"
     ]
    }
   ],
   "source": [
    "#basic stats\n",
    "print(links.shape[0],'links',(links.length.sum() / 5280).round(0),'miles',nodes.shape[0],'nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "issubclass() arg 1 must be a class",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#types and length\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m summary_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43mlinks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlink_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      3\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength_mi\u001b[39m\u001b[38;5;124m'\u001b[39m:links\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink_type\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mlength\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m5280\u001b[39m),\n\u001b[0;32m      4\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength_pct\u001b[39m\u001b[38;5;124m'\u001b[39m:links\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink_type\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mlength\u001b[38;5;241m.\u001b[39msum()) \u001b[38;5;241m/\u001b[39m links\u001b[38;5;241m.\u001b[39mlength\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m})\n\u001b[0;32m      5\u001b[0m summary_df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength_mi\u001b[39m\u001b[38;5;124m'\u001b[39m,ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\tpassmore6\\Anaconda3\\envs\\geo-env\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1828\u001b[0m, in \u001b[0;36mGroupBy.size\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1825\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;66;03m# GH28330 preserve subclassed Series/DataFrames through calls\u001b[39;00m\n\u001b[1;32m-> 1828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43missubclass\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSeries\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1829\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_1d_constructor(result, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1830\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: issubclass() arg 1 must be a class"
     ]
    }
   ],
   "source": [
    "#types and length\n",
    "summary_df = pd.DataFrame({'size':links.groupby('link_type').size(),\n",
    "                           'length_mi':links.groupby('link_type')['geometry'].apply(lambda x: x.length.sum() / 5280),\n",
    "                           'length_pct':links.groupby('link_type')['geometry'].apply(lambda x: x.length.sum()) / links.length.sum() * 100})\n",
    "summary_df.sort_values('length_mi',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size          129627.000000\n",
       "length_mi       4267.800074\n",
       "length_pct       100.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and add attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add attributes back (especially the oneway column)\n",
    "osm_attrs = gpd.read_file(config['osmdwnld_fp'] / f\"osm_{config['geofabrik_year']}.gpkg\",layer='raw')#ignore_geometry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60908"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_attrs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4598.927912949249"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get basic stats\n",
    "osm_attrs.to_crs(links.crs,inplace=True)\n",
    "(osm_attrs.length / 5280).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.merge(links,osm_attrs.drop(columns=['geometry']),left_on='osmid',right_on='osmid')\n",
    "del osm_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn oneway into boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links['oneway'] = links['oneway'].isin(['yes','-1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add bicycle infrastructure and dates (city of atlanta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO temporary version while i get the dates added\n",
    "# cycling_infra_dates = gpd.read_file(config['bicycle_facilities_fp']/'osm_cycleways_w_dates.gpkg',layer='test_dates',ignore_geometry=True)\n",
    "# links = pd.merge(links,cycling_infra_dates[['osmid','facility_fwd','facility_rev','year']],on='osmid',how='left')\n",
    "# del cycling_infra_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add bicycle infrastructure (savannah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_cycleways = gpd.read_file(config['bicycle_facilities_fp'] / 'reference_layers.gpkg',layer='osm_cycleways')\n",
    "links = pd.merge(links,osm_cycleways[['osmid','facility_fwd','facility_rev','year']],on='osmid',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add network improvements (atlanta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improvements = gpd.read_file(config['bicycle_facilities_fp']/'network_improvements.gpkg',layer='coa',ignore_geometry=True)\n",
    "# links = pd.merge(links,improvements,left_on='osm_linkid',right_on='linkid',how='left')\n",
    "# links.drop(columns=['linkid'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add network improvements (savannah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improvements = gpd.read_file(config['bicycle_facilities_fp']/'network_improvements.gpkg',layer='savannah',ignore_geometry=True)\n",
    "links = pd.merge(links,improvements,on='osm_linkid',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links.drop(columns=['linkid'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add GDOT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdot_lanes = gpd.read_file(config['network_fp']/\"conflation.gpkg\",layer=\"gdot_lanes\",ignore_geometry=True)\n",
    "gdot_traffic = gpd.read_file(config['network_fp']/\"conflation.gpkg\",layer=\"gdot_traffic\",ignore_geometry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.merge(links,gdot_lanes,on=\"osmid\",how='left')\n",
    "links = pd.merge(links,gdot_traffic,on='osmid',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['osm_A', 'osm_B', 'osm_linkid', 'link_type', 'osmid', 'geometry',\n",
       "       'timestamp', 'version', 'type', 'highway', 'oneway', 'name', 'bridge',\n",
       "       'tunnel', 'cycleway', 'service', 'footway', 'sidewalk', 'bicycle',\n",
       "       'foot', 'access', 'area', 'all_tags', 'geom_type', 'facility_fwd',\n",
       "       'facility_rev', 'year', 'savannah_id', 'improvement', 'route_type',\n",
       "       'gdot_lanes', 'AADT', 'truck_pct'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add HERE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here = gpd.read_file(config['network_fp']/\"conflation.gpkg\",layer=\"here\",ignore_geometry=True)\n",
    "# links = pd.merge(links,here,on='osm_linkid',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Bike Ottawa LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lts_paths = (Path.home() / \"Documents/GitHub/stressmodel/data\").glob(\"*.json\")\n",
    "# lts_list = []\n",
    "# for lts_path in lts_paths:\n",
    "#     lts_edges = gpd.read_file(lts_path,ignore_geometry=True)\n",
    "#     lts_edges['osmid'] = lts_edges['id'].str.split('/').apply(lambda x: x[1]).astype(int)\n",
    "#     lts = int(lts_path.name.split('_')[1].split('.')[0])\n",
    "#     lts_edges['lts'] = lts\n",
    "#     lts_list.append(lts_edges[['osmid','lts']])\n",
    "# lts_df = pd.concat(lts_list)\n",
    "# links = pd.merge(links,lts_df,on='osmid',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add elevation data\n",
    "Assign the correct direction, then flip signs for reverse links later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation = gpd.read_file(config['network_fp']/'elevation.gpkg',layer='elevation',ignore_geometry=True)\n",
    "ascent_columns = [col for col in elevation.columns if \"ascent\" in col]\n",
    "descent_columns = [col for col in elevation.columns if \"descent\" in col]\n",
    "links = pd.merge(links,elevation[['osm_linkid']+ascent_columns+descent_columns],on='osm_linkid',how='left')\n",
    "del elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.rename(columns={'osm_A':'A','osm_B':'B','osm_linkid':'linkid'},inplace=True)\n",
    "nodes.rename(columns={'osm_N':'N'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate link lengths\n",
    "links['length_ft'] = links.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove non-routable links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links['link_type'].unique()\n",
    "keep_type = ['road', 'service', 'parking_and_driveways', 'pedestrian', 'bike', 'sidewalk_or_crossing']\n",
    "links = links[links['link_type'].isin(keep_type)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove isolated links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before connected components: Links 111949 Nodes 106786\n",
      "After connected components: Links 107397 Nodes 91820\n"
     ]
    }
   ],
   "source": [
    "links, nodes = prepare_network.largest_comp_and_simplify(links,nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create reverse links and turn dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO change this to not create the turn graph (just make it an extra optional step)\n",
    "## Create turn graph dataframe\n",
    "directed_links, turns_df = modeling_turns.create_pseudo_dual_graph(links,'A','B','linkid','oneway')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add signals (do later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source_A', 'source_B', 'target_A', 'target_B', 'source_reverse_link',\n",
       "       'source_linkid', 'source_azimuth', 'target_reverse_link',\n",
       "       'target_linkid', 'target_azimuth', 'azimuth_change', 'turn_type',\n",
       "       'signalized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_signals = pd.read_parquet(config['network_fp']/'osm_signals.parquet')\n",
    "osm_signals.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     2318\n",
       "False      30\n",
       "Name: signalized, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_signals['signalized'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "turns_df = pd.merge(turns_df,osm_signals[['source_linkid','source_reverse_link','target_linkid','target_reverse_link','signalized']],on=['source_linkid','source_reverse_link','target_linkid','target_reverse_link'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "turns_df.loc[turns_df['signalized'].isna(),'signalized'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    378982\n",
       "True       2052\n",
       "Name: signalized, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turns_df['signalized'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding stressful intersections\n",
    "# import pandas as pd\n",
    "# highway_order = {\n",
    "#     'trunk': 0,\n",
    "#     'trunk_link': 1,\n",
    "#     'primary': 2,\n",
    "#     'primary_link': 3,\n",
    "#     'secondary': 4,\n",
    "#     'secondary_link': 5,\n",
    "#     'tertiary': 6,\n",
    "#     'tertiary_link': 7,\n",
    "#     'unclassified': 8,\n",
    "#     'residential': 9\n",
    "# }\n",
    "# highway_order = pd.Series(highway_order)\n",
    "# highway_order = highway_order.reset_index()\n",
    "# highway_order.columns = ['highway','order']\n",
    "# #add highway ranking based on the above\n",
    "# pseudo_df['target_highway_order'] = pseudo_df['target_highway'].map(highway_order.set_index('highway')['order'])\n",
    "# pseudo_df['source_highway_order'] = pseudo_df['source_highway'].map(highway_order.set_index('highway')['order'])\n",
    "# #remove straight and uturn\n",
    "# cond1 = pseudo_df['turn_type'].isin(['left','right'])\n",
    "# #only road to road for now\n",
    "# cond2 = (pseudo_df['source_link_type'] == 'road') & (pseudo_df['target_link_type'] == 'road')\n",
    "# cross_streets = pseudo_df[cond1 & cond2]\n",
    "\n",
    "# #use groupby to find the max target_highway order\n",
    "# cross_streets = cross_streets.groupby(['source_linkid','source_A','source_B'])['target_highway_order'].min()\n",
    "# cross_streets.name = 'cross_street'\n",
    "\n",
    "# #add to main df\n",
    "# pseudo_df = pd.merge(pseudo_df,cross_streets,left_on=['source_linkid','source_A','source_B'],right_index=True,how='left')\n",
    "\n",
    "# #change numbers back to normal\n",
    "# pseudo_df['cross_street_order'] = pseudo_df['cross_street']\n",
    "# pseudo_df['cross_street'] = pseudo_df['cross_street'].map(highway_order.set_index('order')['highway'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add directional attributes and flip as needed\n",
    "directed_links = pd.merge(directed_links,links[['linkid','facility_fwd','facility_rev']+ascent_columns+descent_columns],on='linkid')\n",
    "directed_links.loc[directed_links['reverse_link']==True,ascent_columns+descent_columns] = directed_links.loc[directed_links['reverse_link']==True,descent_columns+ascent_columns].values\n",
    "directed_links.loc[directed_links['reverse_link']==True,['facility_fwd','facility_rev']] = directed_links.loc[directed_links['reverse_link']==True,['facility_rev','facility_fwd']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple columns not compatible with parquet\n",
    "turns_df.drop(columns=['source','target'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "turns_df.to_parquet(config['network_fp']/'turns_df.parquet')\n",
    "directed_links.to_parquet(config['network_fp']/'directed_edges.parquet')\n",
    "links.to_file(config['network_fp']/'final_network.gpkg',layer='edges')\n",
    "nodes.to_file(config['network_fp']/'final_network.gpkg',layer='nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional add geo data to turns and export for examination\n",
    "from shapely.ops import MultiLineString\n",
    "geo_dict = dict(zip(links['linkid'],links['geometry']))\n",
    "turns_df['source_geo'] = turns_df['source_linkid'].map(geo_dict)\n",
    "turns_df['target_geo'] = turns_df['target_linkid'].map(geo_dict)\n",
    "turns_df['geometry'] = turns_df.apply(lambda row: MultiLineString([row['source_geo'],row['target_geo']]),axis=1)\n",
    "turns_df.drop(columns=['source_geo','target_geo'],inplace=True)\n",
    "turns_gdf = gpd.GeoDataFrame(turns_df,crs=links.crs)\n",
    "turns_gdf.drop(columns=['source','target']).to_file(config['network_fp']/'final_network.gpkg',layer='turns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO serialize the attributes to add as needed?\n",
    "# with (config['network_fp'] / 'edges_with_attributes.pkl').open('wb') as fh:\n",
    "#     pickle.dump(links,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with (config['network_fp'] / 'edges.pkl').open('wb') as fh:\n",
    "#     pickle.dump(links,fh,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with (config['network_fp'] / 'nodes.pkl').open('wb') as fh:\n",
    "#     pickle.dump(nodes,fh,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with (config['network_fp'] / 'directed_edges.pkl').open('wb') as fh:\n",
    "#     pickle.dump(edges,fh,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with (config['network_fp'] / 'turn_df.pkl').open('wb') as fh:\n",
    "#     pickle.dump(turn_df,fh,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated past here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #add attributes back and then flip elevation/bicyccle attributes\n",
    "# #do so i don't have to re-flip everytime i import? could potentially save memory though\n",
    "# #TODO it would still be smarter to store as a dict or something\n",
    "# edges\n",
    "# links.columns\n",
    "# edges = pd.merge(edges,links.drop(columns=['A','B']),on='linkid')\n",
    "# #if reverse_geo == true then ascent should be descent and vice versa\n",
    "# # loops have reverse_geometry is np.nana\n",
    "# # assume that all elevation columns will be paired by what is after ascent/descent\n",
    "# elevation_columns = ['ascent_m', 'descent_m', 'ascent_grade_%','descent_grade_%']\n",
    "# # Remove elements containing \"ascent\" or \"descent\"\n",
    "# cleaned_columns = [col for col in elevation_columns if \"ascent\" not in col and \"descent\" not in col]    \n",
    "# # Remove duplicates by converting the list to a set and back to a list\n",
    "# cleaned_columns = list(set(cleaned_columns))\n",
    "\n",
    "# for cleaned_column in cleaned_columns:\n",
    "#     #swap if reverse geometry == true\n",
    "#     links.loc[links['reverse_geometry']==True,ascent_columns+descent_columns] = links.loc[links['reverse_geometry']==True,descent_columns+ascent_columns]\n",
    "    \n",
    "    \n",
    "#     df_edges[] = np.where(df_edges['reverse_link'], df_edges[elev_columns[1]].abs(), df_edges[elev_columns[0]])\n",
    "#     #drop the down version?\n",
    "#     df_edges.drop(columns=elev_columns[1],inplace=True)\n",
    "# #if reverse_geo == true then ascent should be descent and vice versa\n",
    "# # loops have reverse_geometry is np.nan\n",
    "# # assume that all elevation columns will be paired by what is after ascent/descent\n",
    "# elevation_columns = ['ascent_m', 'descent_m', 'ascent_grade_%','descent_grade_%']\n",
    "# # Remove elements containing \"ascent\" or \"descent\"\n",
    "# cleaned_columns = [col for col in elevation_columns if \"ascent\" not in col and \"descent\" not in col]    \n",
    "# # Remove duplicates by converting the list to a set and back to a list\n",
    "# cleaned_columns = list(set(cleaned_columns))\n",
    "\n",
    "# for cleaned_column in cleaned_columns:\n",
    "#     #swap\n",
    "    \n",
    "    \n",
    "    \n",
    "#     df_edges[] = np.where(df_edges['reverse_link'], df_edges[elev_columns[1]].abs(), df_edges[elev_columns[0]])\n",
    "#     #drop the down version?\n",
    "#     df_edges.drop(columns=elev_columns[1],inplace=True)\n",
    "# ## Rename columns\n",
    "\n",
    "# links.rename(columns={'osm_A':'A','osm_B':'B','osm_linkid':'linkid'},inplace=True)\n",
    "# nodes.rename(columns={'osm_N':'N'},inplace=True)\n",
    "# ## \n",
    "# ## Create turn graph dataframe\n",
    "# edges, turn_df = modeling_turns.create_pseudo_dual_graph(links,'A','B','linkid','oneway')\n",
    "# ## Flip attributes if needed (elevation, bicycle facilities)\n",
    "# Turns should be good as is\n",
    "# #add geo (needed for map matching part)\n",
    "# df_edges = df_edges.merge(links.drop(columns=['A','B']),on=['linkid'])\n",
    "# df_edges = gpd.GeoDataFrame(df_edges,geometry='geometry',crs=links.crs)\n",
    "# df_edges = df_edges.loc[:,~df_edges.columns.duplicated()].copy()\n",
    "# df_edges.reset_index(drop=True,inplace=True)\n",
    "# #just export the df_edges?\n",
    "# df_edges.to_file(export_fp/'Map_Matching/matching.gpkg',layer='edges')\n",
    "# nodes.to_file(export_fp/'Map_Matching/matching.gpkg',layer='nodes')\n",
    "# pseudo_df.columns\n",
    "# #add geo to the turns too\n",
    "# from shapely.ops import MultiLineString\n",
    "# pseudo_df = pseudo_df.merge(links[['linkid','geometry']],left_on='source_linkid',right_on='linkid')\n",
    "# pseudo_df = pseudo_df.merge(links[['linkid','geometry']],left_on='target_linkid',right_on='linkid')\n",
    "\n",
    "# geometry = pseudo_df.apply(lambda row: MultiLineString([row['geometry_x'],row['geometry_y']]),axis=1)\n",
    "# pseudo_df.drop(columns=['geometry_x','geometry_y','linkid_x','linkid_y'],inplace=True)\n",
    "# pseudo_df = gpd.GeoDataFrame(pseudo_df,geometry=geometry,crs=links.crs)\n",
    "\n",
    "# # pseudo_edges = pseudo_edges.loc[:,~pseudo_edges.columns.duplicated()].copy()\n",
    "# # pseudo_edges.reset_index(drop=True,inplace=True)\n",
    "# pseudo_df['source'] = pseudo_df['source'].astype(str)\n",
    "# pseudo_df['target'] = pseudo_df['target'].astype(str)\n",
    "# pseudo_df.to_file(export_fp/'Map_Matching/matching.gpkg',layer='turns')\n",
    "# #pickle the graph\n",
    "# with (export_fp / 'Map_Matching/turn_G.pkl').open('wb') as fh:\n",
    "#     pickle.dump(pseudo_G,fh)\n",
    "# # Come back to below later\n",
    "# # Network Prepare\n",
    "# This notebook prepares the final routing network.\n",
    "\n",
    "# 1. Import the desired routing network\n",
    "# 1. Add attributes\n",
    "# 1. Add reconciled attributes\n",
    "# 1. Add signals\n",
    "# 1. Add elevation\n",
    "\n",
    "# Then the network will be turned into a directed network graph complete with an edge list representing the directed edges and another one representing turns. Some attribute values are reversed to account for direction (e.g., elevation, signals).\n",
    "# Import the data from previous notebooks and merge them. Merge here so updates can be done at each step without having to repeat everything.\n",
    "# network_filepath = Path.home() / \"Documents/BikewaySimData/Projects/gdot/networks\"\n",
    "# #filtered data\n",
    "# links = gpd.read_file(network_filepath/'filtered.gpkg',layer='osm_links')\n",
    "# nodes = gpd.read_file(network_filepath/'filtered.gpkg',layer='osm_nodes')\n",
    "# links.columns\n",
    "# #add osm data\n",
    "# links = add_attributes.add_osm_attr(links,network_filepath / 'osm_attr.pkl')\n",
    "# #rename\n",
    "# links.rename(columns={'osm_A':'A','osm_B':'B','osm_linkid':'linkid'},inplace=True)\n",
    "# nodes.rename(columns={'osm_N':'N'},inplace=True)\n",
    "# links.columns\n",
    "# #reconciled data\n",
    "# reconciled = gpd.read_file(network_filepath/'reconciled.gpkg',layer='links',ignore_geometry=True)\n",
    "# #[col for col in reconciled.columns if col not in links.columns]\n",
    "# cols_to_keep = ['osm_linkid','speedlimit_range_mph','lanes_per_direction']\n",
    "# links = links.merge(reconciled[cols_to_keep],on='osm_linkid',how='left')\n",
    "# del reconciled\n",
    "# #rename\n",
    "# links.rename(columns={'osm_A':'A','osm_B':'B','osm_linkid':'linkid'},inplace=True)\n",
    "# nodes.rename(columns={'osm_N':'N'},inplace=True)\n",
    "# #signals added\n",
    "# links_w_signals = gpd.read_file(network_filepath/'signals_added.gpkg',layer='links',ignore_geometry=True)\n",
    "\n",
    "# nodes_w_signals = gpd.read_file(network_filepath/'signals_added.gpkg',layer='nodes',ignore_geometry=True)\n",
    "# nodes_w_signals\n",
    "# #TODO change linkid to osm_linkid later\n",
    "# cols_to_keep = ['linkid','signal_A','signal_B']\n",
    "# links = links.merge(links_w_signals[cols_to_keep],on='linkid',how='left')\n",
    "# ##del nodes_w_signals\n",
    "\n",
    "# #elevation added\n",
    "# links_w_elevation = gpd.read_file(network_filepath/'elevation_added.gpkg',ignore_geometry=True)\n",
    "# links_w_elevation.columns\n",
    "# links_w_elevation.rename(columns={\n",
    "#     'a_s_c_e_n_t___m':'ascent_m',\n",
    "#     'd_e_s_c_e_n_t___m':'descent_m',\n",
    "#     'a_s_c_e_n_t___g_r_a_d_e':'ascent_grade',\n",
    "#     'd_e_s_c_e_n_t___g_r_a_d_e':'descent_grade',\n",
    "# }, inplace =True)\n",
    "# cols_to_keep = ['linkid','ascent_m','descent_m','ascent_grade','descent_grade','(0,2]_descent',\n",
    "#        '(2,4]_descent', '(4,6]_descent', '(6,10]_descent', '(10,15]_descent',\n",
    "#        '(15,inf]_descent', '(0,2]_ascent', '(2,4]_ascent', '(4,6]_ascent',\n",
    "#        '(6,10]_ascent', '(10,15]_ascent', '(15,inf]_ascent']\n",
    "# links = links.merge(links_w_elevation[cols_to_keep],on='linkid')\n",
    "# del links_w_elevation\n",
    "# links.columns\n",
    "# fp = Path.home() / \"Documents/BikewaySimData/Projects/gdot\"\n",
    "# edges = gpd.read_file(fp/'networks/elevation_added.gpkg',layer=\"links\")\n",
    "\n",
    "\n",
    "# edges.columns\n",
    "# #use geometry one last time\n",
    "# edges['length_ft'] = edges.length\n",
    "\n",
    "# #turn bridge and tunnel to boolean values\n",
    "# edges['tunnel'] = edges['tunnel'].notna()\n",
    "# edges['bridge'] = edges['bridge'].notna()\n",
    "# #turn bike facil into one column\n",
    "# edges['bike_facility_type'] = np.nan\n",
    "# edges.loc[(edges['mu'] == 1) & (edges['bike_facility_type'].isna()),'bike_facility_type'] = 'shared-use path'\n",
    "# edges.loc[(edges['pbl'] == 1) & (edges['bike_facility_type'].isna()),'bike_facility_type'] = 'protected bike lane'\n",
    "# edges.loc[(edges['bl'] == 1) & (edges['bike_facility_type'].isna()),'bike_facility_type'] = 'bike lane'\n",
    "# df_edges, pseudo_df, pseudo_G = modeling_turns.create_pseudo_dual_graph(edges,'A','B','linkid','oneway',True)\n",
    "# ## Add desired attributes from links to df_edges\n",
    "# #df_edges = df_edges.merge(edges[['linkid','geometry']])\n",
    "\n",
    "# basic_cols = ['linkid', 'osmid', 'link_type', 'name', 'oneway','length_ft']\n",
    "\n",
    "# #anything that's an instance or would be better as a count value (but not a turn)\n",
    "# event_cols = ['bridge','tunnel']\n",
    "\n",
    "# #anything that's for the duration of the entire link and has categories\n",
    "# category_cols = ['link_type','highway','speedlimit_range_mph',\n",
    "#                'lanes_per_direction','bike_facility_type']\n",
    "\n",
    "# #reverse in tuple form (these need to be flipped if going the other direction)\n",
    "# rev_columns = [('ascent_m','descent_m'),\n",
    "#                ('ascent_grade','descent_grade'),\n",
    "#                ('(0,2]_ascent','(0,2]_descent'),\n",
    "#                ('(2,4]_ascent','(2,4]_descent'),\n",
    "#                ('(4,6]_ascent','(4,6]_descent'),\n",
    "#                ('(6,10]_ascent','(6,10]_descent'),\n",
    "#                ('(10,15]_ascent','(10,15]_descent'),\n",
    "#                ('(15,inf]_ascent','(15,inf]_descent')]\n",
    "\n",
    "# from itertools import chain\n",
    "# keep_cols = basic_cols + event_cols + category_cols + list(chain(*rev_columns))\n",
    "# # attrs = ['linkid', 'osmid', 'link_type', 'name', 'highway',\n",
    "# #        'bridge', 'tunnel', 'bl', 'pbl', 'mu','speedlimit_range_mph',\n",
    "# #        'lanes_per_direction', 'up_grade', 'down_grade', 'length_ft',\n",
    "# #        'vehicle_separation','geometry']\n",
    "# df_edges = df_edges.merge(edges[keep_cols],on='linkid',how='left')\n",
    "# df_edges\n",
    "# ## Deal with grade\n",
    "# Need to flip sign of grade for reverse links\n",
    "# # def combine_up_down_tuples(lst):\n",
    "# #     result = []\n",
    "# #     current_tuple = []\n",
    "\n",
    "# #     for item in lst:\n",
    "# #         if 'ascent' in item or 'descent' in item:\n",
    "# #             current_tuple.append(item)\n",
    "# #             if len(current_tuple) == 2:\n",
    "# #                 result.append(tuple(current_tuple))\n",
    "# #                 current_tuple = []\n",
    "\n",
    "# #     return result\n",
    "\n",
    "# # rev_columns = ['ascent_m','descent_m','ascent_grade','descent_grade',\n",
    "# #                '(0,2]_down', '(2,4]_down', '(4,6]_down',\n",
    "# #                '(6,10]_down', '(10,15]_down','(15,inf]_down',\n",
    "# #                '(0,2]_up', '(2,4]_up', '(4,6]_up', '(6,10]_up',\n",
    "# #                '(10,15]_up', '(15,inf]_up'\n",
    "# #                ]\n",
    "\n",
    "# # combined_tuples = combine_up_down_tuples(rev_columns)\n",
    "\n",
    "# for elev_columns in rev_columns:\n",
    "#     df_edges[elev_columns[0]] = np.where(df_edges['reverse_link'], df_edges[elev_columns[1]].abs(), df_edges[elev_columns[0]])\n",
    "#     #drop the down version?\n",
    "#     df_edges.drop(columns=elev_columns[1],inplace=True)\n",
    "# ## Turns and Signals\n",
    "# #add additional attributes needed for processing\n",
    "# source_links = edges[['linkid','osmid','link_type','name','highway']]\n",
    "# target_links = edges[['linkid','osmid','link_type','name','highway']]\n",
    "# source_links.columns = 'source_' + source_links.columns\n",
    "# target_links.columns = 'target_' + target_links.columns\n",
    "# pseudo_df = pseudo_df.merge(source_links,on='source_linkid',how='left')\n",
    "# pseudo_df = pseudo_df.merge(target_links,on='target_linkid',how='left')\n",
    "# ## Turn Restrictions\n",
    "# Two types in OSM (represented as OSM relations):\n",
    "# - No (blank) turns\n",
    "# - Only this turn allowed\n",
    "\n",
    "# For chosen we don't need to consider turn restrictions\n",
    "# # turn_restrictions = pd.read_csv(fp.parent/'osm_turn_restrictions.csv')\n",
    "# # pseudo_df = pseudo_df.merge(turn_restrictions,left_on=['source_osmid','target_osmid'],right_on=['from_way_id','to_way_id'],how='left')\n",
    "# # road_cond = (pseudo_df['source_link_type'] == 'road') & (pseudo_df['target_link_type'] == 'road')\n",
    "# # no_restr = pseudo_df['type'] == 'no'\n",
    "# # only_restr = pseudo_df['type'] == 'only'\n",
    "\n",
    "# # #add a remove column\n",
    "# # pseudo_df['remove'] = False\n",
    "\n",
    "# # #remove the no turns\n",
    "# # pseudo_df.loc[road_cond & no_restr,'remove'] = True\n",
    "\n",
    "# # #for only, find all instances road_cond + from source and set to True\n",
    "# # sources = set(turn_restrictions.loc[turn_restrictions['type']=='only','from_way_id'].tolist())\n",
    "# # pseudo_df.loc[road_cond & pseudo_df['source_osmid'].isin(sources) & pseudo_df['type'].isna(),'remove'] = True\n",
    "\n",
    "# # #Remove these turns and drop the added columns\n",
    "# # print((pseudo_df['remove']==True).sum(),'turns removed')\n",
    "# # pseudo_df = pseudo_df[pseudo_df['remove']==False]\n",
    "# # pseudo_df.drop(columns=['relation_id', 'restriction', 'from_way_id',\n",
    "# #        'to_way_id', 'type', 'remove'],inplace=True)\n",
    "# # Deal with signals\n",
    "# Perform two merges and use the source/target reverse link columns to determine which signal ID to keep.\n",
    "# - For the source link, use signal_B if reverse == False else signal_A\n",
    "# - For the target link, use signal_A if reverse == False else signal_B\n",
    "# source = pseudo_df[['source_linkid','source_reverse_link']].merge(edges,left_on='source_linkid',right_on='linkid',how='left')\n",
    "# pseudo_df['source_signal'] = np.where(source['source_reverse_link'], source['signal_A'], source['signal_B'])\n",
    "\n",
    "# target = pseudo_df[['target_linkid','target_reverse_link']].merge(edges,left_on='target_linkid',right_on='linkid',how='left')\n",
    "# pseudo_df['target_signal'] = np.where(target['target_reverse_link']==False, target['signal_B'], target['signal_A'])\n",
    "# ## Identifying signalized/unsignalized turns\n",
    "# - Only look at roads for now\n",
    "# - Filter to left/right turns per source linkid per direction\n",
    "# - Take the highest road classification and assign it as the cross street road classification\n",
    "# import pandas as pd\n",
    "# highway_order = {\n",
    "#     'trunk': 0,\n",
    "#     'trunk_link': 1,\n",
    "#     'primary': 2,\n",
    "#     'primary_link': 3,\n",
    "#     'secondary': 4,\n",
    "#     'secondary_link': 5,\n",
    "#     'tertiary': 6,\n",
    "#     'tertiary_link': 7,\n",
    "#     'unclassified': 8,\n",
    "#     'residential': 9\n",
    "# }\n",
    "# highway_order = pd.Series(highway_order)\n",
    "# highway_order = highway_order.reset_index()\n",
    "# highway_order.columns = ['highway','order']\n",
    "# #add highway ranking based on the above\n",
    "# pseudo_df['target_highway_order'] = pseudo_df['target_highway'].map(highway_order.set_index('highway')['order'])\n",
    "# pseudo_df['source_highway_order'] = pseudo_df['source_highway'].map(highway_order.set_index('highway')['order'])\n",
    "# #remove straight and uturn\n",
    "# cond1 = pseudo_df['turn_type'].isin(['left','right'])\n",
    "# #only road to road for now\n",
    "# cond2 = (pseudo_df['source_link_type'] == 'road') & (pseudo_df['target_link_type'] == 'road')\n",
    "# cross_streets = pseudo_df[cond1 & cond2]\n",
    "\n",
    "# #use groupby to find the max target_highway order\n",
    "# cross_streets = cross_streets.groupby(['source_linkid','source_A','source_B'])['target_highway_order'].min()\n",
    "# cross_streets.name = 'cross_street'\n",
    "\n",
    "# #add to main df\n",
    "# pseudo_df = pd.merge(pseudo_df,cross_streets,left_on=['source_linkid','source_A','source_B'],right_index=True,how='left')\n",
    "\n",
    "# #change numbers back to normal\n",
    "# pseudo_df['cross_street_order'] = pseudo_df['cross_street']\n",
    "# pseudo_df['cross_street'] = pseudo_df['cross_street'].map(highway_order.set_index('order')['highway'])\n",
    "# # TODO Add OSM crossing into this logic\n",
    "#     - Signals\n",
    "#         - Wait on this until we have the route attributes code done\n",
    "#         - Add crossings in signalization\n",
    "#         - Majority of crossings are nodes not ways\n",
    "#         - Cycleway crossings typically dealt the same way\n",
    "#         - If meeting nodes are both crossings and within the traffic signal buffer, they're signalized crossings\n",
    "#             - Or if both connecting links are crossings/connect to the road etc\n",
    "#         - Way attributes\n",
    "#             - Footway = crossing\n",
    "#             - Highway = footway\n",
    "#         - Node attributes\n",
    "#             - Crossing = * (traffic signals/marked/etc)\n",
    "#             - Highway = crossing\n",
    "#         - Link attributes\n",
    "#             - Some links are labeled as crossings but this is not as consistent\n",
    "\n",
    "# signalized = pseudo_df['source_signal'] == pseudo_df['target_signal']\n",
    "# left_or_straight =  pseudo_df['turn_type'].isin(['left','straight'])\n",
    "# both_road = (pseudo_df['source_link_type'] == 'road') & (pseudo_df['target_link_type'] == 'road')\n",
    "# cross_street = pseudo_df['cross_street_order'] <= 5\n",
    "\n",
    "# #signalized\n",
    "# pseudo_df.loc[signalized & both_road,'signalized'] = True\n",
    "# pseudo_df.loc[pseudo_df['signalized'].isna(),'signalized'] = False\n",
    "# # pseudo_df.loc[signalized & left_or_straight & both_road,'signalized_left_straight'] = True\n",
    "# # pseudo_df.loc[pseudo_df['signalized_left_straight'].isna(),'signalized_left_straight'] = False\n",
    "\n",
    "# pseudo_df.loc[-signalized & both_road & cross_street,'unsignalized'] = True\n",
    "# pseudo_df.loc[pseudo_df['unsignalized'].isna(),'unsignalized'] = False\n",
    "\n",
    "# #clean up\n",
    "# rem =  ['source_osmid', 'source_link_type', 'source_name',\n",
    "#        'source_highway', 'target_osmid', 'target_link_type', 'target_name',\n",
    "#        'target_highway', 'source_signal', 'target_signal',\n",
    "#        'target_highway_order', 'source_highway_order', 'cross_street',\n",
    "#        'cross_street_order']\n",
    "# pseudo_df.drop(columns=rem,inplace=True)\n",
    "# # Export for impedance calibration\n",
    "\n",
    "# # df_edges = gpd.GeoDataFrame(df_edges,crs='epsg:2240')\n",
    "# df_edges.columns\n",
    "# with (fp.parent / 'chosen.pkl').open('wb') as fh:\n",
    "#     export = (df_edges,pseudo_df,pseudo_G)\n",
    "#     pickle.dump(export,fh)\n",
    "# ## Add geometry to examine results in QGIS\n",
    "# #add geo\n",
    "# link_geo = dict(zip(links['linkid'],links['geometry']))\n",
    "# pseudo_df['src_geo'] = pseudo_df['source_linkid'].map(link_geo)\n",
    "# pseudo_df['trgt_geo'] = pseudo_df['target_linkid'].map(link_geo)\n",
    "# pseudo_df['geometry'] = pseudo_df[['src_geo','trgt_geo']].apply(lambda row: MultiLineString([row['src_geo'],row['trgt_geo']]),axis=1)\n",
    "\n",
    "# pseudo_df.drop(columns=['src_geo','trgt_geo'],inplace=True)\n",
    "# pseudo_df = gpd.GeoDataFrame(pseudo_df,crs=links.crs)\n",
    "\n",
    "# pseudo_df['source'] = pseudo_df['source'].astype(str)\n",
    "# pseudo_df['target'] = pseudo_df['target'].astype(str)\n",
    "\n",
    "# #check results (may need a smaller road network to test on)\n",
    "# pseudo_df.to_file(Path.home()/'Downloads/testing.gpkg',layer='cross_streets')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
