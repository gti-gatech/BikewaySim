{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Prepare\n",
    "This step adds in the supplemental attributes prepared in the previous notebooks (e.g., traffic signals, bicycle facilities (with approx. install dates), elevation, etc.). Links where cyclists are absolutely not allowed to travel (Interstates) are removed. A psuedo dual graph for modeling turn movements is created, and the links, nodes, and turns are exported for further processing in the `impedance_calibration` module.\n",
    "\n",
    "For the GDOT/NCST projects, the following attributes were available: (TURN THIS INTO A TABLE LIKE IN THE REPORT LATER)\n",
    "- Length\n",
    "- Grade/Elevation\n",
    "- Bike Facility w Dates\n",
    "- Oneway\n",
    "- Signals\n",
    "- AADT\n",
    "- Truck %\n",
    "- Lanes\n",
    "- Speed Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from shapely.ops import Point\n",
    "\n",
    "from bikewaysim.paths import config\n",
    "from bikewaysim.network import modeling_turns, add_attributes, prepare_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import network links and add attributes back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = gpd.read_file(config['network_fp'] /'networks.gpkg',layer='osm_links')\n",
    "og_cols = links.columns\n",
    "nodes = gpd.read_file(config['network_fp'] / 'networks.gpkg',layer='osm_nodes')\n",
    "\n",
    "#TODO modify network filter to stop adding the network name like this\n",
    "links.rename(columns={'osm_A':'A','osm_B':'B','osm_linkid':'linkid'},inplace=True)\n",
    "nodes.rename(columns={'osm_N':'N'},inplace=True)\n",
    "\n",
    "#calculate link lengths\n",
    "links['length_ft'] = links.length\n",
    "\n",
    "#basic stats\n",
    "print(links.shape[0],'links',(links.length.sum() / 5280).round(0),'miles',nodes.shape[0],'nodes')\n",
    "\n",
    "#types and lengths\n",
    "summary_df = pd.DataFrame({'size':links['link_type'].value_counts(),\n",
    "                           'length_mi':links.groupby('link_type')['geometry'].apply(lambda x: x.length.sum() / 5280),\n",
    "                           'length_pct':links.groupby('link_type')['geometry'].apply(lambda x: x.length.sum()) / links.length.sum() * 100})\n",
    "print(summary_df.sort_values('length_mi',ascending=False))\n",
    "\n",
    "#add osm attributes back (especially the oneway column)\n",
    "osm_attrs = gpd.read_file(config['osmdwnld_fp'] / f\"osm.gpkg\",layer='raw')\n",
    "\n",
    "# # get basic stats\n",
    "osm_attrs.to_crs(links.crs,inplace=True)\n",
    "print('Raw network is',(osm_attrs.length / 5280).sum().round(0),'miles')\n",
    "links = pd.merge(links,osm_attrs.drop(columns=['oneway','geometry']),on='osmid')\n",
    "del osm_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add bicycle infrastructure and approximate date of opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycling_infra_dates = gpd.read_file(config['bicycle_facilities_fp']/'osm_cycleways_w_dates.gpkg',layer='dates_network')\n",
    "links = pd.merge(links,cycling_infra_dates[['osm_linkid','facility_fwd','facility_rev','facility','year']],left_on='linkid',right_on='osm_linkid',how='left')\n",
    "links.drop(columns=['osm_linkid'],inplace=True)\n",
    "(links.groupby(['facility','year'])['length_ft'].sum() / 5280).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add adjacent multi-use paths and cycletracks to roads as an attribute and vice versa. Think Stone Mountain Trail or Beltline next to Wylie Street."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mups_and_cycletrack = gpd.read_file(config['bicycle_facilities_fp']/'osm_cycleways_w_dates.gpkg',layer='dates_network')\n",
    "mups_and_cycletracks = cycling_infra_dates.loc[cycling_infra_dates['link_type']!='road',['facility','year','geometry']]\n",
    "roads = links.loc[links['link_type']=='road',['linkid','geometry']].copy()\n",
    "roads['og_length'] = roads.length\n",
    "\n",
    "# get azimuth for getting angle change\n",
    "roads.to_crs('epsg:4326',inplace=True)\n",
    "roads[['fwd_azimuth','bck_azimuth']] = roads.apply(lambda row: modeling_turns.find_azimuth(row),axis=1)\n",
    "roads.to_crs(config['projected_crs_epsg'],inplace=True)\n",
    "\n",
    "mups_and_cycletracks.to_crs('epsg:4326',inplace=True)\n",
    "mups_and_cycletracks[['fwd_azimuth','bck_azimuth']] = mups_and_cycletracks.apply(lambda row: modeling_turns.find_azimuth(row),axis=1)\n",
    "mups_and_cycletracks.to_crs(config['projected_crs_epsg'],inplace=True)\n",
    "\n",
    "# buffer by small amount\n",
    "mups_and_cycletracks.set_geometry(mups_and_cycletracks.buffer(50),inplace=True)\n",
    "\n",
    "# intersect\n",
    "intersection = gpd.overlay(roads,mups_and_cycletracks)\n",
    "\n",
    "# calculate coverage and angle change (hausdorff distance returns too many false positives)\n",
    "intersection['new_length'] = intersection.length\n",
    "intersection['ratio'] = intersection['new_length']/intersection['og_length']\n",
    "\n",
    "# angle difference (take min to account for direction differences)\n",
    "intersection['diff1'] = np.abs(intersection['fwd_azimuth_1'] - intersection['bck_azimuth_2'])\n",
    "intersection['diff2'] = np.abs(intersection['fwd_azimuth_1'] - intersection['fwd_azimuth_2'])\n",
    "intersection['mindiff'] = intersection[['diff1','diff2']].min(axis=1)\n",
    "\n",
    "# set minimum conditions for accepting\n",
    "cond0 = intersection['ratio'] > 0.95 # this much coverage of the original link\n",
    "cond1 = intersection['mindiff'] < 30 # no more than this change in angle\n",
    "intersection = intersection[cond0&cond1]\n",
    "\n",
    "# just take the one with the most overlap after that\n",
    "has_sidepath = intersection.loc[intersection.groupby('linkid')['ratio'].idxmax(),['linkid','facility','year','geometry']]\n",
    "has_sidepath.rename(columns={'facility':'sidepath','year':'sidepath_year'},inplace=True)\n",
    "\n",
    "has_sidepath.to_file(config['bicycle_facilities_fp']/'sidepaths.gpkg',layer='sidepaths')\n",
    "\n",
    "# merge back into main network dataframe\n",
    "links = pd.merge(links,has_sidepath.drop(columns='geometry'),on='linkid',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove off street infrastructure built after 2016. Some of these may have still existed as informal dirt paths (Beltline). In that case add them back in manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_year = 2016\n",
    "max_year_cond = links['year'] > max_year\n",
    "\n",
    "links.loc[max_year_cond].to_file(config['bicycle_facilities_fp']/'removed_bicycle_infra.gpkg')\n",
    "\n",
    "# # remove infra before 2016 so it doesn't match to these\n",
    "links.loc[max_year_cond & (links['link_type']=='road'),'facility_fwd'] = None\n",
    "links.loc[max_year_cond & (links['link_type']=='road'),'facility_rev'] = None\n",
    "links.loc[max_year_cond & (links['link_type']=='road'),'facility'] = None\n",
    "\n",
    "#TODO reimplement this but don't remove links from the link database, just the graph so that we can do it dynamically\n",
    "after = links['facility'].isin(['cycletrack','multi use path']) & \\\n",
    "          (links['link_type']!='road') & \\\n",
    "          links['year'].notna() & \\\n",
    "          (links['year']>max_year)\n",
    "links = links[after==False]\n",
    "\n",
    "# set no facility values to null\n",
    "links.loc[links['facility_fwd'] == 'no facility','facility_fwd'] = None\n",
    "links.loc[links['facility_rev'] == 'no facility','facility_rev'] = None\n",
    "\n",
    "# nans to None\n",
    "links.loc[links['facility_fwd'].isna(),'facility_fwd'] = None\n",
    "links.loc[links['facility_rev'].isna(),'facility_rev'] = None\n",
    "links.loc[links['facility'].isna(),'facility'] = None\n",
    "\n",
    "# save in new column for reference for comparison\n",
    "# links['future_facility'] = links['facility_fwd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links['facility'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE there are going to be cases where there are bike lanes next to mups\n",
    "links[['facility','sidepath']].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.loc[links['sidepath_year']>max_year,'sidepath'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Network Improvements (in development)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atlanta Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improvements = gpd.read_file(config['bicycle_facilities_fp']/'network_improvements.gpkg',layer='coa',ignore_geometry=True)\n",
    "# links = pd.merge(links,improvements,left_on='osm_linkid',right_on='linkid',how='left')\n",
    "# links.drop(columns=['linkid'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Savannah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improvements = gpd.read_file(config['bicycle_facilities_fp']/'network_improvements.gpkg',layer='savannah',ignore_geometry=True)\n",
    "# links = pd.merge(links,improvements,on='osm_linkid',how='left')\n",
    "# links.drop(columns=['linkid'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add GDOT data\n",
    "GDOT provides # of lanes data, AADT, and truck %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdot_lanes = gpd.read_file(config['network_fp']/\"conflation.gpkg\",layer=\"gdot_lanes\",ignore_geometry=True)\n",
    "gdot_traffic = gpd.read_file(config['network_fp']/\"conflation.gpkg\",layer=\"gdot_traffic\",ignore_geometry=True)\n",
    "\n",
    "links = pd.merge(links,gdot_lanes,on=\"osmid\",how='left')\n",
    "links = pd.merge(links,gdot_traffic,on='osmid',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle null aadt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this to explore na links\n",
    "# links[(links['link_type']=='road')&links['AADT'].isna()].explore()\n",
    "# give residential roads the lowest aadt category or below\n",
    "links.loc[links['AADT'].isna() &\n",
    "          (links['highway'].isin(['residential','service','unclassified','living_street'])) &\n",
    "          (links['link_type']=='road'),'AADT'] = '[0,4k)'\n",
    "# all others the middle category\n",
    "links.loc[links['AADT'].isna() & (links['link_type']=='road'),'AADT'] = '[4k,10k)'\n",
    "# any remaining nulls (bike paths, service roads, parking lots, get the lowest category)\n",
    "links.loc[links['AADT'].isna(),'AADT'] = '[0,4k)'\n",
    "\n",
    "#turn it into categorical data\n",
    "links['AADT'] = pd.Categorical(links['AADT'],ordered=True,categories=['[0,4k)','[4k,10k)','[10k,inf)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add HERE data (SKIP IF NO HERE DATA)\n",
    "HERE provides speed and lanes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "here = gpd.read_file(config['network_fp']/\"conflation.gpkg\",layer=\"here\",ignore_geometry=True)\n",
    "links = pd.merge(links,here,left_on='linkid',right_on='osm_linkid',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling null speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this to explore na links\n",
    "# links[(links['link_type']=='road')&links['here_speed'].isna()].explore()\n",
    "# links[links['here_speed'].isna()&(links['link_type']=='road')]['highway'].unique()\n",
    "# give residential roads a speed limit of 30 or below\n",
    "links.loc[links['here_speed'].isna() &\n",
    "          (links['highway'].isin(['residential','service','unclassified','living_street'])) &\n",
    "          (links['link_type']=='road'),'here_speed'] = '[0,30]'\n",
    "# all others get 30 +\n",
    "links.loc[links['here_speed'].isna() & (links['link_type']=='road'),'here_speed'] = '(30,40]'\n",
    "# any remaining nulls (bike paths, service roads, parking lots, get a speed limit of 30 or below)\n",
    "links.loc[links['here_speed'].isna(),'here_speed'] = '[0,30]'\n",
    "links.rename(columns={'here_speed':'speed'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links['speed'] = pd.Categorical(links['speed'],ordered=True,categories=['[0,30]', '(30,40]', '(40,inf)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve GDOT/HERE lanes data\n",
    "- All non-road links get a 1 (doing this so lanes attribute isn't being confounded with vehicle access)\n",
    "- By direction is too detailed, use a per direction estimate (i.e. treat a 5 lane oneway road the same as a 10 lane twoway road or a 5 lane per direction)\n",
    "- Simplify to:\n",
    "    - 1 lane per direction\n",
    "    - 2 lanes per direction\n",
    "    - 3+ lanes per direction\n",
    "- If unequal number of lanes use direction that would result in the higher category\n",
    "    - Example: 10th Street NE would be 2 lanes per direction because it has 2/1 lanes by direction\n",
    "- Turn lanes (middle, right, etc) are NOT counted in HERE or GDOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO figure out what to do when there is a disrepency between the lanes\n",
    "## Examine where there's a big mismatch between HERE and GDOT\n",
    "# - There are a couple of cases where GDOT will be way off, like North Highland Ave NE which shows up as having four lanes when it's mostly 2 lanes for most of its length.\n",
    "# - Memorial Drive is also marked as having four lanes but it was road dieted post 2016, so just use the old value.\n",
    "# - Unless it's a residential street or a few cases that were identified, use the GDOT values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give everything a default value of 1 (before we would give non-motorized links a value of 0)\n",
    "links['lanes'] = 1\n",
    "\n",
    "#if one is null take the non null value\n",
    "links.loc[links['here_lanes'].isna() & links['gdot_lanes'].notna(),'lanes'] = links['gdot_lanes']\n",
    "links.loc[links['here_lanes'].notna() & links['gdot_lanes'].isna(),'lanes'] = links['here_lanes']\n",
    "\n",
    "#otherwise choose whichever is smaller\n",
    "links.loc[links['here_lanes'].notna() & links['gdot_lanes'].notna(),'lanes'] = links[['here_lanes','gdot_lanes']].min(axis=1)\n",
    "\n",
    "#drop to trim down the df\n",
    "links.drop(columns=['gdot_lanes','here_lanes'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add elevation data\n",
    "Assign the correct direction for reverse links later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation = gpd.read_file(config['network_fp']/'elevation.gpkg',layer='elevation',ignore_geometry=True)\n",
    "elevation = elevation[['linkid','ascent_ft','descent_ft','ascent_grade_cat','descent_grade_cat']]\n",
    "links = pd.merge(links,elevation,on='linkid',how='left')\n",
    "# del elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set ascent grade and descent grade to zero\n",
    "links.loc[links['ascent_grade_cat'].isna(),'ascent_grade_cat'] = '[0,4)'\n",
    "links.loc[links['descent_grade_cat'].isna(),'descent_grade_cat'] = '[0,4)'\n",
    "links.loc[:,['ascent_ft','descent_ft']] = links.loc[:,['ascent_ft','descent_ft']].fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for remaining bridge where lidar data was not available set the grade to 0 if grade exceeds 10 percent\n",
    "# links.loc[(links['bridge'] == 'yes') & (links['ascent_grade_%'] > 10),'ascent_grade_%'] = 0\n",
    "# #also for tunnels\n",
    "# links.loc[(links['tunnel'] == 'yes'),'ascent_grade_%'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create reverse links and turn dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO change this to not create the turn graph (just make it an extra optional step)\n",
    "## Create turn graph dataframe\n",
    "from importlib import reload\n",
    "reload(modeling_turns)\n",
    "directed_links, turns_df = modeling_turns.create_pseudo_dual_graph(links,'A','B','linkid','oneway')\n",
    "\n",
    "# find the degree of the intersection node and re-classify anything with degree 2 as straight turn movement?\n",
    "# what about interstate exits that got removed?\n",
    "from collections import Counter\n",
    "node_degree = dict(Counter(links['A'].tolist()+links['B'].tolist()))\n",
    "turns_df['node_degree'] = turns_df['source_B'].map(node_degree)\n",
    "# turns_df[turns_df['node_degree']==2,'turn_type'] = 'straight'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add signals from OSM and GDOT to turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_signals = pd.read_parquet(config['network_fp']/'osm_signals.parquet')[['source_linkid','source_reverse_link','target_linkid','target_reverse_link']]\n",
    "gdot_signals = pd.read_parquet(config['network_fp']/'gdot_signals.parquet')[['source_linkid','source_reverse_link','target_linkid','target_reverse_link']]\n",
    "\n",
    "osm_signals = set([tuple(x) for x in osm_signals.values])\n",
    "gdot_signals = set([tuple(x) for x in gdot_signals.values])\n",
    "added_signals = set.union(osm_signals,gdot_signals)\n",
    "\n",
    "turns_df.set_index(['source_linkid','source_reverse_link','target_linkid','target_reverse_link'],inplace=True)\n",
    "\n",
    "added_signals = set.intersection(set(turns_df.index.tolist()),added_signals)\n",
    "turns_df.loc[added_signals,'signalized'] = True\n",
    "turns_df.loc[turns_df['signalized'].isna(),'signalized'] = False\n",
    "\n",
    "turns_df.reset_index(inplace=True)\n",
    "\n",
    "turns_df['signalized'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add in cross street variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach speed, lanes, AADT, and osm hihgway\n",
    "link_attrs = links.copy()[['linkid','highway','link_type','lanes','AADT','speed']]\n",
    "link_attrs.set_index('linkid',inplace=True)\n",
    "\n",
    "source_cols = ['source_' + x for x in link_attrs.columns]\n",
    "target_cols = ['target_' + x for x in link_attrs.columns]\n",
    "\n",
    "link_attrs.columns = source_cols\n",
    "turns_df = pd.merge(turns_df,link_attrs,left_on='source_linkid',right_index=True,how='left')\n",
    "link_attrs.columns = target_cols\n",
    "turns_df = pd.merge(turns_df,link_attrs,left_on='target_linkid',right_index=True,how='left')\n",
    "turns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross street would be to the left or right\n",
    "cond1 = turns_df['turn_type'].isin(['left','right'])\n",
    "\n",
    "#only road to road for now\n",
    "cond2 = (turns_df['source_link_type'] == 'road') & (turns_df['target_link_type'] == 'road')\n",
    "cross_streets = turns_df[cond1&cond2]\n",
    "\n",
    "# get the worst possible cross street attribute\n",
    "cross_streets = cross_streets.groupby(['source_linkid','source_reverse_link'])['target_AADT','target_lanes','target_speed'].max()\n",
    "cross_streets.columns = ['cross_AADT','cross_lanes','cross_speed']\n",
    "test = turns_df.merge(cross_streets,left_on=['source_linkid','source_reverse_link'],right_index=True)#,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a stressful turn would be\n",
    "aadt_cross_cond = test['cross_AADT'] == '[10k,inf)'\n",
    "lanes_cross_cond = test['cross_lanes'] > 2\n",
    "speed_cross_cond = test['cross_speed'] >= '(30,40]'\n",
    "cross_high_stress = aadt_cross_cond | lanes_cross_cond | speed_cross_cond\n",
    "\n",
    "# if the source street has these stats then assume that there is a signal\n",
    "aadt_source_cond = test['source_AADT'] == '[10k,inf)'\n",
    "lanes_source_cond = test['source_lanes'] > 2\n",
    "speed_source_cond = test['source_speed'] >= '(30,40]'\n",
    "source_high_stress = aadt_source_cond | lanes_source_cond | speed_source_cond\n",
    "\n",
    "test['unsig_crossing'] = False\n",
    "test.loc[(source_high_stress==False) & cross_high_stress & (test['signalized']==False) & (test['turn_type'].isin(['straight','left'])),'unsig_crossing'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional add geo data to turns and export for examination\n",
    "reload(modeling_turns)\n",
    "cross_streets_gdf = modeling_turns.turn_gdf(links,test)\n",
    "for idx,x in enumerate(cross_streets_gdf.dtypes):\n",
    "    if (str(x) == \"category\") | (str(x)=='object'):\n",
    "        cross_streets_gdf.iloc[:,idx] = cross_streets_gdf.iloc[:,idx].astype(str)\n",
    "cross_streets_gdf.to_file(config['network_fp']/'scratch.gpkg',layer='cross_streets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the worst possible cross street attribute\n",
    "# cross_streets.groupby(['source_linkid','source_reverse_link'])['target_AADT','target_lanes','target_speed'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_streets.loc[18242]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_streets.groupby(['source_linkid','source_reverse_link'])['target_speed'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# find the worst cross street if there are multiple\n",
    "cross_streets.groupby(['source_linkid','source_reverse_link'])['aadt'].apply(lambda x: aadt_order)\n",
    "\n",
    "\n",
    "# TODO do this for the other variables too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['AADT','lanes','speed']\n",
    "for x in cols:\n",
    "    print(links[x].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules for high stress turns\n",
    "\n",
    "#Major/minor road classification to create high traffic stress variable\n",
    "major_road_values = ['primary','secondary']\n",
    "major_road_values = major_road_values + [item + '_link' for item in major_road_values]\n",
    "minor_road_values = ['tertiary','unclassified','residential','service','trunk','living_street','service']\n",
    "minor_road_values = minor_road_values + [item + '_link' for item in minor_road_values]\n",
    "\n",
    "#traffic\n",
    "\n",
    "#override major road if only one lane per direction\n",
    "major_road = set(links.loc[links['highway'].isin(major_road_values) & (links['lanes'] >= 2),'linkid'].tolist())\n",
    "minor_road = set(links.loc[links['highway'].isin(minor_road_values) | \\\n",
    "                           (links['highway'].isin(major_road_values) & (links['lanes'] < 2) ), \n",
    "                            'linkid'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_streets.loc[cross_streets['source_linkid']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(links[links['linkid']==3].squeeze())\n",
    "links[links['linkid']==3].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# grouby the source link\n",
    "cross_streets.groupby(['source_linkid'])['target_highway'].agg(list)#['target_highway_order'].min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cross_streets.name = 'cross_street'\n",
    "\n",
    "#add to main df\n",
    "pd.merge(turns_df,cross_streets,left_on=['source_linkid','source_A','source_B'],right_index=True,how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wasn't able to get major/minor to be significant\n",
    "#Major/minor road classification to create high traffic stress variable\n",
    "major_road_values = ['primary','secondary']\n",
    "major_road_values = major_road_values + [item + '_link' for item in major_road_values]\n",
    "minor_road_values = ['tertiary','unclassified','residential','service','trunk','living_street','service']\n",
    "minor_road_values = minor_road_values + [item + '_link' for item in minor_road_values]\n",
    "\n",
    "#override major road if only one lane per direction\n",
    "major_road = set(links.loc[links['highway'].isin(major_road_values) & (links['lanes'] >= 2),'linkid'].tolist())\n",
    "minor_road = set(links.loc[links['highway'].isin(minor_road_values) | links['lanes'] < 2,'linkid'].tolist())\n",
    "\n",
    "#unsignalized straight/left turn where crossing street is a major road\n",
    "turns_df['unsig_major_road_crossing'] = (turns_df['signalized']==False) & \\\n",
    "    turns_df['target_linkid'].isin(major_road) & \\\n",
    "    turns_df['source_linkid'].isin(minor_road) & \\\n",
    "    turns_df['turn_type'].isin(['left','straight'])\n",
    "\n",
    "# #sets turns that are not from road to road to None, effectively ignoring them\n",
    "# exclude = ['road','service']\n",
    "# turns_df.loc[(turns_df['source_link_type'].isin(exclude)==False) & \n",
    "#              (turns_df['target_link_type'].isin(exclude)==False),'turn_type'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create layer of unsignalized crossings for examining\n",
    "unsig_major_road_crossing = set(turns_df.loc[turns_df['unsig_major_road_crossing']==True,'source_B'].tolist())\n",
    "nodes = gpd.read_file(config['network_fp']/'final_network.gpkg',layer='nodes')\n",
    "nodes = nodes[nodes['N'].isin(unsig_major_road_crossing)]\n",
    "nodes.to_file(config['calibration_fp']/'unsig_major_road_crossing.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links[links['AADT']=='nan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Interstates and Private Links\n",
    "Remove these because we're absolutely sure we don't want bikes on these links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(links['link_type'].unique())\n",
    "remove = ['no_access_or_private','restricted_access_road','no_bike']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cond = links['link_type'].isin(remove)\n",
    "links = links[remove_cond==False]\n",
    "print(remove_cond.sum(),'links removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove isolated links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links, nodes = prepare_network.largest_comp_and_simplify(links,nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = ['A', 'B', 'linkid', 'oneway', 'link_type', 'osmid', 'geometry',\n",
    "       'length_ft', 'highway', 'name','all_tags',\n",
    "       'facility_fwd', 'facility_rev', 'facility', 'year', 'sidepath',\n",
    "       'sidepath_year', 'route_type', 'AADT',\n",
    "       'speed', 'lanes', 'ascent_ft', 'descent_ft', 'ascent_grade_cat',\n",
    "       'descent_grade_cat']\n",
    "links[final_cols].to_file(config['network_fp']/'final_network.gpkg',layer='edges')\n",
    "nodes.to_file(config['network_fp']/'final_network.gpkg',layer='nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add directional attributes and flip as needed\n",
    "ascent_columns = ['ascent_ft', 'ascent_grade_cat']\n",
    "descent_columns = ['descent_ft', 'descent_grade_cat']\n",
    "directed_links = pd.merge(directed_links,links[['linkid','facility_fwd','facility_rev']+ascent_columns+descent_columns],on='linkid')\n",
    "directed_links.loc[directed_links['reverse_link']==True,ascent_columns+descent_columns] = directed_links.loc[directed_links['reverse_link']==True,descent_columns+ascent_columns].values\n",
    "directed_links.loc[directed_links['reverse_link']==True,['facility_fwd','facility_rev']] = directed_links.loc[directed_links['reverse_link']==True,['facility_rev','facility_fwd']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple columns not compatible with parquet\n",
    "turns_df.drop(columns=['source','target'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO pickles later\n",
    "turns_df.to_parquet(config['network_fp']/'turns_df.parquet')\n",
    "directed_links.to_parquet(config['network_fp']/'directed_edges.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional add geo data to turns and export for examination\n",
    "from shapely.ops import MultiLineString\n",
    "geo_dict = dict(zip(links['linkid'],links['geometry']))\n",
    "turns_df['source_geo'] = turns_df['source_linkid'].map(geo_dict)\n",
    "turns_df['target_geo'] = turns_df['target_linkid'].map(geo_dict)\n",
    "turns_df['geometry'] = turns_df.apply(lambda row: MultiLineString([row['source_geo'],row['target_geo']]),axis=1)\n",
    "turns_df.drop(columns=['source_geo','target_geo'],inplace=True)\n",
    "turns_gdf = gpd.GeoDataFrame(turns_df,crs=links.crs)\n",
    "# turns_gdf.drop(columns=['source','target'])\n",
    "turns_gdf.to_file(config['network_fp']/'final_network.gpkg',layer='turns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO serialize the attributes to add as needed?\n",
    "# with (config['network_fp'] / 'edges_with_attributes.pkl').open('wb') as fh:\n",
    "#     pickle.dump(links,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with (config['network_fp'] / 'edges.pkl').open('wb') as fh:\n",
    "#     pickle.dump(links,fh,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with (config['network_fp'] / 'nodes.pkl').open('wb') as fh:\n",
    "#     pickle.dump(nodes,fh,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with (config['network_fp'] / 'directed_edges.pkl').open('wb') as fh:\n",
    "#     pickle.dump(edges,fh,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with (config['network_fp'] / 'turn_df.pkl').open('wb') as fh:\n",
    "#     pickle.dump(turn_df,fh,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated past here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #add attributes back and then flip elevation/bicyccle attributes\n",
    "# #do so i don't have to re-flip everytime i import? could potentially save memory though\n",
    "# #TODO it would still be smarter to store as a dict or something\n",
    "# edges\n",
    "# links.columns\n",
    "# edges = pd.merge(edges,links.drop(columns=['A','B']),on='linkid')\n",
    "# #if reverse_geo == true then ascent should be descent and vice versa\n",
    "# # loops have reverse_geometry is np.nana\n",
    "# # assume that all elevation columns will be paired by what is after ascent/descent\n",
    "# elevation_columns = ['ascent_m', 'descent_m', 'ascent_grade_%','descent_grade_%']\n",
    "# # Remove elements containing \"ascent\" or \"descent\"\n",
    "# cleaned_columns = [col for col in elevation_columns if \"ascent\" not in col and \"descent\" not in col]    \n",
    "# # Remove duplicates by converting the list to a set and back to a list\n",
    "# cleaned_columns = list(set(cleaned_columns))\n",
    "\n",
    "# for cleaned_column in cleaned_columns:\n",
    "#     #swap if reverse geometry == true\n",
    "#     links.loc[links['reverse_geometry']==True,ascent_columns+descent_columns] = links.loc[links['reverse_geometry']==True,descent_columns+ascent_columns]\n",
    "    \n",
    "    \n",
    "#     df_edges[] = np.where(df_edges['reverse_link'], df_edges[elev_columns[1]].abs(), df_edges[elev_columns[0]])\n",
    "#     #drop the down version?\n",
    "#     df_edges.drop(columns=elev_columns[1],inplace=True)\n",
    "# #if reverse_geo == true then ascent should be descent and vice versa\n",
    "# # loops have reverse_geometry is np.nan\n",
    "# # assume that all elevation columns will be paired by what is after ascent/descent\n",
    "# elevation_columns = ['ascent_m', 'descent_m', 'ascent_grade_%','descent_grade_%']\n",
    "# # Remove elements containing \"ascent\" or \"descent\"\n",
    "# cleaned_columns = [col for col in elevation_columns if \"ascent\" not in col and \"descent\" not in col]    \n",
    "# # Remove duplicates by converting the list to a set and back to a list\n",
    "# cleaned_columns = list(set(cleaned_columns))\n",
    "\n",
    "# for cleaned_column in cleaned_columns:\n",
    "#     #swap\n",
    "    \n",
    "    \n",
    "    \n",
    "#     df_edges[] = np.where(df_edges['reverse_link'], df_edges[elev_columns[1]].abs(), df_edges[elev_columns[0]])\n",
    "#     #drop the down version?\n",
    "#     df_edges.drop(columns=elev_columns[1],inplace=True)\n",
    "# ## Rename columns\n",
    "\n",
    "# links.rename(columns={'osm_A':'A','osm_B':'B','osm_linkid':'linkid'},inplace=True)\n",
    "# nodes.rename(columns={'osm_N':'N'},inplace=True)\n",
    "# ## \n",
    "# ## Create turn graph dataframe\n",
    "# edges, turn_df = modeling_turns.create_pseudo_dual_graph(links,'A','B','linkid','oneway')\n",
    "# ## Flip attributes if needed (elevation, bicycle facilities)\n",
    "# Turns should be good as is\n",
    "# #add geo (needed for map matching part)\n",
    "# df_edges = df_edges.merge(links.drop(columns=['A','B']),on=['linkid'])\n",
    "# df_edges = gpd.GeoDataFrame(df_edges,geometry='geometry',crs=links.crs)\n",
    "# df_edges = df_edges.loc[:,~df_edges.columns.duplicated()].copy()\n",
    "# df_edges.reset_index(drop=True,inplace=True)\n",
    "# #just export the df_edges?\n",
    "# df_edges.to_file(export_fp/'Map_Matching/matching.gpkg',layer='edges')\n",
    "# nodes.to_file(export_fp/'Map_Matching/matching.gpkg',layer='nodes')\n",
    "# pseudo_df.columns\n",
    "# #add geo to the turns too\n",
    "# from shapely.ops import MultiLineString\n",
    "# pseudo_df = pseudo_df.merge(links[['linkid','geometry']],left_on='source_linkid',right_on='linkid')\n",
    "# pseudo_df = pseudo_df.merge(links[['linkid','geometry']],left_on='target_linkid',right_on='linkid')\n",
    "\n",
    "# geometry = pseudo_df.apply(lambda row: MultiLineString([row['geometry_x'],row['geometry_y']]),axis=1)\n",
    "# pseudo_df.drop(columns=['geometry_x','geometry_y','linkid_x','linkid_y'],inplace=True)\n",
    "# pseudo_df = gpd.GeoDataFrame(pseudo_df,geometry=geometry,crs=links.crs)\n",
    "\n",
    "# # pseudo_edges = pseudo_edges.loc[:,~pseudo_edges.columns.duplicated()].copy()\n",
    "# # pseudo_edges.reset_index(drop=True,inplace=True)\n",
    "# pseudo_df['source'] = pseudo_df['source'].astype(str)\n",
    "# pseudo_df['target'] = pseudo_df['target'].astype(str)\n",
    "# pseudo_df.to_file(export_fp/'Map_Matching/matching.gpkg',layer='turns')\n",
    "# #pickle the graph\n",
    "# with (export_fp / 'Map_Matching/turn_G.pkl').open('wb') as fh:\n",
    "#     pickle.dump(pseudo_G,fh)\n",
    "# # Come back to below later\n",
    "# # Network Prepare\n",
    "# This notebook prepares the final routing network.\n",
    "\n",
    "# 1. Import the desired routing network\n",
    "# 1. Add attributes\n",
    "# 1. Add reconciled attributes\n",
    "# 1. Add signals\n",
    "# 1. Add elevation\n",
    "\n",
    "# Then the network will be turned into a directed network graph complete with an edge list representing the directed edges and another one representing turns. Some attribute values are reversed to account for direction (e.g., elevation, signals).\n",
    "# Import the data from previous notebooks and merge them. Merge here so updates can be done at each step without having to repeat everything.\n",
    "# network_filepath = Path.home() / \"Documents/BikewaySimData/Projects/gdot/networks\"\n",
    "# #filtered data\n",
    "# links = gpd.read_file(network_filepath/'filtered.gpkg',layer='osm_links')\n",
    "# nodes = gpd.read_file(network_filepath/'filtered.gpkg',layer='osm_nodes')\n",
    "# links.columns\n",
    "# #add osm data\n",
    "# links = add_attributes.add_osm_attr(links,network_filepath / 'osm_attr.pkl')\n",
    "# #rename\n",
    "# links.rename(columns={'osm_A':'A','osm_B':'B','osm_linkid':'linkid'},inplace=True)\n",
    "# nodes.rename(columns={'osm_N':'N'},inplace=True)\n",
    "# links.columns\n",
    "# #reconciled data\n",
    "# reconciled = gpd.read_file(network_filepath/'reconciled.gpkg',layer='links',ignore_geometry=True)\n",
    "# #[col for col in reconciled.columns if col not in links.columns]\n",
    "# cols_to_keep = ['osm_linkid','speedlimit_range_mph','lanes_per_direction']\n",
    "# links = links.merge(reconciled[cols_to_keep],on='osm_linkid',how='left')\n",
    "# del reconciled\n",
    "# #rename\n",
    "# links.rename(columns={'osm_A':'A','osm_B':'B','osm_linkid':'linkid'},inplace=True)\n",
    "# nodes.rename(columns={'osm_N':'N'},inplace=True)\n",
    "# #signals added\n",
    "# links_w_signals = gpd.read_file(network_filepath/'signals_added.gpkg',layer='links',ignore_geometry=True)\n",
    "\n",
    "# nodes_w_signals = gpd.read_file(network_filepath/'signals_added.gpkg',layer='nodes',ignore_geometry=True)\n",
    "# nodes_w_signals\n",
    "# #TODO change linkid to osm_linkid later\n",
    "# cols_to_keep = ['linkid','signal_A','signal_B']\n",
    "# links = links.merge(links_w_signals[cols_to_keep],on='linkid',how='left')\n",
    "# ##del nodes_w_signals\n",
    "\n",
    "# #elevation added\n",
    "# links_w_elevation = gpd.read_file(network_filepath/'elevation_added.gpkg',ignore_geometry=True)\n",
    "# links_w_elevation.columns\n",
    "# links_w_elevation.rename(columns={\n",
    "#     'a_s_c_e_n_t___m':'ascent_m',\n",
    "#     'd_e_s_c_e_n_t___m':'descent_m',\n",
    "#     'a_s_c_e_n_t___g_r_a_d_e':'ascent_grade',\n",
    "#     'd_e_s_c_e_n_t___g_r_a_d_e':'descent_grade',\n",
    "# }, inplace =True)\n",
    "# cols_to_keep = ['linkid','ascent_m','descent_m','ascent_grade','descent_grade','(0,2]_descent',\n",
    "#        '(2,4]_descent', '(4,6]_descent', '(6,10]_descent', '(10,15]_descent',\n",
    "#        '(15,inf]_descent', '(0,2]_ascent', '(2,4]_ascent', '(4,6]_ascent',\n",
    "#        '(6,10]_ascent', '(10,15]_ascent', '(15,inf]_ascent']\n",
    "# links = links.merge(links_w_elevation[cols_to_keep],on='linkid')\n",
    "# del links_w_elevation\n",
    "# links.columns\n",
    "# fp = Path.home() / \"Documents/BikewaySimData/Projects/gdot\"\n",
    "# edges = gpd.read_file(fp/'networks/elevation_added.gpkg',layer=\"links\")\n",
    "\n",
    "\n",
    "# edges.columns\n",
    "# #use geometry one last time\n",
    "# edges['length_ft'] = edges.length\n",
    "\n",
    "# #turn bridge and tunnel to boolean values\n",
    "# edges['tunnel'] = edges['tunnel'].notna()\n",
    "# edges['bridge'] = edges['bridge'].notna()\n",
    "# #turn bike facil into one column\n",
    "# edges['bike_facility_type'] = np.nan\n",
    "# edges.loc[(edges['mu'] == 1) & (edges['bike_facility_type'].isna()),'bike_facility_type'] = 'shared-use path'\n",
    "# edges.loc[(edges['pbl'] == 1) & (edges['bike_facility_type'].isna()),'bike_facility_type'] = 'protected bike lane'\n",
    "# edges.loc[(edges['bl'] == 1) & (edges['bike_facility_type'].isna()),'bike_facility_type'] = 'bike lane'\n",
    "# df_edges, pseudo_df, pseudo_G = modeling_turns.create_pseudo_dual_graph(edges,'A','B','linkid','oneway',True)\n",
    "# ## Add desired attributes from links to df_edges\n",
    "# #df_edges = df_edges.merge(edges[['linkid','geometry']])\n",
    "\n",
    "# basic_cols = ['linkid', 'osmid', 'link_type', 'name', 'oneway','length_ft']\n",
    "\n",
    "# #anything that's an instance or would be better as a count value (but not a turn)\n",
    "# event_cols = ['bridge','tunnel']\n",
    "\n",
    "# #anything that's for the duration of the entire link and has categories\n",
    "# category_cols = ['link_type','highway','speedlimit_range_mph',\n",
    "#                'lanes_per_direction','bike_facility_type']\n",
    "\n",
    "# #reverse in tuple form (these need to be flipped if going the other direction)\n",
    "# rev_columns = [('ascent_m','descent_m'),\n",
    "#                ('ascent_grade','descent_grade'),\n",
    "#                ('(0,2]_ascent','(0,2]_descent'),\n",
    "#                ('(2,4]_ascent','(2,4]_descent'),\n",
    "#                ('(4,6]_ascent','(4,6]_descent'),\n",
    "#                ('(6,10]_ascent','(6,10]_descent'),\n",
    "#                ('(10,15]_ascent','(10,15]_descent'),\n",
    "#                ('(15,inf]_ascent','(15,inf]_descent')]\n",
    "\n",
    "# from itertools import chain\n",
    "# keep_cols = basic_cols + event_cols + category_cols + list(chain(*rev_columns))\n",
    "# # attrs = ['linkid', 'osmid', 'link_type', 'name', 'highway',\n",
    "# #        'bridge', 'tunnel', 'bl', 'pbl', 'mu','speedlimit_range_mph',\n",
    "# #        'lanes_per_direction', 'up_grade', 'down_grade', 'length_ft',\n",
    "# #        'vehicle_separation','geometry']\n",
    "# df_edges = df_edges.merge(edges[keep_cols],on='linkid',how='left')\n",
    "# df_edges\n",
    "# ## Deal with grade\n",
    "# Need to flip sign of grade for reverse links\n",
    "# # def combine_up_down_tuples(lst):\n",
    "# #     result = []\n",
    "# #     current_tuple = []\n",
    "\n",
    "# #     for item in lst:\n",
    "# #         if 'ascent' in item or 'descent' in item:\n",
    "# #             current_tuple.append(item)\n",
    "# #             if len(current_tuple) == 2:\n",
    "# #                 result.append(tuple(current_tuple))\n",
    "# #                 current_tuple = []\n",
    "\n",
    "# #     return result\n",
    "\n",
    "# # rev_columns = ['ascent_m','descent_m','ascent_grade','descent_grade',\n",
    "# #                '(0,2]_down', '(2,4]_down', '(4,6]_down',\n",
    "# #                '(6,10]_down', '(10,15]_down','(15,inf]_down',\n",
    "# #                '(0,2]_up', '(2,4]_up', '(4,6]_up', '(6,10]_up',\n",
    "# #                '(10,15]_up', '(15,inf]_up'\n",
    "# #                ]\n",
    "\n",
    "# # combined_tuples = combine_up_down_tuples(rev_columns)\n",
    "\n",
    "# for elev_columns in rev_columns:\n",
    "#     df_edges[elev_columns[0]] = np.where(df_edges['reverse_link'], df_edges[elev_columns[1]].abs(), df_edges[elev_columns[0]])\n",
    "#     #drop the down version?\n",
    "#     df_edges.drop(columns=elev_columns[1],inplace=True)\n",
    "# ## Turns and Signals\n",
    "# #add additional attributes needed for processing\n",
    "# source_links = edges[['linkid','osmid','link_type','name','highway']]\n",
    "# target_links = edges[['linkid','osmid','link_type','name','highway']]\n",
    "# source_links.columns = 'source_' + source_links.columns\n",
    "# target_links.columns = 'target_' + target_links.columns\n",
    "# pseudo_df = pseudo_df.merge(source_links,on='source_linkid',how='left')\n",
    "# pseudo_df = pseudo_df.merge(target_links,on='target_linkid',how='left')\n",
    "# ## Turn Restrictions\n",
    "# Two types in OSM (represented as OSM relations):\n",
    "# - No (blank) turns\n",
    "# - Only this turn allowed\n",
    "\n",
    "# For chosen we don't need to consider turn restrictions\n",
    "# # turn_restrictions = pd.read_csv(fp.parent/'osm_turn_restrictions.csv')\n",
    "# # pseudo_df = pseudo_df.merge(turn_restrictions,left_on=['source_osmid','target_osmid'],right_on=['from_way_id','to_way_id'],how='left')\n",
    "# # road_cond = (pseudo_df['source_link_type'] == 'road') & (pseudo_df['target_link_type'] == 'road')\n",
    "# # no_restr = pseudo_df['type'] == 'no'\n",
    "# # only_restr = pseudo_df['type'] == 'only'\n",
    "\n",
    "# # #add a remove column\n",
    "# # pseudo_df['remove'] = False\n",
    "\n",
    "# # #remove the no turns\n",
    "# # pseudo_df.loc[road_cond & no_restr,'remove'] = True\n",
    "\n",
    "# # #for only, find all instances road_cond + from source and set to True\n",
    "# # sources = set(turn_restrictions.loc[turn_restrictions['type']=='only','from_way_id'].tolist())\n",
    "# # pseudo_df.loc[road_cond & pseudo_df['source_osmid'].isin(sources) & pseudo_df['type'].isna(),'remove'] = True\n",
    "\n",
    "# # #Remove these turns and drop the added columns\n",
    "# # print((pseudo_df['remove']==True).sum(),'turns removed')\n",
    "# # pseudo_df = pseudo_df[pseudo_df['remove']==False]\n",
    "# # pseudo_df.drop(columns=['relation_id', 'restriction', 'from_way_id',\n",
    "# #        'to_way_id', 'type', 'remove'],inplace=True)\n",
    "# # Deal with signals\n",
    "# Perform two merges and use the source/target reverse link columns to determine which signal ID to keep.\n",
    "# - For the source link, use signal_B if reverse == False else signal_A\n",
    "# - For the target link, use signal_A if reverse == False else signal_B\n",
    "# source = pseudo_df[['source_linkid','source_reverse_link']].merge(edges,left_on='source_linkid',right_on='linkid',how='left')\n",
    "# pseudo_df['source_signal'] = np.where(source['source_reverse_link'], source['signal_A'], source['signal_B'])\n",
    "\n",
    "# target = pseudo_df[['target_linkid','target_reverse_link']].merge(edges,left_on='target_linkid',right_on='linkid',how='left')\n",
    "# pseudo_df['target_signal'] = np.where(target['target_reverse_link']==False, target['signal_B'], target['signal_A'])\n",
    "# ## Identifying signalized/unsignalized turns\n",
    "# - Only look at roads for now\n",
    "# - Filter to left/right turns per source linkid per direction\n",
    "# - Take the highest road classification and assign it as the cross street road classification\n",
    "# import pandas as pd\n",
    "# highway_order = {\n",
    "#     'trunk': 0,\n",
    "#     'trunk_link': 1,\n",
    "#     'primary': 2,\n",
    "#     'primary_link': 3,\n",
    "#     'secondary': 4,\n",
    "#     'secondary_link': 5,\n",
    "#     'tertiary': 6,\n",
    "#     'tertiary_link': 7,\n",
    "#     'unclassified': 8,\n",
    "#     'residential': 9\n",
    "# }\n",
    "# highway_order = pd.Series(highway_order)\n",
    "# highway_order = highway_order.reset_index()\n",
    "# highway_order.columns = ['highway','order']\n",
    "# #add highway ranking based on the above\n",
    "# pseudo_df['target_highway_order'] = pseudo_df['target_highway'].map(highway_order.set_index('highway')['order'])\n",
    "# pseudo_df['source_highway_order'] = pseudo_df['source_highway'].map(highway_order.set_index('highway')['order'])\n",
    "# #remove straight and uturn\n",
    "# cond1 = pseudo_df['turn_type'].isin(['left','right'])\n",
    "# #only road to road for now\n",
    "# cond2 = (pseudo_df['source_link_type'] == 'road') & (pseudo_df['target_link_type'] == 'road')\n",
    "# cross_streets = pseudo_df[cond1 & cond2]\n",
    "\n",
    "# #use groupby to find the max target_highway order\n",
    "# cross_streets = cross_streets.groupby(['source_linkid','source_A','source_B'])['target_highway_order'].min()\n",
    "# cross_streets.name = 'cross_street'\n",
    "\n",
    "# #add to main df\n",
    "# pseudo_df = pd.merge(pseudo_df,cross_streets,left_on=['source_linkid','source_A','source_B'],right_index=True,how='left')\n",
    "\n",
    "# #change numbers back to normal\n",
    "# pseudo_df['cross_street_order'] = pseudo_df['cross_street']\n",
    "# pseudo_df['cross_street'] = pseudo_df['cross_street'].map(highway_order.set_index('order')['highway'])\n",
    "# # TODO Add OSM crossing into this logic\n",
    "#     - Signals\n",
    "#         - Wait on this until we have the route attributes code done\n",
    "#         - Add crossings in signalization\n",
    "#         - Majority of crossings are nodes not ways\n",
    "#         - Cycleway crossings typically dealt the same way\n",
    "#         - If meeting nodes are both crossings and within the traffic signal buffer, they're signalized crossings\n",
    "#             - Or if both connecting links are crossings/connect to the road etc\n",
    "#         - Way attributes\n",
    "#             - Footway = crossing\n",
    "#             - Highway = footway\n",
    "#         - Node attributes\n",
    "#             - Crossing = * (traffic signals/marked/etc)\n",
    "#             - Highway = crossing\n",
    "#         - Link attributes\n",
    "#             - Some links are labeled as crossings but this is not as consistent\n",
    "\n",
    "# signalized = pseudo_df['source_signal'] == pseudo_df['target_signal']\n",
    "# left_or_straight =  pseudo_df['turn_type'].isin(['left','straight'])\n",
    "# both_road = (pseudo_df['source_link_type'] == 'road') & (pseudo_df['target_link_type'] == 'road')\n",
    "# cross_street = pseudo_df['cross_street_order'] <= 5\n",
    "\n",
    "# #signalized\n",
    "# pseudo_df.loc[signalized & both_road,'signalized'] = True\n",
    "# pseudo_df.loc[pseudo_df['signalized'].isna(),'signalized'] = False\n",
    "# # pseudo_df.loc[signalized & left_or_straight & both_road,'signalized_left_straight'] = True\n",
    "# # pseudo_df.loc[pseudo_df['signalized_left_straight'].isna(),'signalized_left_straight'] = False\n",
    "\n",
    "# pseudo_df.loc[-signalized & both_road & cross_street,'unsignalized'] = True\n",
    "# pseudo_df.loc[pseudo_df['unsignalized'].isna(),'unsignalized'] = False\n",
    "\n",
    "# #clean up\n",
    "# rem =  ['source_osmid', 'source_link_type', 'source_name',\n",
    "#        'source_highway', 'target_osmid', 'target_link_type', 'target_name',\n",
    "#        'target_highway', 'source_signal', 'target_signal',\n",
    "#        'target_highway_order', 'source_highway_order', 'cross_street',\n",
    "#        'cross_street_order']\n",
    "# pseudo_df.drop(columns=rem,inplace=True)\n",
    "# # Export for impedance calibration\n",
    "\n",
    "# # df_edges = gpd.GeoDataFrame(df_edges,crs='epsg:2240')\n",
    "# df_edges.columns\n",
    "# with (fp.parent / 'chosen.pkl').open('wb') as fh:\n",
    "#     export = (df_edges,pseudo_df,pseudo_G)\n",
    "#     pickle.dump(export,fh)\n",
    "# ## Add geometry to examine results in QGIS\n",
    "# #add geo\n",
    "# link_geo = dict(zip(links['linkid'],links['geometry']))\n",
    "# pseudo_df['src_geo'] = pseudo_df['source_linkid'].map(link_geo)\n",
    "# pseudo_df['trgt_geo'] = pseudo_df['target_linkid'].map(link_geo)\n",
    "# pseudo_df['geometry'] = pseudo_df[['src_geo','trgt_geo']].apply(lambda row: MultiLineString([row['src_geo'],row['trgt_geo']]),axis=1)\n",
    "\n",
    "# pseudo_df.drop(columns=['src_geo','trgt_geo'],inplace=True)\n",
    "# pseudo_df = gpd.GeoDataFrame(pseudo_df,crs=links.crs)\n",
    "\n",
    "# pseudo_df['source'] = pseudo_df['source'].astype(str)\n",
    "# pseudo_df['target'] = pseudo_df['target'].astype(str)\n",
    "\n",
    "# #check results (may need a smaller road network to test on)\n",
    "# pseudo_df.to_file(Path.home()/'Downloads/testing.gpkg',layer='cross_streets')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
