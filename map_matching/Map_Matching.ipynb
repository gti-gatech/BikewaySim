{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO (in order of priority):\n",
    "1. Make sure that the cleanup code works (come back to this)\n",
    "    1. Want to get rid of the really small out and backs\n",
    "    1. Have method for doing this, but worried that it will remove valid out and backing\n",
    "    1. Frechet distance could be a way to tell if the cleaning removes too much out and backing? Or some sort of overlap metric?\n",
    "1. Use geo_dict to assemble edge for calculated frechet distance\n",
    "    1. Frechet distance will give a decent measure of how close the match is to the trace\n",
    "    1. Will need to reverse link geometry if passing the other way otherwise it won't work\n",
    "1. Match all the traces and export for impedance calibration\n",
    "    1. The process flow into this step is still uncertain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from leuvenmapmatching.matcher.distance import DistanceMatcher\n",
    "from leuvenmapmatching.map.inmem import InMemMap\n",
    "#from leuvenmapmatching import visualization as mmviz\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from shapely.ops import Point, LineString\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "from bikewaysim.paths import config\n",
    "from bikewaysim import map_match\n",
    "from bikewaysim.network import prepare_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Network\n",
    "Bring in the pre-processed network and filter as needed to remove links that are unlikely to be used for routing.\n",
    "\n",
    "### Break Apart Multi-Edges\n",
    "Occasionally, there will be multiple edges between two nodes. A common place for this to occur are local roads connected to main roads that form u-shapes. Leuven map mapmatching cannot handle multi-edges as the only information stored in the network graph are the nodes. Usually, it's obvious which edge should be retained, but an easy way to still include both edges for map matching is to break these edges by thier centroid into smaller edges with new nodes. This is done by ``explode_network_midpoint`` in the ``prepare_mapmatch_network`` function. The centroid nodes created in this step are given a unique id. The map matching function removes these new nodes so that they don't appear in the results.\n",
    "\n",
    "### Iterative Matching Approach\n",
    "Sometimes matching can be improved just by limiting what can be matched to. For this, we want to start by mapping to the most restrictive network. Then we add more links if a match hasn't been found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = gpd.read_file(config['network_fp']/'final_network.gpkg',layer='edges')\n",
    "nodes = gpd.read_file(config['network_fp']/'final_network.gpkg',layer='nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide which link types to allow for routing\n",
    "Suggest starting with the most restrictive network. Just bike, pedestrian, and road links. These do not need to match up to the calibration network. Anything included in the matching network that actually gets routed on will be included in calibration network automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove infra before 2016 so it doesn't match to these\n",
    "after = links['facility'].isin(['cycletrack','multi use path']) & \\\n",
    "          (links['link_type']!='road') & \\\n",
    "          links['year'].notna() & \\\n",
    "          (links['year']>2016)\n",
    "# links[after].drop(columns=['all_tags']).explore()\n",
    "links = links[after==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only allow roads + pedestrian + bike\n",
    "link_types_allowed = ['bike','pedestrian','road','service','connector']\n",
    "print(links['link_type'].unique())\n",
    "\n",
    "#TODO add the ability to go the wrongway on residential streets ONLY\n",
    "allow_wrongway_on = ['residential','living_street']\n",
    "links.loc[links['highway'].isin(allow_wrongway_on),'oneway'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_links, exploded_nodes, map_con = map_match.prepare_mapmatch_network(links,nodes,link_types_allowed,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIONAL inspect exploded network\n",
    "# exploded_nodes.to_file(config['network_fp']/'matching_network.gpkg',layer='exploded_nodes')\n",
    "# exploded_links.to_file(config['network_fp']/'matching_network.gpkg',layer='exploded_links')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace Data\n",
    "For map matching, we're using GPS traces that have been processed so that each point is spaced a certain distance apart, coordinates in between this distance are dropped to improve computation time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all traces\n",
    "with (config['cycleatl_fp']/'reduced_spacing.pkl').open('rb') as fh:\n",
    "    coords_dict = pickle.load(fh)\n",
    "\n",
    "# import trips that we want to match\n",
    "with (config['cycleatl_fp']/'trips_4.pkl').open('rb') as fh:\n",
    "    trips_df = pickle.load(fh)\n",
    "\n",
    "# subset the coords dict by just the trips we're trying to match\n",
    "coords_dict = {key:item for key, item in coords_dict.items() if key in trips_df['tripid'].tolist()}\n",
    "# drop speed_mph below zero if that hasn't been done\n",
    "coords_dict = {key:item[item['speed_mph']>1] for key, item in coords_dict.items() if item[item['speed_mph']>1].shape[0] > 0}\n",
    "\n",
    "print('Map matching',len(coords_dict.keys()),'trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the privacy filter \n",
    "def privacy_distance(df,privacy_dist=500):\n",
    "    first_point = df['geometry'].iloc[0].buffer(privacy_dist)\n",
    "    last_point = df['geometry'].iloc[-1].buffer(privacy_dist)\n",
    "    double_buffer = df['geometry'].iloc[0].buffer(privacy_dist*2)\n",
    "    if df['geometry'].intersects(double_buffer).all():\n",
    "        return\n",
    "    else:\n",
    "        first_cut = df['geometry'].intersects(first_point).idxmin() # find the first point where it's false\n",
    "        last_cut = df['geometry'].intersects(last_point).idxmax() - 1\n",
    "        if df.loc[first_cut:last_cut,:].shape[0] == 0:\n",
    "            return\n",
    "        else:\n",
    "            return df.loc[first_cut:last_cut,:]\n",
    "coords_dict = {key:privacy_distance(item) for key, item in coords_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Match Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_matching_settings = {\n",
    "    'obs_noise': 50, #Standard deviation of noise\n",
    "    'obs_noise_ne': 100, #Standard deviation of noise for non-emitting states (is set to obs_noise if not given)\n",
    "    'max_dist_init': 2000, #Maximum distance from start location (if not given, uses max_dist)\n",
    "    'max_dist': 1500, #Maximum distance from path (this is a hard cut, min_prob_norm should be better)\n",
    "    'min_prob_norm': 0.001, #Minimum normalized probability of observations (ema)\n",
    "    'non_emitting_states': True, #Allow non-emitting states\n",
    "    'non_emitting_length_factor': 0.001, #Reduce the probability of a sequence of non-emitting states the longer it is.\n",
    "    'max_lattice_width': 20, #Restrict the lattice (or possible candidate states per observation) to this value.\n",
    "    'dist_noise': 50, #Standard deviation of difference between distance between states and distance between observations.\n",
    "    'dist_noise_ne': 200, #for no emitting If not given, set to dist_noise\n",
    "    'restrained_ne': False, #Avoid non-emitting states if the distance between states and between observations is close to each other.\n",
    "    'avoid_goingback': False, #If true, the probability is lowered for a transition that returns back to a previous edges or returns to a position on an edge.\n",
    "    'increase_max_lattice_width': False,\n",
    "    'export_graph': False\n",
    "}\n",
    "from importlib import reload\n",
    "reload(map_match)\n",
    "import random\n",
    "random_trip = random.choice(list(coords_dict.keys()))\n",
    "# random_trip = 3079\n",
    "trace = coords_dict[random_trip]\n",
    "# trace = trace[trace['speed_mph']>1]\n",
    "single_matches = {}\n",
    "single_matches[random_trip] = map_match.leuven_match(trace,single_matching_settings,map_con,exploded_links)\n",
    "if isinstance(single_matches,str) == False:\n",
    "    m = map_match.visualize_match(random_trip, single_matches, links, config)\n",
    "print(trips_df.loc[random_trip,['trip_type','description','avg_speed_mph']])\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Matching Settings\n",
    "Check the [Leueven Documentation](https://github.com/wannesm/LeuvenMapMatching/blob/9ca9f0b73665252f2ee492fae9dd243feef2f39d/leuvenmapmatching/matcher/distance.py) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(map_match)\n",
    "\n",
    "# The matching setting dictionary stores all of the settings used for map matching, so they can be retrieved later for study\n",
    "if (config['matching_fp'] / 'matching_settings_df.pkl').exists():\n",
    "    with (config['matching_fp'] / 'matching_settings_df.pkl').open('rb') as fh:\n",
    "        matching_settings_df = pickle.load(fh)\n",
    "else:\n",
    "    matching_settings_df = pd.DataFrame()\n",
    "\n",
    "matching_settings = {\n",
    "    'obs_noise': 50, #Standard deviation of noise\n",
    "    'obs_noise_ne': 100, #Standard deviation of noise for non-emitting states (is set to obs_noise if not given)\n",
    "    'max_dist_init': 2000, #Maximum distance from start location (if not given, uses max_dist)\n",
    "    'max_dist': 1000, #Maximum distance from path (this is a hard cut, min_prob_norm should be better)\n",
    "    'min_prob_norm': 0.005, #Minimum normalized probability of observations (ema)\n",
    "    'non_emitting_states': True, #Allow non-emitting states\n",
    "    'non_emitting_length_factor': 0.75, #Reduce the probability of a sequence of non-emitting states the longer it is.\n",
    "    'max_lattice_width': 20, #Restrict the lattice (or possible candidate states per observation) to this value.\n",
    "    'dist_noise': 50, #Standard deviation of difference between distance between states and distance between observations.\n",
    "    'dist_noise_ne': 200, #for no emitting If not given, set to dist_noise\n",
    "    'restrained_ne': True, #Avoid non-emitting states if the distance between states and between observations is close to each other.\n",
    "    'avoid_goingback': True, #If true, the probability is lowered for a transition that returns back to a previous edges or returns to a position on an edge.\n",
    "    'increase_max_lattice_width': False,\n",
    "    'export_graph': False,\n",
    "    'link_types': str(np.sort(link_types_allowed)),\n",
    "    'allow_wrongway': False\n",
    "}\n",
    "#add to matching_settings_tuple if contents are unique\n",
    "row = pd.DataFrame([matching_settings])\n",
    "matching_settings_df = pd.concat([matching_settings_df,row],ignore_index=True)\n",
    "if matching_settings_df.duplicated().any():\n",
    "    print('Settings have been used before')\n",
    "matching_settings_df.drop_duplicates(inplace=True)\n",
    "\n",
    "#check if there are existing matches, using these settings\n",
    "\n",
    "#use this in the qaqc section to line up the ratings with the settings used\n",
    "matching_index = matching_settings_df[(matching_settings_df == tuple(row.loc[0,:])).all(axis=1)].index.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dict = {tripid:map_match.leuven_match(trace,matching_settings,map_con,exploded_links) for tripid, trace in tqdm(coords_dict.items(),total=len(coords_dict))}\n",
    "#553 at 2hr:10min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe with the ratio of points matched, the total gps distance, the total network distance, and mean match distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['matching_fp'].exists() == False:\n",
    "    config['matching_fp'].mkdir(parents=True)\n",
    "\n",
    "# export the matching settings tested\n",
    "with (config['matching_fp']/'matching_settings_df.pkl').open('wb') as fh:\n",
    "    pickle.dump(matching_settings_df,fh)\n",
    "\n",
    "# export the matched traces\n",
    "#TODO add date\n",
    "print('saving as',f'matched_{matching_index}.pkl')\n",
    "with (config['matching_fp']/f'matched_{matching_index}.pkl').open('wb') as fh:\n",
    "    pickle.dump(match_dict,fh)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2, add more links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(map_match)\n",
    "\n",
    "# load matches remove bad matches\n",
    "index = 2\n",
    "matching_index = 4\n",
    "\n",
    "# export \n",
    "with (config['matching_fp']/f'matched_{matching_index}.pkl').open('rb') as fh:\n",
    "    match_dict = pickle.load(fh)\n",
    "\n",
    "#load all traces\n",
    "with (config['cycleatl_fp']/'reduced_spacing.pkl').open('rb') as fh:\n",
    "    coords_dict = pickle.load(fh)\n",
    "\n",
    "with (config['cycleatl_fp']/'trips_4.pkl').open('rb') as fh:\n",
    "    trips_df = pickle.load(fh)\n",
    "coords_dict = {key:item for key, item in coords_dict.items() if key in trips_df['tripid'].tolist()}\n",
    "\n",
    "# get bad matches\n",
    "cutoff = 0.9\n",
    "success, failed_matches, substandard_matches, match_ratios = map_match.mapmatch_results(match_dict,cutoff)\n",
    "\n",
    "# subset the coords dict\n",
    "coords_dict = {key:item for key,item in coords_dict.items() if key in failed_matches+substandard_matches}\n",
    "print(len(coords_dict),'to match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = gpd.read_file(config['network_fp']/'networks.gpkg',layer='osm_links')\n",
    "nodes = gpd.read_file(config['network_fp']/'networks.gpkg',layer='osm_nodes')\n",
    "links.rename(columns={'osm_linkid':'linkid','osm_A':'A','osm_B':'B'},inplace=True)\n",
    "nodes.rename(columns={'osm_N':'N'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(links['link_type'].unique())\n",
    "link_types_allowed = ['bike','pedestrian','road','service']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_links, exploded_nodes, map_con = map_match.prepare_mapmatch_network(links,nodes,link_types_allowed,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try increasing the lattice width adn the obs_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (config['matching_fp'] / 'matching_settings_df.pkl').exists():\n",
    "    with (config['matching_fp'] / 'matching_settings_df.pkl').open('rb') as fh:\n",
    "        matching_settings_df = pickle.load(fh)\n",
    "else:\n",
    "    matching_settings_df = pd.DataFrame()\n",
    "\n",
    "matching_settings = {\n",
    "    'obs_noise': 200, #Standard deviation of noise\n",
    "    'obs_noise_ne': 100, #Standard deviation of noise for non-emitting states (is set to obs_noise if not given)\n",
    "    'max_dist_init': 2000, #Maximum distance from start location (if not given, uses max_dist)\n",
    "    'max_dist': 1000, #Maximum distance from path (this is a hard cut, min_prob_norm should be better)\n",
    "    'min_prob_norm': 0.005, #Minimum normalized probability of observations (ema)\n",
    "    'non_emitting_states': True, #Allow non-emitting states\n",
    "    'non_emitting_length_factor': 0.75, #Reduce the probability of a sequence of non-emitting states the longer it is.\n",
    "    'max_lattice_width': 50, #Restrict the lattice (or possible candidate states per observation) to this value.\n",
    "    'dist_noise': 50, #Standard deviation of difference between distance between states and distance between observations.\n",
    "    'dist_noise_ne': 200, #for no emitting If not given, set to dist_noise\n",
    "    'restrained_ne': True, #Avoid non-emitting states if the distance between states and between observations is close to each other.\n",
    "    'avoid_goingback': True, #If true, the probability is lowered for a transition that returns back to a previous edges or returns to a position on an edge.\n",
    "    'increase_max_lattice_width': False,\n",
    "    'export_graph': False,\n",
    "    'link_types': str(np.sort(link_types_allowed)),\n",
    "    'allow_wrongway': False\n",
    "}\n",
    "\n",
    "#add to matching_settings_tuple if contents are unique\n",
    "row = pd.DataFrame([matching_settings])\n",
    "matching_settings_df = pd.concat([matching_settings_df,row],ignore_index=True)\n",
    "if matching_settings_df.duplicated().any():\n",
    "    print('Settings have been used before')\n",
    "matching_settings_df.drop_duplicates(inplace=True)\n",
    "\n",
    "#check if there are existing matches, using these settings\n",
    "\n",
    "#use this in the qaqc section to line up the ratings with the settings used\n",
    "matching_index = matching_settings_df[(matching_settings_df == tuple(row.loc[0,:])).all(axis=1)].index.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dict = {tripid:map_match.leuven_match(trace,matching_settings,map_con,exploded_links) for tripid, trace in tqdm(coords_dict.items(),total=len(coords_dict))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bad matches\n",
    "reload(map_match)\n",
    "cutoff = 0.9\n",
    "success_matches, substandard_matches, failed_matches, match_ratios = map_match.mapmatch_results(match_dict,cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining Issues\n",
    "- on the beltline trip 262 starts midway and because it's not near any nodes it appears to just fail outright\n",
    "- should i try breaking apart certain links to have more nodes?\n",
    "- trip 17722 still starts out in a parking lot need the parking lot links for that or start removing points from the origin until a match is made\n",
    "- for trip 12479, mdconough blvd is under construction now but it wasn't back then. for the network need to add back in the few links under construction\n",
    "- not sure why 10126 isn't matching but \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from importlib import reload\n",
    "reload(map_match)\n",
    "tripid = random.choice(failed_matches)\n",
    "# tripid = 262\n",
    "print(tripid)\n",
    "map_match.visualize_failed_match(tripid, coords_dict, links, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try re-matching\n",
    "trace = coords_dict[tripid]\n",
    "matching_settings['max_lattice_width'] = 50\n",
    "matching_settings['dist_noise'] = 200\n",
    "match = map_match.leuven_match(trace,matching_settings,map_con,exploded_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dict[tripid] = match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(map_match)\n",
    "html_map = map_match.visualize_match(tripid, match_dict,links,config)\n",
    "html_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['matching_fp'].exists() == False:\n",
    "    config['matching_fp'].mkdir(parents=True)\n",
    "\n",
    "# export the matching settings tested\n",
    "with (config['matching_fp']/'matching_settings_df.pkl').open('wb') as fh:\n",
    "    pickle.dump(matching_settings_df,fh)\n",
    "\n",
    "# export the matched traces\n",
    "#TODO add date for second round\n",
    "with (config['matching_fp']/f'matched_{matching_index}_2.pkl').open('wb') as fh:\n",
    "    pickle.dump(match_dict,fh)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated past here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tmw:\n",
    "- Create functions to reduce clutter\n",
    "- figure out why there are fewer trips now\n",
    "- examine some of the new matches\n",
    "- have a crossing one\n",
    "- add a wrongway one \n",
    "- need to conflate bike paths with road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Print matching stats\n",
    "# Outputs:\n",
    "# - 'edges', \n",
    "# - 'last_matched', \n",
    "# - 'match_ratio', \n",
    "# - 'max_lattice_width', \n",
    "# - 'trace', \n",
    "# - 'match_lines', \n",
    "# - 'interpolated_points', \n",
    "# - 'match_time_sec', \n",
    "# - 'gps_distance', \n",
    "# - 'time', \n",
    "# - 'settings'\n",
    "# matching_index = 0\n",
    "# with (export_fp/f'matched_{matching_index}.pkl').open('rb') as fh:\n",
    "#     match_dict = pickle.load(fh)    \n",
    "# #how many failed matches\n",
    "# failed = {key:item for key, item in match_dict.items() if isinstance(item,str)}\n",
    "# print(len(failed.keys()),'failed matches')\n",
    "# success = {key:item for key, item in match_dict.items() if isinstance(item,str) == False}\n",
    "# length_dict = {row['linkid']:row['geometry'].length for idx, row in df_edges[['linkid','geometry']].drop_duplicates().iterrows()}\n",
    "# results = [[key,item['match_ratio'],item['gps_distance'],item['edges']['linkid'].map(length_dict).sum(),item['match_lines']['length'].mean()] for key, item in success.items()]\n",
    "# results = pd.DataFrame(results,columns=['tripid','match_ratio','gps_distance','network_distance','mean_match_distance'])\n",
    "# results\n",
    "# print((results['match_ratio']>0.9).sum(),'/',results.shape[0],'trips had a quality match')\n",
    "# trips_df_export = trips_df.reset_index(drop=True).merge(results,on='tripid')\n",
    "# with (export_fp/'matched_trips_df.pkl').open('wb') as fh:\n",
    "#     pickle.dump(trips_df_export,fh)\n",
    "# # Examine matches\n",
    "# import random\n",
    "\n",
    "# def get_random_key(dictionary):\n",
    "#     random_key =  random.choice(list(dictionary.keys()))\n",
    "#     #recursion?\n",
    "#     if isinstance(dictionary.get(random_key),str):\n",
    "#         random_key = get_random_key(dictionary)\n",
    "#     return random_key\n",
    "# results['match_ratio'].hist()\n",
    "# (results['match_ratio']>0.8).sum()\n",
    "# pool = results.loc[(results['match_ratio']>.9) & (results['match_ratio']<1),'tripid'].tolist()\n",
    "# #tripid = get_random_key(match_dict)\n",
    "# tripid = random.choice(pool)\n",
    "# map_match.visualize_match(tripid, match_dict, df_edges)\n",
    "\n",
    "# # Post Match Cleanup (in development)\n",
    "# Some trips have out-and-backing and or take the wrong link if two nodes have more than one link between them. This step goes through and cleans these trips.\n",
    "\n",
    "# For out and backing:\n",
    "# Subset network graph to only the edges between origin and destination and then use Dijkstra's algorithim to return the shortest path. Check the Frechet distance to determine if cleaned match better represents trajectory than previously.\n",
    "# trip_w_out_and_backing = 550\n",
    "# map_match.visualize_match(trip_w_out_and_backing, match_dict, df_edges)\n",
    "\n",
    "# match_dict[801]['edges']\n",
    "# gpd.GeoDataFrame(match_dict[801]['edges'].merge(edges,on='linkid')).explore()\n",
    "# #TODO use to make network graph, then subset with trip\n",
    "# import networkx as nx\n",
    "\n",
    "# MDG = nx.MultiDiGraph()  # Create a MultiDiGraph\n",
    "#     #itertuples used to maintain the type\n",
    "# for idx, row in exploded_edges.iterrows():\n",
    "#     #edge_data = {linkid: row[2],'reverse_link': False, 'azimuth': row[4]}\n",
    "#     MDG.add_edge(int(row['A']), int(row['B']), **{'weight': row['length_ft']})#**edge_data)  # Add edge with linkid attribute\n",
    "#     #add reverse link if oneway is not true\n",
    "#     MDG.add_edge(int(row['B']), int(row['A']), **{'weight': row['length_ft']})\n",
    "#     # if row[3] == False:\n",
    "#     #     edge_data['reverse_link'] = True \n",
    "#     #     #reverse the azimuth\n",
    "#     #     edge_data['azimuth'] = row[5]\n",
    "#     #     MDG.add_edge(row[1], row[0], **edge_data)\n",
    "\n",
    "# #exploded_edges, exploded_nodes\n",
    "# tripid = 801\n",
    "\n",
    "# test = match_dict[801]['edges'].merge(edges,on='linkid')\n",
    "# sub_nodes = test['A'].append(test['B']).unique().tolist()\n",
    "# #get start and end linkid\n",
    "# start = match_dict[tripid]['edges'].iloc[0,:]\n",
    "# end = match_dict[tripid]['edges'].iloc[-1,:]\n",
    "\n",
    "# #get start and end node\n",
    "# start_a_b = edges.loc[edges['linkid']==start['linkid'],['A','B']]\n",
    "# end_a_b = edges.loc[edges['linkid']==end['linkid'],['A','B']]\n",
    "\n",
    "# if start['forward']:\n",
    "#     start = start_a_b['A'].item()\n",
    "# else:\n",
    "#     start = start_a_b['B'].item()\n",
    "\n",
    "# if end['forward']:\n",
    "#     end = end_a_b['B'].item()\n",
    "# else:\n",
    "#     end = end_a_b['A'].item()\n",
    "# sub_nodes[0]\n",
    "# start\n",
    "# start in sub_nodes\n",
    "# end in sub_nodes\n",
    "# path\n",
    "# subgraph = MDG.subgraph(sub_nodes)\n",
    "# length, path = nx.single_source_dijkstra(subgraph,start,end,weight='weight')\n",
    "\n",
    "# #turn to edge list\n",
    "# edge_list = [(path[i],path[i+1]) for i in range(len(path)-1)]\n",
    "# edge_df = pd.DataFrame(edge_list,columns=['A','B'])\n",
    "# forward = pd.merge(edge_df,edges[['A','B','linkid','geometry']],on=['A','B'])#[['linkid','A','B','geometry']]\n",
    "# forward\n",
    "# reverse = pd.merge(edge_df,edges[['A','B','linkid','geometry']],left_on=['B','A'],right_on=['A','B'])[['linkid','A','B','geometry']]\n",
    "# shortest_path = pd.concat([forward,reverse],ignore_index=True)\n",
    "# shortest_path = shortest_path.loc[shortest_path.groupby(['A','B'])['length_ft'].idxmin()]\n",
    "# gpd.GeoDataFrame(shortest_path).explore()\n",
    "# #TODO deal with duplicate links\n",
    "# shortest_path.explore()\n",
    "# For multi-edges, buffer the 2+ edges and take the one that hits the most gps points\n",
    "# import numpy as np\n",
    "# exploded_edges['A_sort'] = np.sort(exploded_edges[['A','B']].to_numpy())[:,0]\n",
    "# exploded_edges['B_sort'] = np.sort(exploded_edges[['A','B']].to_numpy())[:,1]\n",
    "# duplicate_edges = exploded_edges.loc[exploded_edges[['A_sort','B_sort']].duplicated(keep=False),'linkid'].unique()\n",
    "# gps_points = match_dict[tripid]['trace']\n",
    "# # matched_trip = match_dict[tripid]['edges'].merge(edges, on='linkid')\n",
    "# # matched_trip = gpd.GeoDataFrame(matched_trip)\n",
    "# # from shapely.ops import MultiLineString\n",
    "# # buffered_geo = MultiLineString(matched_trip.geometry.tolist()).buffer(100)\n",
    "# # match['trace'].intersects(buffered_geo).sum()\n",
    "# # # export \n",
    "# # with (export_fp/'sample_matched.pkl').open('wb') as fh:\n",
    "# #     pickle.dump(match_dict,fh)\n",
    "# # with (export_fp/'sample_matched.pkl').open('rb') as fh:\n",
    "# #     match_dict = pickle.load(fh)\n",
    "\n",
    "# # Visualization\n",
    "\n",
    "# import folium\n",
    "# import geopandas as gpd\n",
    "# from folium.plugins import MarkerCluster, PolyLineTextPath\n",
    "# from folium.map import FeatureGroup\n",
    "\n",
    "# #tripid = 29837#7257#9806#30000#8429\n",
    "\n",
    "# # Your GeoDataFrames\n",
    "# matched_trip = match_dict[tripid]['edges'].merge(edges, on='linkid')\n",
    "# matched_trip = gpd.GeoDataFrame(matched_trip)\n",
    "# gps_points = match_dict[tripid]['trace']\n",
    "# match_lines = match_dict[tripid]['match_lines']\n",
    "\n",
    "# #get the start and end point for mapping\n",
    "# start_pt = gps_points.to_crs(epsg='4326').loc[gps_points['sequence'].idxmin(),'geometry']\n",
    "# end_pt = gps_points.to_crs(epsg='4326').loc[gps_points['sequence'].idxmax(),'geometry']\n",
    "\n",
    "# # reproject and get the center of the map\n",
    "# x_mean = gps_points.to_crs(epsg='4326')['geometry'].x.mean()\n",
    "# y_mean = gps_points.to_crs(epsg='4326')['geometry'].y.mean()\n",
    "\n",
    "# # Create a Folium map centered around the mean of the GPS points\n",
    "# center = [y_mean,x_mean]\n",
    "# mymap = folium.Map(location=center, zoom_start=14)\n",
    "\n",
    "# # Convert GeoDataFrames to GeoJSON\n",
    "# matched_trip_geojson = matched_trip[['linkid','geometry']].to_crs(epsg='4326').to_json()\n",
    "# gps_points_geojson = gps_points[['sequence','geometry']].to_crs(epsg='4326').to_json()\n",
    "# match_lines_geojson = match_lines[['sequence','match_lines']].to_crs(epsg='4326').to_json()\n",
    "\n",
    "# # Create FeatureGroups for each GeoDataFrame\n",
    "# matched_trip_fg = FeatureGroup(name='Matched Trip')\n",
    "# gps_points_fg = FeatureGroup(name='GPS Points')\n",
    "# match_lines_fg = FeatureGroup(name='Match Lines')\n",
    "\n",
    "# # Add GeoJSON data to FeatureGroups\n",
    "# folium.GeoJson(matched_trip_geojson, name='Matched Trip', style_function=lambda x: {'color': 'red'}).add_to(matched_trip_fg)\n",
    "\n",
    "# # Add circles to the GPS Points FeatureGroup\n",
    "# for idx, row in gps_points.iterrows():\n",
    "#     folium.Circle(location=[row['lat'], row['lon']], radius=5, color='grey', fill=True, fill_color='grey').add_to(gps_points_fg)\n",
    "\n",
    "# # Add GeoJSON data to Match Lines FeatureGroup with transparent and grey style\n",
    "# folium.GeoJson(match_lines_geojson, name='Match Lines', style_function=lambda x: {'color': 'grey', 'opacity': 0.5}).add_to(match_lines_fg)\n",
    "\n",
    "# # Add FeatureGroups to the map\n",
    "# matched_trip_fg.add_to(mymap)\n",
    "# gps_points_fg.add_to(mymap)\n",
    "# match_lines_fg.add_to(mymap)\n",
    "\n",
    "# # Add start and end points with play and stop buttons\n",
    "# start_icon = folium.Icon(color='green',icon='play',prefix='fa')\n",
    "# end_icon = folium.Icon(color='red',icon='stop',prefix='fa')\n",
    "# folium.Marker(location=[start_pt.y, start_pt.x],icon=start_icon).add_to(mymap)\n",
    "# folium.Marker(location=[end_pt.y, end_pt.x],icon=end_icon).add_to(mymap)\n",
    "\n",
    "# # Add layer control to toggle layers on/off\n",
    "# folium.LayerControl().add_to(mymap)\n",
    "\n",
    "# # Add legend with statistics\n",
    "# #TODO what happened to duration\n",
    "# legend_html = f'''\n",
    "#     <div style=\"position: fixed; \n",
    "#             bottom: 5px; left: 5px; width: 300px; height: 250px; \n",
    "#             border:2px solid grey; z-index:9999; font-size:14px;\n",
    "#             background-color: white;\n",
    "#             opacity: 0.9;\">\n",
    "#     &nbsp; <b>Trip ID: {tripid} </b> <br>\n",
    "#     &nbsp; <b> Match Date: {match_dict[tripid]['time']} </b> <br>\n",
    "#     &nbsp; Start Point &nbsp; <i class=\"fa fa-play\" style=\"color:green\"></i>,\n",
    "#     End Point &nbsp; <i class=\"fa fa-stop\" style=\"color:red\"></i> <br>\n",
    "    \n",
    "#     &nbsp; Matched Path &nbsp; <div style=\"width: 20px; height: 5px; background-color: red; display: inline-block;\"></div> <br>\n",
    "#     &nbsp; Match Lines Path &nbsp; <div style=\"width: 20px; height: 5px; background-color: gray; display: inline-block;\"></div> <br>\n",
    " \n",
    "#     &nbsp; Points Matched: {match_dict[tripid]['last_matched']}/{match_dict[tripid]['trace'].shape[0]} <br>\n",
    "#     &nbsp; Match Ratio: {match_dict[tripid]['match_ratio']:.2f} <br>\n",
    "#     &nbsp; GPS Distance: {match_dict[tripid]['gps_distance']:.1f} ft. <br>\n",
    "#     &nbsp; Matched Trace Distance: {matched_trip.length.sum():.0f} ft. <br>\n",
    "#     &nbsp; Mean Matching Distance: {match_dict[tripid]['match_lines'].length.mean():.0f} ft. \n",
    "\n",
    "#     </div>\n",
    "#     '''\n",
    "# mymap.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# # Save the map to an HTML file or display it in a Jupyter notebook\n",
    "# #mymap.save('map.html')\n",
    "# # mymap.save('/path/to/save/map.html')  # Use an absolute path if needed\n",
    "# mymap  # Uncomment if you are using Jupyter notebook\n",
    "\n",
    "# #TODO add in the legend with trip info and then we're golden\n",
    "\n",
    "# match_dict[tripid].keys()\n",
    "# match_dict[tripid]['match_ratio']\n",
    "# match_dict[tripid].keys()\n",
    "# help(InMemMap)\n",
    "# help(DistanceMatcher)\n",
    "# :param map_con: Map object to connect to map database\n",
    "#         :param obs_noise: Standard deviation of noise\n",
    "#         :param obs_noise_ne: Standard deviation of noise for non-emitting states (is set to obs_noise if not given)\n",
    "#         :param max_dist_init: Maximum distance from start location (if not given, uses max_dist)\n",
    "#         :param max_dist: Maximum distance from path (this is a hard cut, min_prob_norm should be better)\n",
    "#         :param min_prob_norm: Minimum normalized probability of observations (ema)\n",
    "#         :param non_emitting_states: Allow non-emitting states. A non-emitting state is a state that is\n",
    "#             not associated with an observation. Here we assume it can be associated with a location in between\n",
    "#             two observations to allow for pruning. It is advised to set min_prob_norm and/or max_dist to avoid\n",
    "#             visiting all possible nodes in the graph.\n",
    "#         :param non_emitting_length_factor: Reduce the probability of a sequence of non-emitting states the longer it\n",
    "#             is. This can be used to prefer shorter paths. This is separate from the transition probabilities because\n",
    "#             transition probabilities are averaged for non-emitting states and thus the length is also averaged out.\n",
    "#         :param max_lattice_width: Restrict the lattice (or possible candidate states per observation) to this value.\n",
    "#             If there are more possible next states, the states with the best likelihood so far are selected.\n",
    "\n",
    "#         :param dist_noise: Standard deviation of difference between distance between states and distance\n",
    "#             between observatoins. If not given, set to obs_noise\n",
    "#         :param dist_noise_ne: If not given, set to dist_noise\n",
    "#         :param restrained_ne: Avoid non-emitting states if the distance between states and between\n",
    "#             observations is close to each other.\n",
    "#         :param avoid_goingback: If true, the probability is lowered for a transition that returns back to a\n",
    "#             previous edges or returns to a position on an edge.\n",
    "\n",
    "# # #get list of coords\n",
    "# # gps_trace = list(zip(trace.geometry.y,trace.geometry.x))\n",
    "\n",
    "# # #perform matching\n",
    "# # states, last_matched = matcher.match(gps_trace)\n",
    "# # only_nodes = matcher.path_pred_onlynodes\n",
    "\n",
    "# # print(\"States\\n------\")\n",
    "# # print(states)\n",
    "# # print(\"Nodes\\n------\")\n",
    "# # print(only_nodes)\n",
    "# # print(\"\")\n",
    "# # matcher.print_lattice_stats()\n",
    "# # fig, ax = plt.subplots(1, 1)\n",
    "# # mmviz.plot_map(map_con, matcher=matcher,\n",
    "# #                ax=ax,\n",
    "# #                show_labels=True, show_matching=True, show_graph=False,\n",
    "# #                filename=\"my_plot.png\")\n",
    "# # test = matcher.lattice[4]\n",
    "# # m = max(test.values_all(), key=lambda m: m.logprob) # for the 4th point get the one with the highest logprob\n",
    "\n",
    "# # m.logprob\n",
    "# # import numpy as np\n",
    "# # t = {x.cname.split('_')[0] + '_' + x.cname.split('_')[1]: x.logprob for x in test.values_all()}\n",
    "# # check = pd.DataFrame.from_dict(t,orient='index',columns=['logprob']).sort_values('logprob',ascending=False)\n",
    "# # check\n",
    "# # (check.index == '5424132517_7151205661').sum()\n",
    "# # testing = trace.copy()\n",
    "# # testing.geometry = testing.buffer(1000)\n",
    "# # intersect = gpd.overlay(edges,testing)\n",
    "# # intersect[(intersect['A_B'] == '5424132517_7151205661') & (intersect['sequence'] == 4)]\n",
    "\n",
    "# # #reduce the states size with match_nodes\n",
    "# # reduced_states = list(set(edges))\n",
    "\n",
    "# # #calculate the match ratio\n",
    "# # match_ratio = last_matched / (len(gps_trace)-1)\n",
    "    \n",
    "# # #retreive matched edges from network\n",
    "# # geos_list = [geos_dict.get(id,0) for id in reduced_states]\n",
    "\n",
    "# # #turn into geodataframe\n",
    "# # matched_trip = gpd.GeoDataFrame(data={'A_B':reduced_states,'geometry':geos_list},geometry='geometry',crs='epsg:2240')\n",
    "\n",
    "# # #turn tuple to str\n",
    "# # matched_trip['A_B'] = matched_trip['A_B'].apply(lambda row: f'{row[0]}_{row[1]}')\n",
    "\n",
    "# # #reset index to add an edge sequence column\n",
    "# # matched_trip.reset_index().rename(columns={'index':'edge_sequence'},inplace=True)\n",
    "\n",
    "# # trace['interpolated_point'] = pd.Series([ Point(x.edge_m.pi) for x in matcher.lattice_best ])\n",
    "# # trace = trace.loc[0:last_matched]\n",
    "# # trace['match_lines'] = trace.apply(lambda row: LineString([row['geometry'],row['interpolated_point']]),axis=1)\n",
    "\n",
    "# # interpolated_points = trace[['sequence','interpolated_point']]\n",
    "# # interpolated_points = gpd.GeoDataFrame(interpolated_points,geometry='interpolated_point')\n",
    "\n",
    "# # match_lines = trace[['sequence','match_lines']]\n",
    "# # match_lines = gpd.GeoDataFrame(match_lines,geometry='match_lines')\n",
    "# # match_lines['length'] = match_lines.length\n",
    "\n",
    "\n",
    "# # interpolated_points.to_file(project_dir/f\"single_example/{tripid}.gpkg\",layer='interpolated_points')\n",
    "# # match_lines.to_file(project_dir/f\"single_example/{tripid}.gpkg\",layer='match_lines')\n",
    "\n",
    "# # #%%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
