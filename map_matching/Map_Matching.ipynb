{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO (in order of priority):\n",
    "1. Make sure that the cleanup code works (come back to this)\n",
    "    1. Want to get rid of the really small out and backs\n",
    "    1. Have method for doing this, but worried that it will remove valid out and backing\n",
    "    1. Frechet distance could be a way to tell if the cleaning removes too much out and backing? Or some sort of overlap metric?\n",
    "1. Use geo_dict to assemble edge for calculated frechet distance\n",
    "    1. Frechet distance will give a decent measure of how close the match is to the trace\n",
    "    1. Will need to reverse link geometry if passing the other way otherwise it won't work\n",
    "1. Match all the traces and export for impedance calibration\n",
    "    1. The process flow into this step is still uncertain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from leuvenmapmatching.matcher.distance import DistanceMatcher\n",
    "from leuvenmapmatching.map.inmem import InMemMap\n",
    "#from leuvenmapmatching import visualization as mmviz\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from shapely.ops import Point, LineString\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import map_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = json.load((Path.cwd().parent / 'config.json').open('rb'))\n",
    "export_fp = Path(config['project_directory'])\n",
    "if export_fp.exists() == False:\n",
    "    export_fp.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file paths\n",
    "#network_fp = Path(config['project_directory']) / 'Map_Matching/matching.gpkg'\n",
    "network_fp = Path(config['project_directory']) / 'Network' \n",
    "traces_fp = Path(config['project_directory']) / 'CycleAtlanta'\n",
    "export_fp = Path(config['project_directory']) / 'Map_Matching'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Network\n",
    "Bring in the pre-processed network and filter as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Matching\n",
    "Sometimes matching can be improved just by limiting what can be matched to. For this, we want to start by mapping to the most restrictive network. Then we add more links if a match hasn't been found.\n",
    "\n",
    "1. Network without parking lot roads\n",
    "1. Network without oneway restrictions\n",
    "1. Full network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with (network_fp / 'nodes.pkl').open('rb') as fh:\n",
    "#     nodes = pickle.load(fh)\n",
    "# # with (network_fp / 'directed_edges.pkl').open('rb') as fh:\n",
    "# #     df_edges = pickle.load(fh)\n",
    "# directed_links = pd.read_parquet(network_fp/'directed_edges.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges = gpd.read_file(network_fp/'final_network.gpkg',layer='edges')\n",
    "nodes = gpd.read_file(network_fp/'final_network.gpkg',layer='nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['road', 'service', 'parking_and_driveways', 'pedestrian', 'bike',\n",
       "       'sidewalk_or_crossing'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_types_allowed = ['bike','pedestrian','road']\n",
    "df_edges['link_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges = df_edges[df_edges['link_type'].isin(link_types_allowed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges.rename(columns={'A':'source','B':'target'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only allow roads + pedestrian + bike?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_edges = df_edges.merge(edges[['linkid','geometry']])\n",
    "# df_edges = gpd.GeoDataFrame(df_edges,geometry='geometry',crs=edges.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter network and remove isolates\n",
    "\n",
    "# #filter using link type\n",
    "# link_types = ['bike','road','pedestrian','service']\n",
    "# filtered = df_edges[df_edges['link_type'].isin(link_types)]\n",
    "# #create multigraph\n",
    "# import networkx as nx\n",
    "# MDG = nx.Graph()\n",
    "\n",
    "# #remove isolates\n",
    "# MDG.add_edges_from(list(zip(filtered['source'],filtered['target'])))\n",
    "# largest_cc = max(nx.connected_components(MDG), key=len)\n",
    "\n",
    "#subset \n",
    "\n",
    "#turn to multigraph, remove isolates turn back into link dataframe\n",
    "#also remove nodes\n",
    "# in_largest_cc = filtered['source'].isin(largest_cc) | filtered['target'].isin(largest_cc)\n",
    "# filtered['isolate'] = False\n",
    "# filtered.loc[~in_largest_cc,'isolate'] = True\n",
    "# matching_network_links = filtered.loc[filtered['isolate']==False]\n",
    "\n",
    "# node_filt = set(matching_network_links['source'].append(matching_network_links['target']).tolist())\n",
    "# matching_network_nodes = nodes.loc[nodes['N'].isin(node_filt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import network\n",
    "# edges = gpd.read_file(network_fp/'final_network.gpkg',layer=\"final_network\")\n",
    "# #edges.reset_index(inplace=True)\n",
    "# #edges.rename(columns={'index':'linkid'},inplace=True)\n",
    "\n",
    "#TODO next features\n",
    "# need to add the ability to remove isolates when doing this so we don't have matches to links that don't go anywhere\n",
    "# allow_wrongway = \n",
    "# link_types_to_include = ['road','bike','pedestrian]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking Apart Multi-Edges\n",
    "Occasionally, there will be multiple edges between two nodes. A common place for this to occur are local roads connected to main roads that form u-shapes. Leuven map mapmatching cannot handle multi-edges as the only information stored in the network graph are the nodes. Usually, it's obvious which edge should be retained, but an easy way to still include both edges for map matching is to break these edges by thier centroid into smaller edges with new nodes. The below cell does this.\n",
    "\n",
    "The centroid nodes created in this step are given a unique id. The map matching function removes these new nodes so that they don't appear in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO condense this into a funciton\n",
    "\n",
    "#get max ids for adding the new midpoint nodes\n",
    "max_nodeid = nodes['N'].max()\n",
    "max_linkid = df_edges['linkid'].max()\n",
    "\n",
    "# identify multi-edges so that we can break them apart (we already have duplicate edges)\n",
    "df_sorted = df_edges.sort_values(by=['source','target'])\n",
    "grouped_df = df_sorted.groupby(['source','target'])['linkid'].nunique().reset_index(name='num_linkid')\n",
    "grouped_df = grouped_df[grouped_df['num_linkid']>1]\n",
    "merged = pd.merge(df_sorted,grouped_df,on=['source','target'])\n",
    "multi_edges = df_edges[df_edges['linkid'].isin(set(merged['linkid'].tolist()))]\n",
    "\n",
    "#remove these multi-edges from the dataframe\n",
    "non_multi_edges = df_edges[~df_edges['linkid'].isin(set(merged['linkid'].tolist()))]\n",
    "new_links, new_nodes = map_match.explode_network_midpoint(multi_edges,max_nodeid,max_linkid)\n",
    "exploded_nodes = pd.concat([nodes,new_nodes],ignore_index=True)\n",
    "exploded_links = pd.concat([non_multi_edges,new_links],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create map matching graph network (no wrongway)\n",
    "map_con = map_match.make_network(exploded_links,exploded_nodes,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>linkid</th>\n",
       "      <th>link_type</th>\n",
       "      <th>osmid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>version</th>\n",
       "      <th>type</th>\n",
       "      <th>highway</th>\n",
       "      <th>oneway</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>lts</th>\n",
       "      <th>reverse_geometry</th>\n",
       "      <th>ascent_m</th>\n",
       "      <th>ascent_grade_%</th>\n",
       "      <th>descent_m</th>\n",
       "      <th>descent_grade_%</th>\n",
       "      <th>length_ft</th>\n",
       "      <th>geometry</th>\n",
       "      <th>source_linkid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6710023987</td>\n",
       "      <td>69428484</td>\n",
       "      <td>1125582092</td>\n",
       "      <td>road</td>\n",
       "      <td>50346065.0</td>\n",
       "      <td>1.608717e+09</td>\n",
       "      <td>19.0</td>\n",
       "      <td>way</td>\n",
       "      <td>primary</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>1527.030230</td>\n",
       "      <td>LINESTRING (2199028.737 1318143.985, 2198937.4...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6715039076</td>\n",
       "      <td>69359601</td>\n",
       "      <td>1125582111</td>\n",
       "      <td>road</td>\n",
       "      <td>630014235.0</td>\n",
       "      <td>1.565940e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>way</td>\n",
       "      <td>residential</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>256.320965</td>\n",
       "      <td>LINESTRING (2199301.414 1317716.546, 2199244.0...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6715039076</td>\n",
       "      <td>69359606</td>\n",
       "      <td>1125582112</td>\n",
       "      <td>road</td>\n",
       "      <td>630014235.0</td>\n",
       "      <td>1.565940e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>way</td>\n",
       "      <td>residential</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>253.291057</td>\n",
       "      <td>LINESTRING (2199301.414 1317716.546, 2199345.5...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8317885804</td>\n",
       "      <td>69302632</td>\n",
       "      <td>1125582121</td>\n",
       "      <td>road</td>\n",
       "      <td>9250903.0</td>\n",
       "      <td>1.610525e+09</td>\n",
       "      <td>3.0</td>\n",
       "      <td>way</td>\n",
       "      <td>service</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>660.819821</td>\n",
       "      <td>LINESTRING (2200337.281 1317974.107, 2200336.5...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5426585244</td>\n",
       "      <td>5426567807</td>\n",
       "      <td>1125582134</td>\n",
       "      <td>road</td>\n",
       "      <td>143758462.0</td>\n",
       "      <td>1.636971e+09</td>\n",
       "      <td>11.0</td>\n",
       "      <td>way</td>\n",
       "      <td>secondary</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>41.016447</td>\n",
       "      <td>LINESTRING (2202935.350 1318104.648, 2202941.7...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79872</th>\n",
       "      <td>10291023734</td>\n",
       "      <td>3468992596</td>\n",
       "      <td>1125782140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LINESTRING (2219574.311 1391641.987, 2219618.6...</td>\n",
       "      <td>1.125780e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79873</th>\n",
       "      <td>5422336613</td>\n",
       "      <td>10291023735</td>\n",
       "      <td>1125782141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LINESTRING (2223868.217 1395025.098, 2223826.0...</td>\n",
       "      <td>1.125780e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79874</th>\n",
       "      <td>10291023735</td>\n",
       "      <td>69405652</td>\n",
       "      <td>1125782142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LINESTRING (2223826.064 1395054.445, 2223858.1...</td>\n",
       "      <td>1.125780e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79875</th>\n",
       "      <td>5422336613</td>\n",
       "      <td>10291023736</td>\n",
       "      <td>1125782143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LINESTRING (2223868.217 1395025.098, 2223895.7...</td>\n",
       "      <td>1.125780e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79876</th>\n",
       "      <td>10291023736</td>\n",
       "      <td>69405652</td>\n",
       "      <td>1125782144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LINESTRING (2223895.726 1395064.632, 2223858.1...</td>\n",
       "      <td>1.125780e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79877 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            source       target      linkid link_type        osmid  \\\n",
       "0       6710023987     69428484  1125582092      road   50346065.0   \n",
       "1       6715039076     69359601  1125582111      road  630014235.0   \n",
       "2       6715039076     69359606  1125582112      road  630014235.0   \n",
       "3       8317885804     69302632  1125582121      road    9250903.0   \n",
       "4       5426585244   5426567807  1125582134      road  143758462.0   \n",
       "...            ...          ...         ...       ...          ...   \n",
       "79872  10291023734   3468992596  1125782140       NaN          NaN   \n",
       "79873   5422336613  10291023735  1125782141       NaN          NaN   \n",
       "79874  10291023735     69405652  1125782142       NaN          NaN   \n",
       "79875   5422336613  10291023736  1125782143       NaN          NaN   \n",
       "79876  10291023736     69405652  1125782144       NaN          NaN   \n",
       "\n",
       "          timestamp  version type      highway  oneway  ... year  lts  \\\n",
       "0      1.608717e+09     19.0  way      primary   False  ...  NaN  4.0   \n",
       "1      1.565940e+09      2.0  way  residential   False  ...  NaN  2.0   \n",
       "2      1.565940e+09      2.0  way  residential   False  ...  NaN  2.0   \n",
       "3      1.610525e+09      3.0  way      service   False  ...  NaN  3.0   \n",
       "4      1.636971e+09     11.0  way    secondary   False  ...  NaN  4.0   \n",
       "...             ...      ...  ...          ...     ...  ...  ...  ...   \n",
       "79872           NaN      NaN  NaN          NaN   False  ...  NaN  NaN   \n",
       "79873           NaN      NaN  NaN          NaN   False  ...  NaN  NaN   \n",
       "79874           NaN      NaN  NaN          NaN   False  ...  NaN  NaN   \n",
       "79875           NaN      NaN  NaN          NaN   False  ...  NaN  NaN   \n",
       "79876           NaN      NaN  NaN          NaN   False  ...  NaN  NaN   \n",
       "\n",
       "      reverse_geometry ascent_m ascent_grade_% descent_m descent_grade_%  \\\n",
       "0                    1      4.5           0.96      -2.1           -0.45   \n",
       "1                    1      0.6           0.73      -0.1           -0.17   \n",
       "2                    0      0.0           0.00      -1.7           -1.08   \n",
       "3                    0      1.6           0.68      -1.9           -0.79   \n",
       "4                    1      0.0           0.00      -0.1           -0.06   \n",
       "...                ...      ...            ...       ...             ...   \n",
       "79872              NaN      NaN            NaN       NaN             NaN   \n",
       "79873              NaN      NaN            NaN       NaN             NaN   \n",
       "79874              NaN      NaN            NaN       NaN             NaN   \n",
       "79875              NaN      NaN            NaN       NaN             NaN   \n",
       "79876              NaN      NaN            NaN       NaN             NaN   \n",
       "\n",
       "         length_ft                                           geometry  \\\n",
       "0      1527.030230  LINESTRING (2199028.737 1318143.985, 2198937.4...   \n",
       "1       256.320965  LINESTRING (2199301.414 1317716.546, 2199244.0...   \n",
       "2       253.291057  LINESTRING (2199301.414 1317716.546, 2199345.5...   \n",
       "3       660.819821  LINESTRING (2200337.281 1317974.107, 2200336.5...   \n",
       "4        41.016447  LINESTRING (2202935.350 1318104.648, 2202941.7...   \n",
       "...            ...                                                ...   \n",
       "79872          NaN  LINESTRING (2219574.311 1391641.987, 2219618.6...   \n",
       "79873          NaN  LINESTRING (2223868.217 1395025.098, 2223826.0...   \n",
       "79874          NaN  LINESTRING (2223826.064 1395054.445, 2223858.1...   \n",
       "79875          NaN  LINESTRING (2223868.217 1395025.098, 2223895.7...   \n",
       "79876          NaN  LINESTRING (2223895.726 1395064.632, 2223858.1...   \n",
       "\n",
       "      source_linkid  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "...             ...  \n",
       "79872  1.125780e+09  \n",
       "79873  1.125780e+09  \n",
       "79874  1.125780e+09  \n",
       "79875  1.125780e+09  \n",
       "79876  1.125780e+09  \n",
       "\n",
       "[79877 rows x 35 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded_nodes.set_crs(config['projected_crs_epsg'],inplace=True)\n",
    "exploded_links.set_crs(config['projected_crs_epsg'],inplace=True)\n",
    "\n",
    "#optional inspect exploded network\n",
    "#exploded_nodes.to_file(network_fp/'matching_network.gpkg',layer='exploded_nodes')\n",
    "#exploded_links.to_file(network_fp/'matching_network.gpkg',layer='exploded_links')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace Data\n",
    "For map matching, we're using GPS traces that have been processed so that each point is spaced a certain distance apart, coordinates in between this distance are dropped to improve computation time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all traces\n",
    "with (traces_fp/'reduced_spacing.pkl').open('rb') as fh:\n",
    "    coords_dict = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (traces_fp/'trips_3.pkl').open('rb') as fh:\n",
    "    trips_df = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_dict = {key:item for key, item in coords_dict.items() if key in trips_df['tripid'].tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2765"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import list of trips to include\n",
    "len(coords_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matching setting dictionary stores all of the settings used for map matching, so they can be retrieved later for study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (export_fp / 'matching_settings_df.pkl').exists():\n",
    "    with (export_fp / 'matching_settings_df.pkl').open('rb') as fh:\n",
    "        matching_settings_df = pickle.load(fh)\n",
    "else:\n",
    "    matching_settings_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Leueven Documentation](https://github.com/wannesm/LeuvenMapMatching/blob/9ca9f0b73665252f2ee492fae9dd243feef2f39d/leuvenmapmatching/matcher/distance.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach matching index to the match dict instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings have been used before\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(map_match)\n",
    "\n",
    "matching_settings = {\n",
    "    'obs_noise': 50, #Standard deviation of noise\n",
    "    'obs_noise_ne': 100, #Standard deviation of noise for non-emitting states (is set to obs_noise if not given)\n",
    "    'max_dist_init': 2000, #Maximum distance from start location (if not given, uses max_dist)\n",
    "    'max_dist': 1000, #Maximum distance from path (this is a hard cut, min_prob_norm should be better)\n",
    "    'min_prob_norm': 0.005, #Minimum normalized probability of observations (ema)\n",
    "    'non_emitting_states': False, #Allow non-emitting states\n",
    "    'non_emitting_length_factor': 0.75, #Reduce the probability of a sequence of non-emitting states the longer it is.\n",
    "    'max_lattice_width': 55, #Restrict the lattice (or possible candidate states per observation) to this value.\n",
    "    'dist_noise': 50, #Standard deviation of difference between distance between states and distance between observations.\n",
    "    'dist_noise_ne': 200, #for no emitting If not given, set to dist_noise\n",
    "    'restrained_ne': True, #Avoid non-emitting states if the distance between states and between observations is close to each other.\n",
    "    'avoid_goingback': True, #If true, the probability is lowered for a transition that returns back to a previous edges or returns to a position on an edge.\n",
    "    'increase_max_lattice_width': False,\n",
    "    'export_graph': False\n",
    "}\n",
    "\n",
    "#add to matching_settings_tuple if contents are unique\n",
    "row = pd.DataFrame([matching_settings])\n",
    "matching_settings_df = pd.concat([matching_settings_df,row],ignore_index=True)\n",
    "if matching_settings_df.duplicated().any():\n",
    "    print('Settings have been used before')\n",
    "matching_settings_df.drop_duplicates(inplace=True)\n",
    "\n",
    "#check if there are existing matches, using these settings\n",
    "\n",
    "\n",
    "#use this in the qaqc section to line up the ratings with the settings used\n",
    "matching_index = matching_settings_df[(matching_settings_df == tuple(row.loc[0,:])).all(axis=1)].index.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Match Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_matching_settings = {\n",
    "    'obs_noise': 50, #Standard deviation of noise\n",
    "    'obs_noise_ne': 100, #Standard deviation of noise for non-emitting states (is set to obs_noise if not given)\n",
    "    'max_dist_init': 2000, #Maximum distance from start location (if not given, uses max_dist)\n",
    "    'max_dist': 1000, #Maximum distance from path (this is a hard cut, min_prob_norm should be better)\n",
    "    'min_prob_norm': 0.001, #Minimum normalized probability of observations (ema)\n",
    "    'non_emitting_states': True, #Allow non-emitting states\n",
    "    'non_emitting_length_factor': 0.75, #Reduce the probability of a sequence of non-emitting states the longer it is.\n",
    "    'max_lattice_width': 50, #Restrict the lattice (or possible candidate states per observation) to this value.\n",
    "    'dist_noise': 50, #Standard deviation of difference between distance between states and distance between observations.\n",
    "    'dist_noise_ne': 200, #for no emitting If not given, set to dist_noise\n",
    "    'restrained_ne': False, #Avoid non-emitting states if the distance between states and between observations is close to each other.\n",
    "    'avoid_goingback': True, #If true, the probability is lowered for a transition that returns back to a previous edges or returns to a position on an edge.\n",
    "    'increase_max_lattice_width': False,\n",
    "    'export_graph': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- 33393 has issues with the starting match since it starts from the train station, only a really low min_prob_norm helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_trip = random.choice(list(coords_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_trip = 33393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = coords_dict[random_trip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try removing points until a speed above 4mph is detected\n",
    "trace = trace.loc[(trace['speed_mph'] > 4).idxmax():,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33393: {'edges':        linkid  reverse_link\n",
       "  0  1125635393         False,\n",
       "  'last_matched': 3,\n",
       "  'match_ratio': 0.022222222222222223,\n",
       "  'max_lattice_width': 50,\n",
       "  'trace':      sequence  tripid            datetime        lat        lon  speed_mph  \\\n",
       "  0           0   33393 2016-06-24 17:20:57  33.761314 -84.339436   8.032127   \n",
       "  1           1   33393 2016-06-24 17:21:01  33.761309 -84.339261   9.999330   \n",
       "  2           2   33393 2016-06-24 17:21:06  33.761357 -84.339023   9.530636   \n",
       "  3           3   33393 2016-06-24 17:21:11  33.761350 -84.338771  10.693703   \n",
       "  4           4   33393 2016-06-24 17:21:15  33.761321 -84.338569  10.851931   \n",
       "  ..        ...     ...                 ...        ...        ...        ...   \n",
       "  131       131   33393 2016-06-24 17:31:32  33.749136 -84.328601   0.000000   \n",
       "  132       132   33393 2016-06-24 17:32:17  33.749124 -84.328600   0.000000   \n",
       "  133       133   33393 2016-06-24 17:33:16  33.749074 -84.328608   0.000000   \n",
       "  134       134   33393 2016-06-24 17:34:11  33.749080 -84.328598   0.000000   \n",
       "  135       135   33393 2016-06-24 17:35:13  33.749023 -84.328471   2.036718   \n",
       "  \n",
       "       hAccuracy_ft                         geometry             X  \\\n",
       "  0        39.37008  POINT (2244076.099 1368254.486)  2.244076e+06   \n",
       "  1        32.80840  POINT (2244129.281 1368252.577)  2.244129e+06   \n",
       "  2        32.80840  POINT (2244201.642 1368269.922)  2.244202e+06   \n",
       "  3        39.37008  POINT (2244278.225 1368267.246)  2.244278e+06   \n",
       "  4        39.37008  POINT (2244339.598 1368256.591)  2.244340e+06   \n",
       "  ..            ...                              ...           ...   \n",
       "  131      13.12336  POINT (2247362.054 1363817.947)  2.247362e+06   \n",
       "  132      29.52756  POINT (2247362.351 1363813.580)  2.247362e+06   \n",
       "  133      13.12336  POINT (2247359.891 1363795.391)  2.247360e+06   \n",
       "  134      13.12336  POINT (2247362.934 1363797.569)  2.247363e+06   \n",
       "  135      13.12336  POINT (2247401.504 1363776.768)  2.247402e+06   \n",
       "  \n",
       "                  Y       time_diff  acceleration_ft/s**2      delta_time  \\\n",
       "  0    1.368254e+06 0 days 00:00:02              3.199827 0 days 00:00:02   \n",
       "  1    1.368253e+06 0 days 00:00:01              0.201140 0 days 00:00:01   \n",
       "  2    1.368270e+06 0 days 00:00:02             -0.004619 0 days 00:00:02   \n",
       "  3    1.368267e+06 0 days 00:00:02             -0.226420 0 days 00:00:02   \n",
       "  4    1.368257e+06 0 days 00:00:01              1.009005 0 days 00:00:01   \n",
       "  ..            ...             ...                   ...             ...   \n",
       "  131  1.363818e+06 0 days 00:00:01              0.000000 0 days 00:00:01   \n",
       "  132  1.363814e+06 0 days 00:00:02             -0.500194 0 days 00:00:02   \n",
       "  133  1.363795e+06 0 days 00:00:02             -0.492126 0 days 00:00:02   \n",
       "  134  1.363798e+06 0 days 00:00:02              0.000000 0 days 00:00:02   \n",
       "  135  1.363777e+06 0 days 00:00:02             -0.155594 0 days 00:00:02   \n",
       "  \n",
       "       delta_distance_ft  traversed_distance_ft    time_elapsed  \\\n",
       "  0            17.972271             666.772314 0 days 00:01:48   \n",
       "  1            15.351801             720.506492 0 days 00:01:52   \n",
       "  2            28.636276             794.961147 0 days 00:01:57   \n",
       "  3            29.617263             872.129593 0 days 00:02:02   \n",
       "  4            16.809978             934.484577 0 days 00:02:06   \n",
       "  ..                 ...                    ...             ...   \n",
       "  131           2.705373            9415.370866 0 days 00:12:23   \n",
       "  132           2.458738            9466.817735 0 days 00:13:08   \n",
       "  133           1.633961            9517.706279 0 days 00:14:07   \n",
       "  134           0.788660            9568.069758 0 days 00:15:02   \n",
       "  135          11.531653            9627.468674 0 days 00:16:04   \n",
       "  \n",
       "       calculated_speed_mph                            interpolated_point  \n",
       "  0                6.126911  POINT (2244018.071796967 1368284.7480052551)  \n",
       "  1               10.467137  POINT (2244018.071796967 1368284.7480052551)  \n",
       "  2                9.762367  POINT (2244018.071796967 1368284.7480052551)  \n",
       "  3               10.096794  POINT (2244018.071796967 1368284.7480052551)  \n",
       "  4               11.461348                                           NaN  \n",
       "  ..                    ...                                           ...  \n",
       "  131              1.844573                                           NaN  \n",
       "  132              0.838206                                           NaN  \n",
       "  133              0.557032                                           NaN  \n",
       "  134              0.268861                                           NaN  \n",
       "  135              3.931245                                           NaN  \n",
       "  \n",
       "  [136 rows x 18 columns],\n",
       "  'match_lines':    sequence                                        match_lines      length\n",
       "  0         0  LINESTRING (2244076.099 1368254.486, 2244018.0...   65.444603\n",
       "  1         1  LINESTRING (2244129.281 1368252.577, 2244018.0...  115.769326\n",
       "  2         2  LINESTRING (2244201.642 1368269.922, 2244018.0...  184.168357\n",
       "  3         3  LINESTRING (2244278.225 1368267.246, 2244018.0...  260.740922,\n",
       "  'interpolated_points':    sequence               interpolated_point\n",
       "  0         0  POINT (2244018.072 1368284.748)\n",
       "  1         1  POINT (2244018.072 1368284.748)\n",
       "  2         2  POINT (2244018.072 1368284.748)\n",
       "  3         3  POINT (2244018.072 1368284.748),\n",
       "  'match_time_sec': 0.2,\n",
       "  'gps_distance': 204.25595493853092,\n",
       "  'time': datetime.datetime(2024, 6, 1, 18, 27, 31, 156420),\n",
       "  'settings': {'obs_noise': 50,\n",
       "   'obs_noise_ne': 100,\n",
       "   'max_dist_init': 2000,\n",
       "   'max_dist': 1000,\n",
       "   'min_prob_norm': 0.001,\n",
       "   'non_emitting_states': True,\n",
       "   'non_emitting_length_factor': 0.75,\n",
       "   'max_lattice_width': 50,\n",
       "   'dist_noise': 50,\n",
       "   'dist_noise_ne': 200,\n",
       "   'restrained_ne': False,\n",
       "   'avoid_goingback': True,\n",
       "   'increase_max_lattice_width': False,\n",
       "   'export_graph': False}}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_matches = {}\n",
    "single_matches[random_trip] = map_match.leuven_match(trace,single_matching_settings,map_con,exploded_links)\n",
    "single_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map_match.visualize_match(random_trip, single_matches, df_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Match\n",
    "Takes 3hr25min for 2,765 traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load existing matches/if none then create a new dict\n",
    "# if (export_fp/f'matched_{matching_index}.pkl').exists():\n",
    "#     with (export_fp/f'matched_{matching_index}.pkl').open('rb') as fh:\n",
    "#         match_dict = pickle.load(fh)\n",
    "# else:\n",
    "#     match_dict = {}\n",
    "\n",
    "#     # with (export_fp/f'match_{matching_index}_{len(match_dict.keys())}_trips.pkl').open('wb') as fh:\n",
    "# #     pickle.dump(match_dict,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2765/2765 [2:01:20<00:00,  2.63s/it]  \n"
     ]
    }
   ],
   "source": [
    "for tripid in tqdm(list(coords_dict.keys())):\n",
    "    \n",
    "    check = match_dict.get(tripid,False)\n",
    "\n",
    "    if isinstance(check,bool):\n",
    "        trace = coords_dict[tripid]\n",
    "        match = map_match.leuven_match(trace,matching_settings,map_con,exploded_links)\n",
    "        match_dict[tripid] = match\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe with the ratio of points matched, the total gps distance, the total network distance, and mean match distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1717288132.006298"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('D:/PROJECTS/GDOT/Map_Matching')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1717288197.1580367"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_fp.exists() == False:\n",
    "    export_fp.mkdir(parents=True)\n",
    "\n",
    "# export the matching settings tested\n",
    "with (export_fp/'matching_settings_df.pkl').open('wb') as fh:\n",
    "    pickle.dump(matching_settings_df,fh)\n",
    "\n",
    "# export the matched traces\n",
    "#TODO add date\n",
    "with (export_fp/f'matched_{matching_index}.pkl').open('wb') as fh:\n",
    "    pickle.dump(match_dict,fh)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated past here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Print matching stats\n",
    "# Outputs:\n",
    "# - 'edges', \n",
    "# - 'last_matched', \n",
    "# - 'match_ratio', \n",
    "# - 'max_lattice_width', \n",
    "# - 'trace', \n",
    "# - 'match_lines', \n",
    "# - 'interpolated_points', \n",
    "# - 'match_time_sec', \n",
    "# - 'gps_distance', \n",
    "# - 'time', \n",
    "# - 'settings'\n",
    "# matching_index = 0\n",
    "# with (export_fp/f'matched_{matching_index}.pkl').open('rb') as fh:\n",
    "#     match_dict = pickle.load(fh)    \n",
    "# #how many failed matches\n",
    "# failed = {key:item for key, item in match_dict.items() if isinstance(item,str)}\n",
    "# print(len(failed.keys()),'failed matches')\n",
    "# success = {key:item for key, item in match_dict.items() if isinstance(item,str) == False}\n",
    "# length_dict = {row['linkid']:row['geometry'].length for idx, row in df_edges[['linkid','geometry']].drop_duplicates().iterrows()}\n",
    "# results = [[key,item['match_ratio'],item['gps_distance'],item['edges']['linkid'].map(length_dict).sum(),item['match_lines']['length'].mean()] for key, item in success.items()]\n",
    "# results = pd.DataFrame(results,columns=['tripid','match_ratio','gps_distance','network_distance','mean_match_distance'])\n",
    "# results\n",
    "# print((results['match_ratio']>0.9).sum(),'/',results.shape[0],'trips had a quality match')\n",
    "# trips_df_export = trips_df.reset_index(drop=True).merge(results,on='tripid')\n",
    "# with (export_fp/'matched_trips_df.pkl').open('wb') as fh:\n",
    "#     pickle.dump(trips_df_export,fh)\n",
    "# # Examine matches\n",
    "# import random\n",
    "\n",
    "# def get_random_key(dictionary):\n",
    "#     random_key =  random.choice(list(dictionary.keys()))\n",
    "#     #recursion?\n",
    "#     if isinstance(dictionary.get(random_key),str):\n",
    "#         random_key = get_random_key(dictionary)\n",
    "#     return random_key\n",
    "# results['match_ratio'].hist()\n",
    "# (results['match_ratio']>0.8).sum()\n",
    "# pool = results.loc[(results['match_ratio']>.9) & (results['match_ratio']<1),'tripid'].tolist()\n",
    "# #tripid = get_random_key(match_dict)\n",
    "# tripid = random.choice(pool)\n",
    "# map_match.visualize_match(tripid, match_dict, df_edges)\n",
    "\n",
    "# # Post Match Cleanup (in development)\n",
    "# Some trips have out-and-backing and or take the wrong link if two nodes have more than one link between them. This step goes through and cleans these trips.\n",
    "\n",
    "# For out and backing:\n",
    "# Subset network graph to only the edges between origin and destination and then use Dijkstra's algorithim to return the shortest path. Check the Frechet distance to determine if cleaned match better represents trajectory than previously.\n",
    "# trip_w_out_and_backing = 550\n",
    "# map_match.visualize_match(trip_w_out_and_backing, match_dict, df_edges)\n",
    "\n",
    "# match_dict[801]['edges']\n",
    "# gpd.GeoDataFrame(match_dict[801]['edges'].merge(edges,on='linkid')).explore()\n",
    "# #TODO use to make network graph, then subset with trip\n",
    "# import networkx as nx\n",
    "\n",
    "# MDG = nx.MultiDiGraph()  # Create a MultiDiGraph\n",
    "#     #itertuples used to maintain the type\n",
    "# for idx, row in exploded_edges.iterrows():\n",
    "#     #edge_data = {linkid: row[2],'reverse_link': False, 'azimuth': row[4]}\n",
    "#     MDG.add_edge(int(row['A']), int(row['B']), **{'weight': row['length_ft']})#**edge_data)  # Add edge with linkid attribute\n",
    "#     #add reverse link if oneway is not true\n",
    "#     MDG.add_edge(int(row['B']), int(row['A']), **{'weight': row['length_ft']})\n",
    "#     # if row[3] == False:\n",
    "#     #     edge_data['reverse_link'] = True \n",
    "#     #     #reverse the azimuth\n",
    "#     #     edge_data['azimuth'] = row[5]\n",
    "#     #     MDG.add_edge(row[1], row[0], **edge_data)\n",
    "\n",
    "# #exploded_edges, exploded_nodes\n",
    "# tripid = 801\n",
    "\n",
    "# test = match_dict[801]['edges'].merge(edges,on='linkid')\n",
    "# sub_nodes = test['A'].append(test['B']).unique().tolist()\n",
    "# #get start and end linkid\n",
    "# start = match_dict[tripid]['edges'].iloc[0,:]\n",
    "# end = match_dict[tripid]['edges'].iloc[-1,:]\n",
    "\n",
    "# #get start and end node\n",
    "# start_a_b = edges.loc[edges['linkid']==start['linkid'],['A','B']]\n",
    "# end_a_b = edges.loc[edges['linkid']==end['linkid'],['A','B']]\n",
    "\n",
    "# if start['forward']:\n",
    "#     start = start_a_b['A'].item()\n",
    "# else:\n",
    "#     start = start_a_b['B'].item()\n",
    "\n",
    "# if end['forward']:\n",
    "#     end = end_a_b['B'].item()\n",
    "# else:\n",
    "#     end = end_a_b['A'].item()\n",
    "# sub_nodes[0]\n",
    "# start\n",
    "# start in sub_nodes\n",
    "# end in sub_nodes\n",
    "# path\n",
    "# subgraph = MDG.subgraph(sub_nodes)\n",
    "# length, path = nx.single_source_dijkstra(subgraph,start,end,weight='weight')\n",
    "\n",
    "# #turn to edge list\n",
    "# edge_list = [(path[i],path[i+1]) for i in range(len(path)-1)]\n",
    "# edge_df = pd.DataFrame(edge_list,columns=['A','B'])\n",
    "# forward = pd.merge(edge_df,edges[['A','B','linkid','geometry']],on=['A','B'])#[['linkid','A','B','geometry']]\n",
    "# forward\n",
    "# reverse = pd.merge(edge_df,edges[['A','B','linkid','geometry']],left_on=['B','A'],right_on=['A','B'])[['linkid','A','B','geometry']]\n",
    "# shortest_path = pd.concat([forward,reverse],ignore_index=True)\n",
    "# shortest_path = shortest_path.loc[shortest_path.groupby(['A','B'])['length_ft'].idxmin()]\n",
    "# gpd.GeoDataFrame(shortest_path).explore()\n",
    "# #TODO deal with duplicate links\n",
    "# shortest_path.explore()\n",
    "# For multi-edges, buffer the 2+ edges and take the one that hits the most gps points\n",
    "# import numpy as np\n",
    "# exploded_edges['A_sort'] = np.sort(exploded_edges[['A','B']].to_numpy())[:,0]\n",
    "# exploded_edges['B_sort'] = np.sort(exploded_edges[['A','B']].to_numpy())[:,1]\n",
    "# duplicate_edges = exploded_edges.loc[exploded_edges[['A_sort','B_sort']].duplicated(keep=False),'linkid'].unique()\n",
    "# gps_points = match_dict[tripid]['trace']\n",
    "# # matched_trip = match_dict[tripid]['edges'].merge(edges, on='linkid')\n",
    "# # matched_trip = gpd.GeoDataFrame(matched_trip)\n",
    "# # from shapely.ops import MultiLineString\n",
    "# # buffered_geo = MultiLineString(matched_trip.geometry.tolist()).buffer(100)\n",
    "# # match['trace'].intersects(buffered_geo).sum()\n",
    "# # # export \n",
    "# # with (export_fp/'sample_matched.pkl').open('wb') as fh:\n",
    "# #     pickle.dump(match_dict,fh)\n",
    "# # with (export_fp/'sample_matched.pkl').open('rb') as fh:\n",
    "# #     match_dict = pickle.load(fh)\n",
    "\n",
    "# # Visualization\n",
    "\n",
    "# import folium\n",
    "# import geopandas as gpd\n",
    "# from folium.plugins import MarkerCluster, PolyLineTextPath\n",
    "# from folium.map import FeatureGroup\n",
    "\n",
    "# #tripid = 29837#7257#9806#30000#8429\n",
    "\n",
    "# # Your GeoDataFrames\n",
    "# matched_trip = match_dict[tripid]['edges'].merge(edges, on='linkid')\n",
    "# matched_trip = gpd.GeoDataFrame(matched_trip)\n",
    "# gps_points = match_dict[tripid]['trace']\n",
    "# match_lines = match_dict[tripid]['match_lines']\n",
    "\n",
    "# #get the start and end point for mapping\n",
    "# start_pt = gps_points.to_crs(epsg='4326').loc[gps_points['sequence'].idxmin(),'geometry']\n",
    "# end_pt = gps_points.to_crs(epsg='4326').loc[gps_points['sequence'].idxmax(),'geometry']\n",
    "\n",
    "# # reproject and get the center of the map\n",
    "# x_mean = gps_points.to_crs(epsg='4326')['geometry'].x.mean()\n",
    "# y_mean = gps_points.to_crs(epsg='4326')['geometry'].y.mean()\n",
    "\n",
    "# # Create a Folium map centered around the mean of the GPS points\n",
    "# center = [y_mean,x_mean]\n",
    "# mymap = folium.Map(location=center, zoom_start=14)\n",
    "\n",
    "# # Convert GeoDataFrames to GeoJSON\n",
    "# matched_trip_geojson = matched_trip[['linkid','geometry']].to_crs(epsg='4326').to_json()\n",
    "# gps_points_geojson = gps_points[['sequence','geometry']].to_crs(epsg='4326').to_json()\n",
    "# match_lines_geojson = match_lines[['sequence','match_lines']].to_crs(epsg='4326').to_json()\n",
    "\n",
    "# # Create FeatureGroups for each GeoDataFrame\n",
    "# matched_trip_fg = FeatureGroup(name='Matched Trip')\n",
    "# gps_points_fg = FeatureGroup(name='GPS Points')\n",
    "# match_lines_fg = FeatureGroup(name='Match Lines')\n",
    "\n",
    "# # Add GeoJSON data to FeatureGroups\n",
    "# folium.GeoJson(matched_trip_geojson, name='Matched Trip', style_function=lambda x: {'color': 'red'}).add_to(matched_trip_fg)\n",
    "\n",
    "# # Add circles to the GPS Points FeatureGroup\n",
    "# for idx, row in gps_points.iterrows():\n",
    "#     folium.Circle(location=[row['lat'], row['lon']], radius=5, color='grey', fill=True, fill_color='grey').add_to(gps_points_fg)\n",
    "\n",
    "# # Add GeoJSON data to Match Lines FeatureGroup with transparent and grey style\n",
    "# folium.GeoJson(match_lines_geojson, name='Match Lines', style_function=lambda x: {'color': 'grey', 'opacity': 0.5}).add_to(match_lines_fg)\n",
    "\n",
    "# # Add FeatureGroups to the map\n",
    "# matched_trip_fg.add_to(mymap)\n",
    "# gps_points_fg.add_to(mymap)\n",
    "# match_lines_fg.add_to(mymap)\n",
    "\n",
    "# # Add start and end points with play and stop buttons\n",
    "# start_icon = folium.Icon(color='green',icon='play',prefix='fa')\n",
    "# end_icon = folium.Icon(color='red',icon='stop',prefix='fa')\n",
    "# folium.Marker(location=[start_pt.y, start_pt.x],icon=start_icon).add_to(mymap)\n",
    "# folium.Marker(location=[end_pt.y, end_pt.x],icon=end_icon).add_to(mymap)\n",
    "\n",
    "# # Add layer control to toggle layers on/off\n",
    "# folium.LayerControl().add_to(mymap)\n",
    "\n",
    "# # Add legend with statistics\n",
    "# #TODO what happened to duration\n",
    "# legend_html = f'''\n",
    "#     <div style=\"position: fixed; \n",
    "#             bottom: 5px; left: 5px; width: 300px; height: 250px; \n",
    "#             border:2px solid grey; z-index:9999; font-size:14px;\n",
    "#             background-color: white;\n",
    "#             opacity: 0.9;\">\n",
    "#     &nbsp; <b>Trip ID: {tripid} </b> <br>\n",
    "#     &nbsp; <b> Match Date: {match_dict[tripid]['time']} </b> <br>\n",
    "#     &nbsp; Start Point &nbsp; <i class=\"fa fa-play\" style=\"color:green\"></i>,\n",
    "#     End Point &nbsp; <i class=\"fa fa-stop\" style=\"color:red\"></i> <br>\n",
    "    \n",
    "#     &nbsp; Matched Path &nbsp; <div style=\"width: 20px; height: 5px; background-color: red; display: inline-block;\"></div> <br>\n",
    "#     &nbsp; Match Lines Path &nbsp; <div style=\"width: 20px; height: 5px; background-color: gray; display: inline-block;\"></div> <br>\n",
    " \n",
    "#     &nbsp; Points Matched: {match_dict[tripid]['last_matched']}/{match_dict[tripid]['trace'].shape[0]} <br>\n",
    "#     &nbsp; Match Ratio: {match_dict[tripid]['match_ratio']:.2f} <br>\n",
    "#     &nbsp; GPS Distance: {match_dict[tripid]['gps_distance']:.1f} ft. <br>\n",
    "#     &nbsp; Matched Trace Distance: {matched_trip.length.sum():.0f} ft. <br>\n",
    "#     &nbsp; Mean Matching Distance: {match_dict[tripid]['match_lines'].length.mean():.0f} ft. \n",
    "\n",
    "#     </div>\n",
    "#     '''\n",
    "# mymap.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# # Save the map to an HTML file or display it in a Jupyter notebook\n",
    "# #mymap.save('map.html')\n",
    "# # mymap.save('/path/to/save/map.html')  # Use an absolute path if needed\n",
    "# mymap  # Uncomment if you are using Jupyter notebook\n",
    "\n",
    "# #TODO add in the legend with trip info and then we're golden\n",
    "\n",
    "# match_dict[tripid].keys()\n",
    "# match_dict[tripid]['match_ratio']\n",
    "# match_dict[tripid].keys()\n",
    "# help(InMemMap)\n",
    "# help(DistanceMatcher)\n",
    "# :param map_con: Map object to connect to map database\n",
    "#         :param obs_noise: Standard deviation of noise\n",
    "#         :param obs_noise_ne: Standard deviation of noise for non-emitting states (is set to obs_noise if not given)\n",
    "#         :param max_dist_init: Maximum distance from start location (if not given, uses max_dist)\n",
    "#         :param max_dist: Maximum distance from path (this is a hard cut, min_prob_norm should be better)\n",
    "#         :param min_prob_norm: Minimum normalized probability of observations (ema)\n",
    "#         :param non_emitting_states: Allow non-emitting states. A non-emitting state is a state that is\n",
    "#             not associated with an observation. Here we assume it can be associated with a location in between\n",
    "#             two observations to allow for pruning. It is advised to set min_prob_norm and/or max_dist to avoid\n",
    "#             visiting all possible nodes in the graph.\n",
    "#         :param non_emitting_length_factor: Reduce the probability of a sequence of non-emitting states the longer it\n",
    "#             is. This can be used to prefer shorter paths. This is separate from the transition probabilities because\n",
    "#             transition probabilities are averaged for non-emitting states and thus the length is also averaged out.\n",
    "#         :param max_lattice_width: Restrict the lattice (or possible candidate states per observation) to this value.\n",
    "#             If there are more possible next states, the states with the best likelihood so far are selected.\n",
    "\n",
    "#         :param dist_noise: Standard deviation of difference between distance between states and distance\n",
    "#             between observatoins. If not given, set to obs_noise\n",
    "#         :param dist_noise_ne: If not given, set to dist_noise\n",
    "#         :param restrained_ne: Avoid non-emitting states if the distance between states and between\n",
    "#             observations is close to each other.\n",
    "#         :param avoid_goingback: If true, the probability is lowered for a transition that returns back to a\n",
    "#             previous edges or returns to a position on an edge.\n",
    "\n",
    "# # #get list of coords\n",
    "# # gps_trace = list(zip(trace.geometry.y,trace.geometry.x))\n",
    "\n",
    "# # #perform matching\n",
    "# # states, last_matched = matcher.match(gps_trace)\n",
    "# # only_nodes = matcher.path_pred_onlynodes\n",
    "\n",
    "# # print(\"States\\n------\")\n",
    "# # print(states)\n",
    "# # print(\"Nodes\\n------\")\n",
    "# # print(only_nodes)\n",
    "# # print(\"\")\n",
    "# # matcher.print_lattice_stats()\n",
    "# # fig, ax = plt.subplots(1, 1)\n",
    "# # mmviz.plot_map(map_con, matcher=matcher,\n",
    "# #                ax=ax,\n",
    "# #                show_labels=True, show_matching=True, show_graph=False,\n",
    "# #                filename=\"my_plot.png\")\n",
    "# # test = matcher.lattice[4]\n",
    "# # m = max(test.values_all(), key=lambda m: m.logprob) # for the 4th point get the one with the highest logprob\n",
    "\n",
    "# # m.logprob\n",
    "# # import numpy as np\n",
    "# # t = {x.cname.split('_')[0] + '_' + x.cname.split('_')[1]: x.logprob for x in test.values_all()}\n",
    "# # check = pd.DataFrame.from_dict(t,orient='index',columns=['logprob']).sort_values('logprob',ascending=False)\n",
    "# # check\n",
    "# # (check.index == '5424132517_7151205661').sum()\n",
    "# # testing = trace.copy()\n",
    "# # testing.geometry = testing.buffer(1000)\n",
    "# # intersect = gpd.overlay(edges,testing)\n",
    "# # intersect[(intersect['A_B'] == '5424132517_7151205661') & (intersect['sequence'] == 4)]\n",
    "\n",
    "# # #reduce the states size with match_nodes\n",
    "# # reduced_states = list(set(edges))\n",
    "\n",
    "# # #calculate the match ratio\n",
    "# # match_ratio = last_matched / (len(gps_trace)-1)\n",
    "    \n",
    "# # #retreive matched edges from network\n",
    "# # geos_list = [geos_dict.get(id,0) for id in reduced_states]\n",
    "\n",
    "# # #turn into geodataframe\n",
    "# # matched_trip = gpd.GeoDataFrame(data={'A_B':reduced_states,'geometry':geos_list},geometry='geometry',crs='epsg:2240')\n",
    "\n",
    "# # #turn tuple to str\n",
    "# # matched_trip['A_B'] = matched_trip['A_B'].apply(lambda row: f'{row[0]}_{row[1]}')\n",
    "\n",
    "# # #reset index to add an edge sequence column\n",
    "# # matched_trip.reset_index().rename(columns={'index':'edge_sequence'},inplace=True)\n",
    "\n",
    "# # trace['interpolated_point'] = pd.Series([ Point(x.edge_m.pi) for x in matcher.lattice_best ])\n",
    "# # trace = trace.loc[0:last_matched]\n",
    "# # trace['match_lines'] = trace.apply(lambda row: LineString([row['geometry'],row['interpolated_point']]),axis=1)\n",
    "\n",
    "# # interpolated_points = trace[['sequence','interpolated_point']]\n",
    "# # interpolated_points = gpd.GeoDataFrame(interpolated_points,geometry='interpolated_point')\n",
    "\n",
    "# # match_lines = trace[['sequence','match_lines']]\n",
    "# # match_lines = gpd.GeoDataFrame(match_lines,geometry='match_lines')\n",
    "# # match_lines['length'] = match_lines.length\n",
    "\n",
    "\n",
    "# # interpolated_points.to_file(project_dir/f\"single_example/{tripid}.gpkg\",layer='interpolated_points')\n",
    "# # match_lines.to_file(project_dir/f\"single_example/{tripid}.gpkg\",layer='match_lines')\n",
    "\n",
    "# # #%%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
