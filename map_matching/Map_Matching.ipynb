{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO (in order of priority):\n",
    "1. Make sure that the cleanup code works (come back to this)\n",
    "    1. Want to get rid of the really small out and backs\n",
    "    1. Have method for doing this, but worried that it will remove valid out and backing\n",
    "    1. Frechet distance could be a way to tell if the cleaning removes too much out and backing? Or some sort of overlap metric?\n",
    "1. Use geo_dict to assemble edge for calculated frechet distance\n",
    "    1. Frechet distance will give a decent measure of how close the match is to the trace\n",
    "    1. Will need to reverse link geometry if passing the other way otherwise it won't work\n",
    "1. Match all the traces and export for impedance calibration\n",
    "    1. The process flow into this step is still uncertain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from leuvenmapmatching.matcher.distance import DistanceMatcher\n",
    "from leuvenmapmatching.map.inmem import InMemMap\n",
    "#from leuvenmapmatching import visualization as mmviz\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from shapely.ops import Point, LineString\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import map_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = json.load((Path.cwd().parent / 'config.json').open('rb'))\n",
    "export_fp = Path(config['project_directory'])\n",
    "if export_fp.exists() == False:\n",
    "    export_fp.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file paths\n",
    "#network_fp = Path(config['project_directory']) / 'Map_Matching/matching.gpkg'\n",
    "network_fp = Path(config['project_directory']) / 'Network' \n",
    "traces_fp = Path(config['project_directory']) / 'CycleAtlanta'\n",
    "export_fp = Path(config['project_directory']) / 'Map_Matching'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Network\n",
    "Bring in the pre-processed network and filter as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Matching\n",
    "Sometimes matching can be improved just by limiting what can be matched to. For this, we want to start by mapping to the most restrictive network. Then we add more links if a match hasn't been found.\n",
    "\n",
    "1. Network without parking lot roads\n",
    "1. Network without oneway restrictions\n",
    "1. Full network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (network_fp / 'nodes.pkl').open('rb') as fh:\n",
    "    nodes = pickle.load(fh)\n",
    "# with (network_fp / 'directed_edges.pkl').open('rb') as fh:\n",
    "#     df_edges = pickle.load(fh)\n",
    "directed_links = pd.read_parquet(network_fp/'directed_edges.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges = df_edges.merge(edges[['linkid','geometry']])\n",
    "df_edges = gpd.GeoDataFrame(df_edges,geometry='geometry',crs=edges.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter network and remove isolates\n",
    "\n",
    "# #filter using link type\n",
    "# link_types = ['bike','road','pedestrian','service']\n",
    "# filtered = df_edges[df_edges['link_type'].isin(link_types)]\n",
    "# #create multigraph\n",
    "# import networkx as nx\n",
    "# MDG = nx.Graph()\n",
    "\n",
    "# #remove isolates\n",
    "# MDG.add_edges_from(list(zip(filtered['source'],filtered['target'])))\n",
    "# largest_cc = max(nx.connected_components(MDG), key=len)\n",
    "\n",
    "#subset \n",
    "\n",
    "#turn to multigraph, remove isolates turn back into link dataframe\n",
    "#also remove nodes\n",
    "# in_largest_cc = filtered['source'].isin(largest_cc) | filtered['target'].isin(largest_cc)\n",
    "# filtered['isolate'] = False\n",
    "# filtered.loc[~in_largest_cc,'isolate'] = True\n",
    "# matching_network_links = filtered.loc[filtered['isolate']==False]\n",
    "\n",
    "# node_filt = set(matching_network_links['source'].append(matching_network_links['target']).tolist())\n",
    "# matching_network_nodes = nodes.loc[nodes['N'].isin(node_filt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import network\n",
    "# edges = gpd.read_file(network_fp/'final_network.gpkg',layer=\"final_network\")\n",
    "# #edges.reset_index(inplace=True)\n",
    "# #edges.rename(columns={'index':'linkid'},inplace=True)\n",
    "\n",
    "#TODO next features\n",
    "# need to add the ability to remove isolates when doing this so we don't have matches to links that don't go anywhere\n",
    "# allow_wrongway = \n",
    "# link_types_to_include = ['road','bike','pedestrian]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking Apart Multi-Edges\n",
    "Occasionally, there will be multiple edges between two nodes. A common place for this to occur are local roads connected to main roads that form u-shapes. Leuven map mapmatching cannot handle multi-edges as the only information stored in the network graph are the nodes. Usually, it's obvious which edge should be retained, but an easy way to still include both edges for map matching is to break these edges by thier centroid into smaller edges with new nodes. The below cell does this.\n",
    "\n",
    "The centroid nodes created in this step are given a unique id. The map matching function removes these new nodes so that they don't appear in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO condense this into a funciton\n",
    "\n",
    "#get max ids for adding the new midpoint nodes\n",
    "max_nodeid = nodes['N'].max()\n",
    "max_linkid = df_edges['linkid'].max()\n",
    "\n",
    "# identify multi-edges so that we can break them apart (we already have duplicate edges)\n",
    "df_sorted = df_edges.sort_values(by=['source','target'])\n",
    "grouped_df = df_sorted.groupby(['source','target'])['linkid'].nunique().reset_index(name='num_linkid')\n",
    "grouped_df = grouped_df[grouped_df['num_linkid']>1]\n",
    "merged = pd.merge(df_sorted,grouped_df,on=['source','target'])\n",
    "multi_edges = df_edges[df_edges['linkid'].isin(set(merged['linkid'].tolist()))]\n",
    "\n",
    "#remove these multi-edges from the dataframe\n",
    "non_multi_edges = df_edges[~df_edges['linkid'].isin(set(merged['linkid'].tolist()))]\n",
    "new_links, new_nodes = map_match.explode_network_midpoint(multi_edges,max_nodeid,max_linkid)\n",
    "exploded_nodes = pd.concat([nodes,new_nodes],ignore_index=True)\n",
    "exploded_links = pd.concat([non_multi_edges,new_links],ignore_index=True)\n",
    "\n",
    "#create map matching graph network\n",
    "map_con = map_match.make_network(exploded_links,exploded_nodes,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_nodes.set_crs(config['projected_crs_epsg'],inplace=True)\n",
    "exploded_links.set_crs(config['projected_crs_epsg'],inplace=True)\n",
    "\n",
    "#optional inspect exploded network\n",
    "#exploded_nodes.to_file(network_fp/'matching_network.gpkg',layer='exploded_nodes')\n",
    "#exploded_links.to_file(network_fp/'matching_network.gpkg',layer='exploded_links')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace Data\n",
    "For map matching, we're using GPS traces that have been processed so that each point is spaced a certain distance apart, coordinates in between this distance are dropped to improve computation time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all traces\n",
    "with (traces_fp/'reduced_spacing.pkl').open('rb') as fh:\n",
    "    coords_dict = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (traces_fp/'trips_3.pkl').open('rb') as fh:\n",
    "    trips_df = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_dict = {key:item for key, item in coords_dict.items() if key in trips_df['tripid'].tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import list of trips to include\n",
    "len(coords_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matching setting dictionary stores all of the settings used for map matching, so they can be retrieved later for study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (export_fp / 'matching_settings_df.pkl').exists():\n",
    "    with (export_fp / 'matching_settings_df.pkl').open('rb') as fh:\n",
    "        matching_settings_df = pickle.load(fh)\n",
    "else:\n",
    "    matching_settings_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Leueven Documentation](https://github.com/wannesm/LeuvenMapMatching/blob/9ca9f0b73665252f2ee492fae9dd243feef2f39d/leuvenmapmatching/matcher/distance.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach matching index to the match dict instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(map_match)\n",
    "\n",
    "matching_settings = {\n",
    "    'obs_noise': 50, #Standard deviation of noise\n",
    "    'obs_noise_ne': 100, #Standard deviation of noise for non-emitting states (is set to obs_noise if not given)\n",
    "    'max_dist_init': 2000, #Maximum distance from start location (if not given, uses max_dist)\n",
    "    'max_dist': 1000, #Maximum distance from path (this is a hard cut, min_prob_norm should be better)\n",
    "    'min_prob_norm': 0.005, #Minimum normalized probability of observations (ema)\n",
    "    'non_emitting_states': False, #Allow non-emitting states\n",
    "    'non_emitting_length_factor': 0.75, #Reduce the probability of a sequence of non-emitting states the longer it is.\n",
    "    'max_lattice_width': 55, #Restrict the lattice (or possible candidate states per observation) to this value.\n",
    "    'dist_noise': 50, #Standard deviation of difference between distance between states and distance between observations.\n",
    "    'dist_noise_ne': 200, #for no emitting If not given, set to dist_noise\n",
    "    'restrained_ne': True, #Avoid non-emitting states if the distance between states and between observations is close to each other.\n",
    "    'avoid_goingback': True, #If true, the probability is lowered for a transition that returns back to a previous edges or returns to a position on an edge.\n",
    "    'increase_max_lattice_width': False,\n",
    "    'export_graph': False\n",
    "}\n",
    "\n",
    "#add to matching_settings_tuple if contents are unique\n",
    "row = pd.DataFrame([matching_settings])\n",
    "matching_settings_df = pd.concat([matching_settings_df,row],ignore_index=True)\n",
    "if matching_settings_df.duplicated().any():\n",
    "    print('Settings have been used before')\n",
    "matching_settings_df.drop_duplicates(inplace=True)\n",
    "\n",
    "#check if there are existing matches, using these settings\n",
    "\n",
    "\n",
    "#use this in the qaqc section to line up the ratings with the settings used\n",
    "matching_index = matching_settings_df[(matching_settings_df == tuple(row.loc[0,:])).all(axis=1)].index.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Match Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_matching_settings = {\n",
    "    'obs_noise': 50, #Standard deviation of noise\n",
    "    'obs_noise_ne': 100, #Standard deviation of noise for non-emitting states (is set to obs_noise if not given)\n",
    "    'max_dist_init': 2000, #Maximum distance from start location (if not given, uses max_dist)\n",
    "    'max_dist': 1000, #Maximum distance from path (this is a hard cut, min_prob_norm should be better)\n",
    "    'min_prob_norm': 0.001, #Minimum normalized probability of observations (ema)\n",
    "    'non_emitting_states': True, #Allow non-emitting states\n",
    "    'non_emitting_length_factor': 0.75, #Reduce the probability of a sequence of non-emitting states the longer it is.\n",
    "    'max_lattice_width': 50, #Restrict the lattice (or possible candidate states per observation) to this value.\n",
    "    'dist_noise': 50, #Standard deviation of difference between distance between states and distance between observations.\n",
    "    'dist_noise_ne': 200, #for no emitting If not given, set to dist_noise\n",
    "    'restrained_ne': False, #Avoid non-emitting states if the distance between states and between observations is close to each other.\n",
    "    'avoid_goingback': True, #If true, the probability is lowered for a transition that returns back to a previous edges or returns to a position on an edge.\n",
    "    'increase_max_lattice_width': False,\n",
    "    'export_graph': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- 33393 has issues with the starting match since it starts from the train station, only a really low min_prob_norm helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_trip = random.choice(list(coords_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_trip = 33393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = coords_dict[random_trip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try removing points until a speed above 4mph is detected\n",
    "trace = trace.loc[(trace['speed_mph'] > 4).idxmax():,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_matches = {}\n",
    "single_matches[random_trip] = map_match.leuven_match(trace,single_matching_settings,map_con,exploded_links)\n",
    "single_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map_match.visualize_match(random_trip, single_matches, df_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Match\n",
    "Takes 3hr25min for 2,765 traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load existing matches/if none then create a new dict\n",
    "# if (export_fp/f'matched_{matching_index}.pkl').exists():\n",
    "#     with (export_fp/f'matched_{matching_index}.pkl').open('rb') as fh:\n",
    "#         match_dict = pickle.load(fh)\n",
    "# else:\n",
    "#     match_dict = {}\n",
    "\n",
    "#     # with (export_fp/f'match_{matching_index}_{len(match_dict.keys())}_trips.pkl').open('wb') as fh:\n",
    "# #     pickle.dump(match_dict,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tripid in tqdm(list(coords_dict.keys())):\n",
    "    \n",
    "    check = match_dict.get(tripid,False)\n",
    "\n",
    "    if isinstance(check,bool):\n",
    "        trace = coords_dict[tripid]\n",
    "        match = map_match.leuven_match(trace,matching_settings,map_con,exploded_links)\n",
    "        match_dict[tripid] = match\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe with the ratio of points matched, the total gps distance, the total network distance, and mean match distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_fp.exists() == False:\n",
    "    export_fp.mkdir(parents=True)\n",
    "\n",
    "# export the matching settings tested\n",
    "with (export_fp/'matching_settings_df.pkl').open('wb') as fh:\n",
    "    pickle.dump(matching_settings_df,fh)\n",
    "\n",
    "# export the matched traces\n",
    "with (export_fp/f'matched_{matching_index}_{time.time}.pkl').open('wb') as fh:\n",
    "    pickle.dump(match_dict,fh)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated past here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Print matching stats\n",
    "# Outputs:\n",
    "# - 'edges', \n",
    "# - 'last_matched', \n",
    "# - 'match_ratio', \n",
    "# - 'max_lattice_width', \n",
    "# - 'trace', \n",
    "# - 'match_lines', \n",
    "# - 'interpolated_points', \n",
    "# - 'match_time_sec', \n",
    "# - 'gps_distance', \n",
    "# - 'time', \n",
    "# - 'settings'\n",
    "# matching_index = 0\n",
    "# with (export_fp/f'matched_{matching_index}.pkl').open('rb') as fh:\n",
    "#     match_dict = pickle.load(fh)    \n",
    "# #how many failed matches\n",
    "# failed = {key:item for key, item in match_dict.items() if isinstance(item,str)}\n",
    "# print(len(failed.keys()),'failed matches')\n",
    "# success = {key:item for key, item in match_dict.items() if isinstance(item,str) == False}\n",
    "# length_dict = {row['linkid']:row['geometry'].length for idx, row in df_edges[['linkid','geometry']].drop_duplicates().iterrows()}\n",
    "# results = [[key,item['match_ratio'],item['gps_distance'],item['edges']['linkid'].map(length_dict).sum(),item['match_lines']['length'].mean()] for key, item in success.items()]\n",
    "# results = pd.DataFrame(results,columns=['tripid','match_ratio','gps_distance','network_distance','mean_match_distance'])\n",
    "# results\n",
    "# print((results['match_ratio']>0.9).sum(),'/',results.shape[0],'trips had a quality match')\n",
    "# trips_df_export = trips_df.reset_index(drop=True).merge(results,on='tripid')\n",
    "# with (export_fp/'matched_trips_df.pkl').open('wb') as fh:\n",
    "#     pickle.dump(trips_df_export,fh)\n",
    "# # Examine matches\n",
    "# import random\n",
    "\n",
    "# def get_random_key(dictionary):\n",
    "#     random_key =  random.choice(list(dictionary.keys()))\n",
    "#     #recursion?\n",
    "#     if isinstance(dictionary.get(random_key),str):\n",
    "#         random_key = get_random_key(dictionary)\n",
    "#     return random_key\n",
    "# results['match_ratio'].hist()\n",
    "# (results['match_ratio']>0.8).sum()\n",
    "# pool = results.loc[(results['match_ratio']>.9) & (results['match_ratio']<1),'tripid'].tolist()\n",
    "# #tripid = get_random_key(match_dict)\n",
    "# tripid = random.choice(pool)\n",
    "# map_match.visualize_match(tripid, match_dict, df_edges)\n",
    "\n",
    "# # Post Match Cleanup (in development)\n",
    "# Some trips have out-and-backing and or take the wrong link if two nodes have more than one link between them. This step goes through and cleans these trips.\n",
    "\n",
    "# For out and backing:\n",
    "# Subset network graph to only the edges between origin and destination and then use Dijkstra's algorithim to return the shortest path. Check the Frechet distance to determine if cleaned match better represents trajectory than previously.\n",
    "# trip_w_out_and_backing = 550\n",
    "# map_match.visualize_match(trip_w_out_and_backing, match_dict, df_edges)\n",
    "\n",
    "# match_dict[801]['edges']\n",
    "# gpd.GeoDataFrame(match_dict[801]['edges'].merge(edges,on='linkid')).explore()\n",
    "# #TODO use to make network graph, then subset with trip\n",
    "# import networkx as nx\n",
    "\n",
    "# MDG = nx.MultiDiGraph()  # Create a MultiDiGraph\n",
    "#     #itertuples used to maintain the type\n",
    "# for idx, row in exploded_edges.iterrows():\n",
    "#     #edge_data = {linkid: row[2],'reverse_link': False, 'azimuth': row[4]}\n",
    "#     MDG.add_edge(int(row['A']), int(row['B']), **{'weight': row['length_ft']})#**edge_data)  # Add edge with linkid attribute\n",
    "#     #add reverse link if oneway is not true\n",
    "#     MDG.add_edge(int(row['B']), int(row['A']), **{'weight': row['length_ft']})\n",
    "#     # if row[3] == False:\n",
    "#     #     edge_data['reverse_link'] = True \n",
    "#     #     #reverse the azimuth\n",
    "#     #     edge_data['azimuth'] = row[5]\n",
    "#     #     MDG.add_edge(row[1], row[0], **edge_data)\n",
    "\n",
    "# #exploded_edges, exploded_nodes\n",
    "# tripid = 801\n",
    "\n",
    "# test = match_dict[801]['edges'].merge(edges,on='linkid')\n",
    "# sub_nodes = test['A'].append(test['B']).unique().tolist()\n",
    "# #get start and end linkid\n",
    "# start = match_dict[tripid]['edges'].iloc[0,:]\n",
    "# end = match_dict[tripid]['edges'].iloc[-1,:]\n",
    "\n",
    "# #get start and end node\n",
    "# start_a_b = edges.loc[edges['linkid']==start['linkid'],['A','B']]\n",
    "# end_a_b = edges.loc[edges['linkid']==end['linkid'],['A','B']]\n",
    "\n",
    "# if start['forward']:\n",
    "#     start = start_a_b['A'].item()\n",
    "# else:\n",
    "#     start = start_a_b['B'].item()\n",
    "\n",
    "# if end['forward']:\n",
    "#     end = end_a_b['B'].item()\n",
    "# else:\n",
    "#     end = end_a_b['A'].item()\n",
    "# sub_nodes[0]\n",
    "# start\n",
    "# start in sub_nodes\n",
    "# end in sub_nodes\n",
    "# path\n",
    "# subgraph = MDG.subgraph(sub_nodes)\n",
    "# length, path = nx.single_source_dijkstra(subgraph,start,end,weight='weight')\n",
    "\n",
    "# #turn to edge list\n",
    "# edge_list = [(path[i],path[i+1]) for i in range(len(path)-1)]\n",
    "# edge_df = pd.DataFrame(edge_list,columns=['A','B'])\n",
    "# forward = pd.merge(edge_df,edges[['A','B','linkid','geometry']],on=['A','B'])#[['linkid','A','B','geometry']]\n",
    "# forward\n",
    "# reverse = pd.merge(edge_df,edges[['A','B','linkid','geometry']],left_on=['B','A'],right_on=['A','B'])[['linkid','A','B','geometry']]\n",
    "# shortest_path = pd.concat([forward,reverse],ignore_index=True)\n",
    "# shortest_path = shortest_path.loc[shortest_path.groupby(['A','B'])['length_ft'].idxmin()]\n",
    "# gpd.GeoDataFrame(shortest_path).explore()\n",
    "# #TODO deal with duplicate links\n",
    "# shortest_path.explore()\n",
    "# For multi-edges, buffer the 2+ edges and take the one that hits the most gps points\n",
    "# import numpy as np\n",
    "# exploded_edges['A_sort'] = np.sort(exploded_edges[['A','B']].to_numpy())[:,0]\n",
    "# exploded_edges['B_sort'] = np.sort(exploded_edges[['A','B']].to_numpy())[:,1]\n",
    "# duplicate_edges = exploded_edges.loc[exploded_edges[['A_sort','B_sort']].duplicated(keep=False),'linkid'].unique()\n",
    "# gps_points = match_dict[tripid]['trace']\n",
    "# # matched_trip = match_dict[tripid]['edges'].merge(edges, on='linkid')\n",
    "# # matched_trip = gpd.GeoDataFrame(matched_trip)\n",
    "# # from shapely.ops import MultiLineString\n",
    "# # buffered_geo = MultiLineString(matched_trip.geometry.tolist()).buffer(100)\n",
    "# # match['trace'].intersects(buffered_geo).sum()\n",
    "# # # export \n",
    "# # with (export_fp/'sample_matched.pkl').open('wb') as fh:\n",
    "# #     pickle.dump(match_dict,fh)\n",
    "# # with (export_fp/'sample_matched.pkl').open('rb') as fh:\n",
    "# #     match_dict = pickle.load(fh)\n",
    "\n",
    "# # Visualization\n",
    "\n",
    "# import folium\n",
    "# import geopandas as gpd\n",
    "# from folium.plugins import MarkerCluster, PolyLineTextPath\n",
    "# from folium.map import FeatureGroup\n",
    "\n",
    "# #tripid = 29837#7257#9806#30000#8429\n",
    "\n",
    "# # Your GeoDataFrames\n",
    "# matched_trip = match_dict[tripid]['edges'].merge(edges, on='linkid')\n",
    "# matched_trip = gpd.GeoDataFrame(matched_trip)\n",
    "# gps_points = match_dict[tripid]['trace']\n",
    "# match_lines = match_dict[tripid]['match_lines']\n",
    "\n",
    "# #get the start and end point for mapping\n",
    "# start_pt = gps_points.to_crs(epsg='4326').loc[gps_points['sequence'].idxmin(),'geometry']\n",
    "# end_pt = gps_points.to_crs(epsg='4326').loc[gps_points['sequence'].idxmax(),'geometry']\n",
    "\n",
    "# # reproject and get the center of the map\n",
    "# x_mean = gps_points.to_crs(epsg='4326')['geometry'].x.mean()\n",
    "# y_mean = gps_points.to_crs(epsg='4326')['geometry'].y.mean()\n",
    "\n",
    "# # Create a Folium map centered around the mean of the GPS points\n",
    "# center = [y_mean,x_mean]\n",
    "# mymap = folium.Map(location=center, zoom_start=14)\n",
    "\n",
    "# # Convert GeoDataFrames to GeoJSON\n",
    "# matched_trip_geojson = matched_trip[['linkid','geometry']].to_crs(epsg='4326').to_json()\n",
    "# gps_points_geojson = gps_points[['sequence','geometry']].to_crs(epsg='4326').to_json()\n",
    "# match_lines_geojson = match_lines[['sequence','match_lines']].to_crs(epsg='4326').to_json()\n",
    "\n",
    "# # Create FeatureGroups for each GeoDataFrame\n",
    "# matched_trip_fg = FeatureGroup(name='Matched Trip')\n",
    "# gps_points_fg = FeatureGroup(name='GPS Points')\n",
    "# match_lines_fg = FeatureGroup(name='Match Lines')\n",
    "\n",
    "# # Add GeoJSON data to FeatureGroups\n",
    "# folium.GeoJson(matched_trip_geojson, name='Matched Trip', style_function=lambda x: {'color': 'red'}).add_to(matched_trip_fg)\n",
    "\n",
    "# # Add circles to the GPS Points FeatureGroup\n",
    "# for idx, row in gps_points.iterrows():\n",
    "#     folium.Circle(location=[row['lat'], row['lon']], radius=5, color='grey', fill=True, fill_color='grey').add_to(gps_points_fg)\n",
    "\n",
    "# # Add GeoJSON data to Match Lines FeatureGroup with transparent and grey style\n",
    "# folium.GeoJson(match_lines_geojson, name='Match Lines', style_function=lambda x: {'color': 'grey', 'opacity': 0.5}).add_to(match_lines_fg)\n",
    "\n",
    "# # Add FeatureGroups to the map\n",
    "# matched_trip_fg.add_to(mymap)\n",
    "# gps_points_fg.add_to(mymap)\n",
    "# match_lines_fg.add_to(mymap)\n",
    "\n",
    "# # Add start and end points with play and stop buttons\n",
    "# start_icon = folium.Icon(color='green',icon='play',prefix='fa')\n",
    "# end_icon = folium.Icon(color='red',icon='stop',prefix='fa')\n",
    "# folium.Marker(location=[start_pt.y, start_pt.x],icon=start_icon).add_to(mymap)\n",
    "# folium.Marker(location=[end_pt.y, end_pt.x],icon=end_icon).add_to(mymap)\n",
    "\n",
    "# # Add layer control to toggle layers on/off\n",
    "# folium.LayerControl().add_to(mymap)\n",
    "\n",
    "# # Add legend with statistics\n",
    "# #TODO what happened to duration\n",
    "# legend_html = f'''\n",
    "#     <div style=\"position: fixed; \n",
    "#             bottom: 5px; left: 5px; width: 300px; height: 250px; \n",
    "#             border:2px solid grey; z-index:9999; font-size:14px;\n",
    "#             background-color: white;\n",
    "#             opacity: 0.9;\">\n",
    "#     &nbsp; <b>Trip ID: {tripid} </b> <br>\n",
    "#     &nbsp; <b> Match Date: {match_dict[tripid]['time']} </b> <br>\n",
    "#     &nbsp; Start Point &nbsp; <i class=\"fa fa-play\" style=\"color:green\"></i>,\n",
    "#     End Point &nbsp; <i class=\"fa fa-stop\" style=\"color:red\"></i> <br>\n",
    "    \n",
    "#     &nbsp; Matched Path &nbsp; <div style=\"width: 20px; height: 5px; background-color: red; display: inline-block;\"></div> <br>\n",
    "#     &nbsp; Match Lines Path &nbsp; <div style=\"width: 20px; height: 5px; background-color: gray; display: inline-block;\"></div> <br>\n",
    " \n",
    "#     &nbsp; Points Matched: {match_dict[tripid]['last_matched']}/{match_dict[tripid]['trace'].shape[0]} <br>\n",
    "#     &nbsp; Match Ratio: {match_dict[tripid]['match_ratio']:.2f} <br>\n",
    "#     &nbsp; GPS Distance: {match_dict[tripid]['gps_distance']:.1f} ft. <br>\n",
    "#     &nbsp; Matched Trace Distance: {matched_trip.length.sum():.0f} ft. <br>\n",
    "#     &nbsp; Mean Matching Distance: {match_dict[tripid]['match_lines'].length.mean():.0f} ft. \n",
    "\n",
    "#     </div>\n",
    "#     '''\n",
    "# mymap.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# # Save the map to an HTML file or display it in a Jupyter notebook\n",
    "# #mymap.save('map.html')\n",
    "# # mymap.save('/path/to/save/map.html')  # Use an absolute path if needed\n",
    "# mymap  # Uncomment if you are using Jupyter notebook\n",
    "\n",
    "# #TODO add in the legend with trip info and then we're golden\n",
    "\n",
    "# match_dict[tripid].keys()\n",
    "# match_dict[tripid]['match_ratio']\n",
    "# match_dict[tripid].keys()\n",
    "# help(InMemMap)\n",
    "# help(DistanceMatcher)\n",
    "# :param map_con: Map object to connect to map database\n",
    "#         :param obs_noise: Standard deviation of noise\n",
    "#         :param obs_noise_ne: Standard deviation of noise for non-emitting states (is set to obs_noise if not given)\n",
    "#         :param max_dist_init: Maximum distance from start location (if not given, uses max_dist)\n",
    "#         :param max_dist: Maximum distance from path (this is a hard cut, min_prob_norm should be better)\n",
    "#         :param min_prob_norm: Minimum normalized probability of observations (ema)\n",
    "#         :param non_emitting_states: Allow non-emitting states. A non-emitting state is a state that is\n",
    "#             not associated with an observation. Here we assume it can be associated with a location in between\n",
    "#             two observations to allow for pruning. It is advised to set min_prob_norm and/or max_dist to avoid\n",
    "#             visiting all possible nodes in the graph.\n",
    "#         :param non_emitting_length_factor: Reduce the probability of a sequence of non-emitting states the longer it\n",
    "#             is. This can be used to prefer shorter paths. This is separate from the transition probabilities because\n",
    "#             transition probabilities are averaged for non-emitting states and thus the length is also averaged out.\n",
    "#         :param max_lattice_width: Restrict the lattice (or possible candidate states per observation) to this value.\n",
    "#             If there are more possible next states, the states with the best likelihood so far are selected.\n",
    "\n",
    "#         :param dist_noise: Standard deviation of difference between distance between states and distance\n",
    "#             between observatoins. If not given, set to obs_noise\n",
    "#         :param dist_noise_ne: If not given, set to dist_noise\n",
    "#         :param restrained_ne: Avoid non-emitting states if the distance between states and between\n",
    "#             observations is close to each other.\n",
    "#         :param avoid_goingback: If true, the probability is lowered for a transition that returns back to a\n",
    "#             previous edges or returns to a position on an edge.\n",
    "\n",
    "# # #get list of coords\n",
    "# # gps_trace = list(zip(trace.geometry.y,trace.geometry.x))\n",
    "\n",
    "# # #perform matching\n",
    "# # states, last_matched = matcher.match(gps_trace)\n",
    "# # only_nodes = matcher.path_pred_onlynodes\n",
    "\n",
    "# # print(\"States\\n------\")\n",
    "# # print(states)\n",
    "# # print(\"Nodes\\n------\")\n",
    "# # print(only_nodes)\n",
    "# # print(\"\")\n",
    "# # matcher.print_lattice_stats()\n",
    "# # fig, ax = plt.subplots(1, 1)\n",
    "# # mmviz.plot_map(map_con, matcher=matcher,\n",
    "# #                ax=ax,\n",
    "# #                show_labels=True, show_matching=True, show_graph=False,\n",
    "# #                filename=\"my_plot.png\")\n",
    "# # test = matcher.lattice[4]\n",
    "# # m = max(test.values_all(), key=lambda m: m.logprob) # for the 4th point get the one with the highest logprob\n",
    "\n",
    "# # m.logprob\n",
    "# # import numpy as np\n",
    "# # t = {x.cname.split('_')[0] + '_' + x.cname.split('_')[1]: x.logprob for x in test.values_all()}\n",
    "# # check = pd.DataFrame.from_dict(t,orient='index',columns=['logprob']).sort_values('logprob',ascending=False)\n",
    "# # check\n",
    "# # (check.index == '5424132517_7151205661').sum()\n",
    "# # testing = trace.copy()\n",
    "# # testing.geometry = testing.buffer(1000)\n",
    "# # intersect = gpd.overlay(edges,testing)\n",
    "# # intersect[(intersect['A_B'] == '5424132517_7151205661') & (intersect['sequence'] == 4)]\n",
    "\n",
    "# # #reduce the states size with match_nodes\n",
    "# # reduced_states = list(set(edges))\n",
    "\n",
    "# # #calculate the match ratio\n",
    "# # match_ratio = last_matched / (len(gps_trace)-1)\n",
    "    \n",
    "# # #retreive matched edges from network\n",
    "# # geos_list = [geos_dict.get(id,0) for id in reduced_states]\n",
    "\n",
    "# # #turn into geodataframe\n",
    "# # matched_trip = gpd.GeoDataFrame(data={'A_B':reduced_states,'geometry':geos_list},geometry='geometry',crs='epsg:2240')\n",
    "\n",
    "# # #turn tuple to str\n",
    "# # matched_trip['A_B'] = matched_trip['A_B'].apply(lambda row: f'{row[0]}_{row[1]}')\n",
    "\n",
    "# # #reset index to add an edge sequence column\n",
    "# # matched_trip.reset_index().rename(columns={'index':'edge_sequence'},inplace=True)\n",
    "\n",
    "# # trace['interpolated_point'] = pd.Series([ Point(x.edge_m.pi) for x in matcher.lattice_best ])\n",
    "# # trace = trace.loc[0:last_matched]\n",
    "# # trace['match_lines'] = trace.apply(lambda row: LineString([row['geometry'],row['interpolated_point']]),axis=1)\n",
    "\n",
    "# # interpolated_points = trace[['sequence','interpolated_point']]\n",
    "# # interpolated_points = gpd.GeoDataFrame(interpolated_points,geometry='interpolated_point')\n",
    "\n",
    "# # match_lines = trace[['sequence','match_lines']]\n",
    "# # match_lines = gpd.GeoDataFrame(match_lines,geometry='match_lines')\n",
    "# # match_lines['length'] = match_lines.length\n",
    "\n",
    "\n",
    "# # interpolated_points.to_file(project_dir/f\"single_example/{tripid}.gpkg\",layer='interpolated_points')\n",
    "# # match_lines.to_file(project_dir/f\"single_example/{tripid}.gpkg\",layer='match_lines')\n",
    "\n",
    "# # #%%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
