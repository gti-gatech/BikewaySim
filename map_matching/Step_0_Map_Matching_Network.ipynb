{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Map Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from leuvenmapmatching.matcher.distance import DistanceMatcher\n",
    "from leuvenmapmatching.map.inmem import InMemMap\n",
    "#from leuvenmapmatching import visualization as mmviz\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from shapely.ops import Point, LineString\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "from bikewaysim.paths import config\n",
    "from bikewaysim.map_matching import map_match\n",
    "from bikewaysim.network import prepare_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = gpd.read_file(config['network_fp']/'final_network.gpkg',layer='edges')\n",
    "nodes = gpd.read_file(config['network_fp']/'final_network.gpkg',layer='nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove infra before 2016 so it doesn't match to these\n",
    "# after = links['facility'].isin(['cycletrack','multi use path']) & \\\n",
    "#           (links['link_type']!='road') & \\\n",
    "#           links['year'].notna() & \\\n",
    "#           (links['year']>2016)\n",
    "# # links[after].drop(columns=['all_tags']).explore()\n",
    "# links = links[after==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['service' 'road' 'sidewalk' 'pedestrian' 'parking_and_driveways' 'bike'\n",
      " None]\n"
     ]
    }
   ],
   "source": [
    "#only allow roads + pedestrian + bike\n",
    "link_types_allowed = ['bike','pedestrian','road','service','connector','parking_and_driveways']\n",
    "print(links['link_type'].unique())\n",
    "\n",
    "#TODO add the ability to go the wrongway on residential streets ONLY\n",
    "allow_wrongway_on = ['residential','living_street']\n",
    "links.loc[links['highway'].isin(allow_wrongway_on),'oneway'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before connected components: Links 22026 Nodes 18712\n",
      "After connected components: Links 21312 Nodes 17869\n"
     ]
    }
   ],
   "source": [
    "exploded_links, exploded_nodes, map_con = map_match.prepare_mapmatch_network(links,nodes,link_types_allowed,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace Data\n",
    "For map matching, we're using GPS traces that have been processed so that each point is spaced a certain distance apart, coordinates in between this distance are dropped to improve computation time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map matching 682 trips\n"
     ]
    }
   ],
   "source": [
    "#load all traces\n",
    "with (config['cycleatl_fp']/'reduced_spacing.pkl').open('rb') as fh:\n",
    "    coords_dict = pickle.load(fh)\n",
    "\n",
    "# import trips that we want to match\n",
    "with (config['cycleatl_fp']/'trips_4.pkl').open('rb') as fh:\n",
    "    trips_df = pickle.load(fh)\n",
    "\n",
    "# subset the coords dict by just the trips we're trying to match\n",
    "coords_dict = {key:item for key, item in coords_dict.items() if key in trips_df['tripid'].tolist()}\n",
    "# drop speed_mph below zero if that hasn't been done\n",
    "coords_dict = {key:item[item['speed_mph']>1] for key, item in coords_dict.items() if item[item['speed_mph']>1].shape[0] > 0}\n",
    "\n",
    "print('Map matching',len(coords_dict.keys()),'trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the privacy filter \n",
    "def privacy_distance(df,privacy_dist=500):\n",
    "    first_point = df['geometry'].iloc[0].buffer(privacy_dist)\n",
    "    last_point = df['geometry'].iloc[-1].buffer(privacy_dist)\n",
    "    double_buffer = df['geometry'].iloc[0].buffer(privacy_dist*2)\n",
    "    if df['geometry'].intersects(double_buffer).all():\n",
    "        return\n",
    "    else:\n",
    "        first_cut = df['geometry'].intersects(first_point).idxmin() # find the first point where it's false\n",
    "        last_cut = df['geometry'].intersects(last_point).idxmax() - 1\n",
    "        if df.loc[first_cut:last_cut,:].shape[0] == 0:\n",
    "            return\n",
    "        else:\n",
    "            return df.loc[first_cut:last_cut,:]\n",
    "coords_dict = {key:privacy_distance(item) for key, item in coords_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['matching_fp'] / \"coords_dict.pkl\").open('wb') as fh:\n",
    "    pickle.dump(coords_dict,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['matching_fp'] / \"map_con.pkl\").open('wb') as fh:\n",
    "    pickle.dump((exploded_links,exploded_nodes),fh)\n",
    "\n",
    "# split the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One off map matching example\n",
    "Use this for testing purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Map Matching Files\n",
    "- Split the data into buckets of 500 each (10hrs to hrs)\n",
    "- Pickle the split up dicts, the network, and the matching settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "682"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coords_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_dict = {tripid:map_match.leuven_match(trace,matching_settings,map_con,exploded_links) for tripid, trace in tqdm(coords_dict.items(),total=len(coords_dict))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#split match_dict into X parts\n",
    "#split\n",
    "small_coords = []\n",
    "small_dict = {}\n",
    "for idx, (tripid, item) in enumerate(coords_dict.items()):\n",
    "    #500 each\n",
    "    if (idx % 500 == 0) & (idx != 0):\n",
    "        small_dict[tripid] = item\n",
    "        small_coords.append(small_dict)\n",
    "        small_dict = {}\n",
    "    elif idx + 1 == len(coords_dict):\n",
    "        small_dict[tripid] = item\n",
    "        small_coords.append(small_dict)\n",
    "    else:\n",
    "        small_dict[tripid] = item\n",
    "print(len(small_coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,x in enumerate(small_coords):\n",
    "    with (config['matching_fp']/f'coords_dict_{idx}.pkl').open('wb') as fh:\n",
    "        pickle.dump(x,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The matching setting dictionary stores all of the settings used for map matching, so they can be retrieved later for study\n",
    "if (config['matching_fp'] / 'matching_settings_df.pkl').exists():\n",
    "    with (config['matching_fp'] / 'matching_settings_df.pkl').open('rb') as fh:\n",
    "        matching_settings_df = pickle.load(fh)\n",
    "else:\n",
    "    matching_settings_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO pickle the matchign settings so that we can still do the tracking of the different matching settings\n",
    "matching_settings = {\n",
    "        'obs_noise': 50, #Standard deviation of noise\n",
    "        'obs_noise_ne': 100, #Standard deviation of noise for non-emitting states (is set to obs_noise if not given)\n",
    "        'max_dist_init': 2000, #Maximum distance from start location (if not given, uses max_dist)\n",
    "        'max_dist': 1000, #Maximum distance from path (this is a hard cut, min_prob_norm should be better)\n",
    "        'min_prob_norm': 0.005, #Minimum normalized probability of observations (ema)\n",
    "        'non_emitting_states': False, #Allow non-emitting states\n",
    "        'non_emitting_length_factor': 0.75, #Reduce the probability of a sequence of non-emitting states the longer it is.\n",
    "        'max_lattice_width': 50, #Restrict the lattice (or possible candidate states per observation) to this value.\n",
    "        'dist_noise': 50, #Standard deviation of difference between distance between states and distance between observations.\n",
    "        'dist_noise_ne': 200, #for no emitting If not given, set to dist_noise\n",
    "        'restrained_ne': True, #Avoid non-emitting states if the distance between states and between observations is close to each other.\n",
    "        'avoid_goingback': True, #If true, the probability is lowered for a transition that returns back to a previous edges or returns to a position on an edge.\n",
    "        'increase_max_lattice_width': False,\n",
    "        'export_graph': False,\n",
    "        'link_types': str(np.sort(link_types_allowed)),\n",
    "        'allow_wrongway': False\n",
    "    }\n",
    "#add to matching_settings_tuple if contents are unique\n",
    "row = pd.DataFrame([matching_settings])\n",
    "matching_settings_df = pd.concat([matching_settings_df,row],ignore_index=True)\n",
    "if matching_settings_df.duplicated().any():\n",
    "    print('Settings have been used before')\n",
    "matching_settings_df.drop_duplicates(inplace=True)\n",
    "matching_index = matching_settings_df[(matching_settings_df == tuple(row.loc[0,:])).all(axis=1)].index.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports only the current matching settings\n",
    "with (config['matching_fp']/'match_settings.pkl').open('wb') as fh:\n",
    "    pickle.dump((matching_index,matching_settings),fh)\n",
    "\n",
    "# export the matching settings tested\n",
    "with (config['matching_fp']/'matching_settings_df.pkl').open('wb') as fh:\n",
    "    pickle.dump(matching_settings_df,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bikewaysim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
