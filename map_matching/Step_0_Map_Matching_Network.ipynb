{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Map Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from leuvenmapmatching.matcher.distance import DistanceMatcher\n",
    "from leuvenmapmatching.map.inmem import InMemMap\n",
    "#from leuvenmapmatching import visualization as mmviz\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from shapely.ops import Point, LineString\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "from bikewaysim.paths import config\n",
    "from bikewaysim.map_matching import map_match\n",
    "from bikewaysim.network import prepare_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = gpd.read_file(config['network_fp']/'final_network.gpkg',layer='edges')\n",
    "nodes = gpd.read_file(config['network_fp']/'final_network.gpkg',layer='nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove infra before 2016 so it doesn't match to these\n",
    "# after = links['facility'].isin(['cycletrack','multi use path']) & \\\n",
    "#           (links['link_type']!='road') & \\\n",
    "#           links['year'].notna() & \\\n",
    "#           (links['year']>2016)\n",
    "# # links[after].drop(columns=['all_tags']).explore()\n",
    "# links = links[after==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['service' 'parking_and_driveways' 'road' 'bike' 'pedestrian' 'sidewalk'\n",
      " None]\n"
     ]
    }
   ],
   "source": [
    "#only allow roads + pedestrian + bike\n",
    "link_types_allowed = ['bike','pedestrian','road','service','connector','parking_and_driveways']\n",
    "print(links['link_type'].unique())\n",
    "\n",
    "#TODO add the ability to go the wrongway on residential streets ONLY\n",
    "allow_wrongway_on = ['residential','living_street']\n",
    "links.loc[links['highway'].isin(allow_wrongway_on),'oneway'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before connected components: Links 170649 Nodes 138048\n",
      "After connected components: Links 168970 Nodes 136131\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Add 69302632 first as node",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m exploded_links, exploded_nodes, map_con \u001b[38;5;241m=\u001b[39m \u001b[43mmap_match\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_mapmatch_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinks\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlink_types_allowed\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\BikewaySimDev\\src\\bikewaysim\\map_matching\\map_match.py:81\u001b[0m, in \u001b[0;36mprepare_mapmatch_network\u001b[1;34m(links, nodes, link_types_allowed, allow_wrongway)\u001b[0m\n\u001b[0;32m     75\u001b[0m exploded_links\u001b[38;5;241m.\u001b[39mset_crs(crs,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# exploded_nodes = gpd.GeoDataFrame(exploded_nodes,crs=links.crs)\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# exploded_links = gpd.GeoDataFrame(exploded_links,crs=links.crs)\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# make the network\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m#create map matching graph network (no wrongway)\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m map_con \u001b[38;5;241m=\u001b[39m \u001b[43mmake_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexploded_links\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexploded_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mallow_wrongway\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exploded_links, exploded_nodes, map_con\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\BikewaySimDev\\src\\bikewaysim\\map_matching\\map_match.py:97\u001b[0m, in \u001b[0;36mmake_network\u001b[1;34m(edges, nodes, allow_wrongway)\u001b[0m\n\u001b[0;32m     94\u001b[0m     map_con\u001b[38;5;241m.\u001b[39madd_node(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m], (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mx,row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39my))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m edges\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 97\u001b[0m     \u001b[43mmap_con\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msource\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m#allow wrongway condition for certain roads like residential\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# TODO item\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# if isinstance(allow_wrongway,list):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m#     new_allow_wrongway = \u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moneway\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m|\u001b[39m (allow_wrongway\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\tpassmore6\\Anaconda3\\envs\\bikewaysim\\Lib\\site-packages\\leuvenmapmatching\\map\\inmem.py:239\u001b[0m, in \u001b[0;36mInMemMap.add_edge\u001b[1;34m(self, node_a, node_b)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add new edge to the map.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m:param node_a: Label for the node that is the start of the edge\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m:param node_b: Label for the node that is the end of the edge\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node_a \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph:\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdd \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_a\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m first as node\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node_b \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdd \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m first as node\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Add 69302632 first as node"
     ]
    }
   ],
   "source": [
    "exploded_links, exploded_nodes, map_con = map_match.prepare_mapmatch_network(links,nodes,link_types_allowed,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace Data\n",
    "For map matching, we're using GPS traces that have been processed so that each point is spaced a certain distance apart, coordinates in between this distance are dropped to improve computation time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all traces\n",
    "with (config['cycleatl_fp']/'reduced_spacing.pkl').open('rb') as fh:\n",
    "    coords_dict = pickle.load(fh)\n",
    "\n",
    "# import trips that we want to match\n",
    "with (config['cycleatl_fp']/'trips_4.pkl').open('rb') as fh:\n",
    "    trips_df = pickle.load(fh)\n",
    "\n",
    "# subset the coords dict by just the trips we're trying to match\n",
    "coords_dict = {key:item for key, item in coords_dict.items() if key in trips_df['tripid'].tolist()}\n",
    "# drop speed_mph below zero if that hasn't been done\n",
    "coords_dict = {key:item[item['speed_mph']>1] for key, item in coords_dict.items() if item[item['speed_mph']>1].shape[0] > 0}\n",
    "\n",
    "print('Map matching',len(coords_dict.keys()),'trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the privacy filter \n",
    "def privacy_distance(df,privacy_dist=500):\n",
    "    first_point = df['geometry'].iloc[0].buffer(privacy_dist)\n",
    "    last_point = df['geometry'].iloc[-1].buffer(privacy_dist)\n",
    "    double_buffer = df['geometry'].iloc[0].buffer(privacy_dist*2)\n",
    "    if df['geometry'].intersects(double_buffer).all():\n",
    "        return\n",
    "    else:\n",
    "        first_cut = df['geometry'].intersects(first_point).idxmin() # find the first point where it's false\n",
    "        last_cut = df['geometry'].intersects(last_point).idxmax() - 1\n",
    "        if df.loc[first_cut:last_cut,:].shape[0] == 0:\n",
    "            return\n",
    "        else:\n",
    "            return df.loc[first_cut:last_cut,:]\n",
    "coords_dict = {key:privacy_distance(item) for key, item in coords_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['matching_fp'] / \"coords_dict.pkl\").open('wb') as fh:\n",
    "    pickle.dump(coords_dict,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['matching_fp'] / \"map_con.pkl\").open('wb') as fh:\n",
    "    pickle.dump((exploded_links,exploded_nodes),fh)\n",
    "\n",
    "# split the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One off map matching example\n",
    "Use this for testing purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Map Matching Files\n",
    "- Split the data into buckets of 500 each (10hrs to hrs)\n",
    "- Pickle the split up dicts, the network, and the matching settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coords_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_dict = {tripid:map_match.leuven_match(trace,matching_settings,map_con,exploded_links) for tripid, trace in tqdm(coords_dict.items(),total=len(coords_dict))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split match_dict into X parts\n",
    "#split\n",
    "small_coords = []\n",
    "small_dict = {}\n",
    "for idx, (tripid, item) in enumerate(coords_dict.items()):\n",
    "    #500 each\n",
    "    if (idx % 500 == 0) & (idx != 0):\n",
    "        small_dict[tripid] = item\n",
    "        small_coords.append(small_dict)\n",
    "        small_dict = {}\n",
    "    elif idx + 1 == len(coords_dict):\n",
    "        small_dict[tripid] = item\n",
    "        small_coords.append(small_dict)\n",
    "    else:\n",
    "        small_dict[tripid] = item\n",
    "print(len(small_coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,x in enumerate(small_coords):\n",
    "    with (config['matching_fp']/f'coords_dict_{idx}.pkl').open('wb') as fh:\n",
    "        pickle.dump(x,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The matching setting dictionary stores all of the settings used for map matching, so they can be retrieved later for study\n",
    "if (config['matching_fp'] / 'matching_settings_df.pkl').exists():\n",
    "    with (config['matching_fp'] / 'matching_settings_df.pkl').open('rb') as fh:\n",
    "        matching_settings_df = pickle.load(fh)\n",
    "else:\n",
    "    matching_settings_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO pickle the matchign settings so that we can still do the tracking of the different matching settings\n",
    "matching_settings = {\n",
    "        'obs_noise': 50, #Standard deviation of noise\n",
    "        'obs_noise_ne': 100, #Standard deviation of noise for non-emitting states (is set to obs_noise if not given)\n",
    "        'max_dist_init': 2000, #Maximum distance from start location (if not given, uses max_dist)\n",
    "        'max_dist': 1000, #Maximum distance from path (this is a hard cut, min_prob_norm should be better)\n",
    "        'min_prob_norm': 0.005, #Minimum normalized probability of observations (ema)\n",
    "        'non_emitting_states': False, #Allow non-emitting states\n",
    "        'non_emitting_length_factor': 0.75, #Reduce the probability of a sequence of non-emitting states the longer it is.\n",
    "        'max_lattice_width': 50, #Restrict the lattice (or possible candidate states per observation) to this value.\n",
    "        'dist_noise': 50, #Standard deviation of difference between distance between states and distance between observations.\n",
    "        'dist_noise_ne': 200, #for no emitting If not given, set to dist_noise\n",
    "        'restrained_ne': True, #Avoid non-emitting states if the distance between states and between observations is close to each other.\n",
    "        'avoid_goingback': True, #If true, the probability is lowered for a transition that returns back to a previous edges or returns to a position on an edge.\n",
    "        'increase_max_lattice_width': False,\n",
    "        'export_graph': False,\n",
    "        'link_types': str(np.sort(link_types_allowed)),\n",
    "        'allow_wrongway': False\n",
    "    }\n",
    "#add to matching_settings_tuple if contents are unique\n",
    "row = pd.DataFrame([matching_settings])\n",
    "matching_settings_df = pd.concat([matching_settings_df,row],ignore_index=True)\n",
    "if matching_settings_df.duplicated().any():\n",
    "    print('Settings have been used before')\n",
    "matching_settings_df.drop_duplicates(inplace=True)\n",
    "matching_index = matching_settings_df[(matching_settings_df == tuple(row.loc[0,:])).all(axis=1)].index.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['matching_fp']/'match_settings.pkl').open('wb') as fh:\n",
    "    pickle.dump((matching_index,matching_settings),fh)\n",
    "\n",
    "# export the matching settings tested\n",
    "with (config['matching_fp']/'matching_settings_df.pkl').open('wb') as fh:\n",
    "    pickle.dump(matching_settings_df,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
