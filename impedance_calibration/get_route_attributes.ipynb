{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Route Attributes from List of Edges IDs and the Direction Travelled\n",
    "We have a list of edges or turns (depending on the settings) from Dijkstra, and we need to get the route attributes (how many turns, how much feet up, miles of bike facility, etc) to report the route characterstics of a trip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- For link attributes we have the linkid and direction of travel (because of elevation).\n",
    "- For turn attributes we just need linkid to linkid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from shapely.ops import MultiLineString\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# #TODO will need to fix this\n",
    "# sys.path.insert(0,str(Path.cwd().parent))\n",
    "# from network.src import modeling_turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = json.load((Path.cwd().parent / 'config.json').open('rb'))\n",
    "export_fp = Path(config['project_directory']) / 'Calibration'\n",
    "cycleatl_fp = Path(config['project_directory']) / 'CycleAtlanta'\n",
    "matching_fp = Path(config['project_directory']) / 'Map_Matching'\n",
    "network_fp = Path(config['project_directory']) / 'Network'\n",
    "if export_fp.exists() == False:\n",
    "    export_fp.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turns_df = pd.read_parquet(network_fp/'turns_df.parquet')\n",
    "edges_w_attr = gpd.read_file(network_fp/'final_network.gpkg',layer='edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = edges_w_attr.loc[edges_w_attr['name'].notna()]\n",
    "test[test['name'].apply(lambda x: 'beltline' in x.lower())].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unit conversions\n",
    "edges_w_attr['length_mi'] = (edges_w_attr['length_ft'] / 5280).round(2)\n",
    "edges_w_attr['ascent_ft'] = (edges_w_attr['ascent_m'] / 3.28084).round(0)\n",
    "edges_w_attr['descent_ft'] = (edges_w_attr['descent_m'] / 3.28084).round(0)\n",
    "edges_w_attr.drop(columns=['length_ft','ascent_m','descent_m'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set index\n",
    "edges_w_attr.set_index('linkid',inplace=True)\n",
    "turns_df.set_index(['source_linkid','source_reverse_link','target_linkid','target_reverse_link'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the columns that need to be reversed (within function)\n",
    "ascent_columns = [col for col in edges_w_attr.columns if 'ascent' in col]\n",
    "descent_columns = [col for col in edges_w_attr.columns if 'descent' in col]\n",
    "bike_facils = ['facility_fwd','facility_rev']\n",
    "\n",
    "#added a major/minor classification, everything else is just left as \"road\"\n",
    "major_road = ['primary','secondary']\n",
    "major_road = major_road + [item + '_link' for item in major_road]\n",
    "minor_road = ['tertiary','unclassified','residential']\n",
    "major_road = major_road + [item + '_link' for item in minor_road]\n",
    "edges_w_attr.loc[edges_w_attr['highway'].isin(major_road),'link_type_new'] = 'major_road'\n",
    "edges_w_attr.loc[edges_w_attr['highway'].isin(minor_road),'link_type_new'] = 'minor_road'\n",
    "edges_w_attr.loc[edges_w_attr['link_type_new'].isna(),'link_type_new'] = edges_w_attr.loc[edges_w_attr['link_type_new'].isna(),'link_type']\n",
    "\n",
    "#do a motorized vs non-motorized split\n",
    "edges_w_attr['mixed_traffic'] = False\n",
    "roads = ['primary', 'residential', 'service', 'secondary', 'tertiary',\n",
    "       'secondary_link', 'unclassified','primary_link','tertiary_link','trunk', 'trunk_link']\n",
    "edges_w_attr.loc[edges_w_attr['highway'].isin(roads),'mixed_traffic'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (matching_fp / 'matched_0.pkl').open('rb') as fh:\n",
    "    matched_traces_dict = pickle.load(fh)\n",
    "with (export_fp/'ready_for_calibration.pkl').open('rb') as fh:\n",
    "    ready_for_calibration = pickle.load(fh)\n",
    "#get trip date for the bike facility check\n",
    "for key, item in ready_for_calibration.items():\n",
    "    ready_for_calibration[key]['start_time'] = matched_traces_dict.get(key,0)['trace'].iloc[0,2].year\n",
    "del matched_traces_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more than 2 links\n",
    "print('Initial:',len(ready_for_calibration))\n",
    "ready_for_calibration = {key:item for key, item in ready_for_calibration.items() if (item['matched_edges'].shape[0] > 2) & (item['shortest_edges'].shape[0] > 2)}\n",
    "print('Successful',len(ready_for_calibration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_attributes(tripid,match_dict_entry,edge_col):\n",
    "    '''\n",
    "    Two different types of summarization:\n",
    "\n",
    "    Instance based (turns, signals, bridges, etc)\n",
    "\n",
    "    Length based on certain tag (bike facilities)\n",
    "\n",
    "    Cumulative (length,elevation)\n",
    "\n",
    "    '''\n",
    "\n",
    "    summary_attributes = {}\n",
    "\n",
    "    summary_attributes['tripid'] = tripid\n",
    "\n",
    "    #get trip date for the bike facility check\n",
    "    trip_date_year = match_dict_entry['start_time']#trip_date_year = match_dict_entry['trace'].iloc[0,2].year\n",
    "\n",
    "    #get route and turns\n",
    "    route = [tuple(x) for x in match_dict_entry[edge_col].values]\n",
    "    turns = [(route[i][0],route[i][1],route[i+1][0],route[i+1][1]) for i in range(0,len(route)-1)]\n",
    "    \n",
    "    #remove any doubling back (might be some of this in the matched dataset)\n",
    "    turns = [turns for turns in turns if turns[0] != turns[2]]\n",
    "    linkids = match_dict_entry[edge_col]['linkid'].tolist()\n",
    "    reverse_links = match_dict_entry[edge_col]['reverse_link'].tolist()\n",
    "    \n",
    "    #get attributes\n",
    "    route_w_attr = edges_w_attr.loc[linkids]\n",
    "    turns_w_attr = turns_df.loc[turns]\n",
    "\n",
    "    #add reverse direction\n",
    "    route_w_attr['reverse_link'] = reverse_links\n",
    "\n",
    "    #turn to gdf\n",
    "    #route_w_attr = gpd.GeoDataFrame(route_w_attr,geometry='geometry',crs=config['projected_crs_epsg'])\n",
    "    summary_attributes[\"geometry\"] = MultiLineString([list(line.coords) for line in route_w_attr['geometry'].values])\n",
    "\n",
    "    #flip relevant attributes\n",
    "    route_w_attr.loc[route_w_attr['reverse_link']==True,ascent_columns+descent_columns+bike_facils] = \\\n",
    "        route_w_attr.loc[route_w_attr['reverse_link']==True,descent_columns+ascent_columns+bike_facils[::-1]].values\n",
    "\n",
    "    #set the bike facility to na if the trip date was before the bike facility\n",
    "    route_w_attr.loc[route_w_attr['year'] > trip_date_year,bike_facils] = np.nan\n",
    "    #set no facility_fwd to nan\n",
    "    route_w_attr.loc[route_w_attr['facility_fwd']=='no facility'] = None\n",
    "\n",
    "    #summary columns\n",
    "    summary_attributes[\"length_mi\"] = route_w_attr['length_mi'].sum()\n",
    "    summary_attributes[\"ascent_ft\"] = route_w_attr['ascent_ft'].sum()\n",
    "    summary_attributes[\"descent_ft\"] = route_w_attr['descent_ft'].sum()\n",
    "\n",
    "    # average grade by category (cut offs from broach)\n",
    "    zero_to_two = (route_w_attr['ascent_grade_%'] >= 0) & (route_w_attr['ascent_grade_%'] < 2)\n",
    "    two_to_four = (route_w_attr['ascent_grade_%'] >= 2) & (route_w_attr['ascent_grade_%'] < 4)\n",
    "    four_to_six = (route_w_attr['ascent_grade_%'] >= 4) & (route_w_attr['ascent_grade_%'] < 6)\n",
    "    six_and_beyond = (route_w_attr['ascent_grade_%'] >= 6)\n",
    "    summary_attributes[\"(0,2]_prop\"] = (route_w_attr.loc[zero_to_two,'length_mi'].sum() / route_w_attr['length_mi'].sum()).round(2)\n",
    "    summary_attributes[\"(2,4]_prop\"] = (route_w_attr.loc[two_to_four,'length_mi'].sum() / route_w_attr['length_mi'].sum()).round(2)\n",
    "    summary_attributes[\"(4,6]_prop\"] = (route_w_attr.loc[four_to_six,'length_mi'].sum() / route_w_attr['length_mi'].sum()).round(2)\n",
    "    summary_attributes[\"(6,inf)_prop\"] = (route_w_attr.loc[six_and_beyond,'length_mi'].sum() / route_w_attr['length_mi'].sum()).round(2)\n",
    "\n",
    "    #TODO add this back in the elevation step and use the same limits?\n",
    "    #add meters on grade segments (i.e. add all in length along x to x)\n",
    "    #could possibly be a more accurate represntation of steep roads\n",
    "\n",
    "    # #instance columns to summarize\n",
    "    # count_cols = ['bridge','tunnel']\n",
    "    # for count_col in count_cols:\n",
    "    #     summary_attributes[count_col] = (route_w_attr[count_col]==True).sum().round(0)\n",
    "\n",
    "    # length of route columns to summarize\n",
    "    cols = ['mixed_traffic','facility_fwd']#['link_type_new','link_type','highway']#,'speedlimit_range_mph','lanes_per_direction']\n",
    "    for col in cols:\n",
    "        #make a summary column for every unique value in that column\n",
    "        for unique_val in route_w_attr[col].unique():\n",
    "            if isinstance(unique_val,str):\n",
    "                summary_attributes[col+'.'+unique_val+'_prop'] = (route_w_attr.loc[route_w_attr[col]==unique_val,'length_mi'].sum() / route_w_attr['length_mi'].sum()).round(2)\n",
    "            elif isinstance(unique_val,bool):\n",
    "                summary_attributes[col+'.'+str(unique_val).lower()+'_prop'] = (route_w_attr.loc[route_w_attr[col]==unique_val,'length_mi'].sum() / route_w_attr['length_mi'].sum()).round(2)\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "    # turns\n",
    "    summary_attributes.update(turns_w_attr['turn_type'].value_counts().to_dict())\n",
    "\n",
    "    return summary_attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_summary = [route_attributes(key,item,'matched_edges') for key, item in ready_for_calibration.items()]\n",
    "matched_summary = pd.DataFrame.from_records(matched_summary)\n",
    "matched_summary = gpd.GeoDataFrame(matched_summary,crs=config['projected_crs_epsg'])\n",
    "matched_summary.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_summary = [route_attributes(key,item,'shortest_edges') for key, item in ready_for_calibration.items()]\n",
    "shortest_summary = pd.DataFrame.from_records(shortest_summary)\n",
    "shortest_summary = gpd.GeoDataFrame(shortest_summary,crs=config['projected_crs_epsg'])\n",
    "shortest_summary.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #calculate proportions for comparison purposes\n",
    "# per_ft = ['ascent_m','descent_m','uturn','straight', 'right', 'left']\n",
    "# proportion_columns = [\n",
    "#     'zero_to_two_%_ft', 'two_to_four_%_ft', 'four_to_six_%_ft',\n",
    "#     'six_and_beyond_%_ft', 'link_type_new.pedestrian_ft',\n",
    "#     'link_type_new.bike_ft', 'link_type_new.minor_road_ft',\n",
    "#     'link_type_new.sidewalk_or_crossing_ft', 'link_type_new.major_road_ft',\n",
    "#     'link_type_new.service_ft', 'facility_fwd.sharrow_ft',\n",
    "#     'facility_fwd.bike lane_ft', 'facility_fwd.multi use path_ft',\n",
    "#     'link_type_new.parking_and_driveways_ft',\n",
    "#     'facility_fwd.cycletrack_ft', 'link_type_new.road_ft',\n",
    "#     'facility_fwd.no facility_ft', 'facility_fwd.buffered bike lane_ft'\n",
    "#     ]\n",
    "# shortest_summary.loc[:,proportion_columns] = shortest_summary.loc[:,proportion_columns].div(shortest_summary['length_ft'],axis=0)\n",
    "# shortest_summary.loc[:,proportion_columns] = shortest_summary.loc[:,proportion_columns].div(shortest_summary['length_ft'],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_summary.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.merge(matched_summary[['tripid','mixed_traffic.false_prop']],shortest_summary[['tripid','mixed_traffic.false_prop']],on='tripid',suffixes=('_matched','_shortest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the shortest path generally has a higher proportion of the route on non mixed traffic streets\n",
    "(however this might just be due to the links allowed for routing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=test[['mixed_traffic.false_prop_matched','mixed_traffic.false_prop_shortest']],cut=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_summary.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_type_new = [col for col in shortest_summary.columns if 'facility_fwd' in col]\n",
    "link_type_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_summary.loc[:,link_type_new].sum(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_summary[link_type_new].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=matched_summary[link_type_new],cut=0,bw_adjust=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=shortest_summary[link_type_new],cut=0,bw_adjust=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = matched_summary[link_type_new] - shortest_summary[link_type_new]\n",
    "#sns.kdeplot(data=test2,cut=0,bw_adjust=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load an example dataset\n",
    "#penguins = sns.load_dataset(\"penguins\")\n",
    "\n",
    "# Define the variables to plot\n",
    "variables = test2.columns.tolist()#['length_mi','ascent_ft','descent_ft'] #+ proportion_columns#['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "\n",
    "# Number of variables\n",
    "num_vars = len(variables)\n",
    "\n",
    "# Determine grid size (e.g., 2x2)\n",
    "ncols = 3\n",
    "nrows = (num_vars // ncols) + (num_vars % ncols > 0)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 5), constrained_layout=True)\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each KDE plot on a separate subplot\n",
    "for ax, variable in zip(axes, variables):\n",
    "    sns.kdeplot(data=test2, x=variable, fill=True, ax=ax, cut=0)\n",
    "    ax.set_title(f'KDE Plot of {variable}')\n",
    "\n",
    "# Remove any empty subplots\n",
    "for ax in axes[len(variables):]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_w_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "# penguins = sns.load_dataset(\"penguins\")\n",
    "\n",
    "# Define the variables to plot\n",
    "variables = test2.columns.tolist()  # ['length_mi', 'ascent_ft', 'descent_ft'] + proportion_columns\n",
    "\n",
    "# Number of variables\n",
    "num_vars = len(variables)\n",
    "\n",
    "# Determine grid size (e.g., 2x2)\n",
    "ncols = 3\n",
    "nrows = (num_vars // ncols) + (num_vars % ncols > 0)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 5), constrained_layout=True)\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each KDE plot on a separate subplot\n",
    "for ax, variable in zip(axes, variables):\n",
    "    sns.kdeplot(data=test2, x=variable, fill=True, ax=ax, cut=0)\n",
    "    \n",
    "    # Get the min and max values of the variable\n",
    "    min_val = test2[variable].min()\n",
    "    max_val = test2[variable].max()\n",
    "    \n",
    "    # Calculate symmetric limits around zero\n",
    "    max_abs_val = max(abs(min_val), abs(max_val))\n",
    "    x_lim = (-max_abs_val, max_abs_val)\n",
    "\n",
    "    #set varaible specific limits\n",
    "    #make this a dictionary\n",
    "    if variable == 'facility_fwd.cycletrack_prop':\n",
    "        x_lim = (-0.1,0.1)\n",
    "    \n",
    "    ax.set_xlim(x_lim)\n",
    "    ax.axvline(0, color='k', linestyle='--')  # Optional: add a vertical line at zero for reference\n",
    "    ax.set_title(f'KDE Plot of {variable}')\n",
    "\n",
    "# Remove any empty subplots\n",
    "for ax in axes[len(variables):]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load an example dataset\n",
    "#penguins = sns.load_dataset(\"penguins\")\n",
    "\n",
    "# Define the variables to plot\n",
    "variables = ['length_mi','ascent_ft','descent_ft'] #+ proportion_columns#['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "\n",
    "# Number of variables\n",
    "num_vars = len(variables)\n",
    "\n",
    "# Determine grid size (e.g., 2x2)\n",
    "ncols = 3\n",
    "nrows = (num_vars // ncols) + (num_vars % ncols > 0)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 5), constrained_layout=True)\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each KDE plot on a separate subplot\n",
    "for ax, variable in zip(axes, variables):\n",
    "    sns.kdeplot(data=matched_summary, x=variable, fill=True, ax=ax, cut=0)\n",
    "    ax.set_title(f'KDE Plot of {variable}')\n",
    "\n",
    "# Remove any empty subplots\n",
    "for ax in axes[len(variables):]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert ft to miles and meters to ft\n",
    "#group relevant variables and make an \"other\" column\n",
    "#link_type, bike facility, non-motorized vs mixed traffic split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load an example dataset\n",
    "penguins = sns.load_dataset(\"penguins\")\n",
    "\n",
    "# Define the variables to plot\n",
    "variables = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "\n",
    "# Create a FacetGrid\n",
    "g = sns.FacetGrid(penguins, col='species', col_wrap=3, sharex=False, sharey=False)\n",
    "\n",
    "# Map the kdeplot onto the grid\n",
    "g.map(sns.kdeplot, 'bill_length_mm', fill=True)\n",
    "\n",
    "# Add titles to the individual plots\n",
    "for ax, title in zip(g.axes.flatten(), variables):\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# person to person density plot\n",
    "# x axis is the proportion/# of distance\n",
    "# y axis is the density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x axis is path attribute differences\n",
    "\n",
    "# y axis is the density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (cycleatl_fp/'trips_3.pkl').open('rb') as fh:\n",
    "    trips_df = pickle.load(fh)\n",
    "trips_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.merge(trips_df,summary_df,on='tripid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "summary_df.to_file(matching_fp/'matched_routes_w_attributes.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO move on to shortest path?\n",
    "summary_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "df = summary_df.copy()\n",
    "\n",
    "# Assuming your dataframe is named df\n",
    "# Drop any rows with missing values (if any)\n",
    "df.fillna(0,inplace=True)\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop(columns=['tripid', 'geometry', 'length_ft'])\n",
    "y = df['length_ft']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(regressor, feature_names=X.columns, filled=True, rounded=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(regressor.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "testtrip = random.choice(list(matched_traces_dict.keys()))\n",
    "testtrip = matched_traces_dict[testtrip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dict = {}\n",
    "\n",
    "#get edges\n",
    "route = testtrip['edges']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get route and turns\n",
    "route = [tuple(x) for x in testtrip['edges'].values]\n",
    "turns = [(route[i][0],route[i][1],route[i+1][0],route[i+1][1]) for i in range(0,len(route)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Two different types of summarization:\n",
    "\n",
    "Instance based (turns, signals, bridges, etc)\n",
    "\n",
    "Length based on certain tag (bike facilities)\n",
    "\n",
    "Cumulative (length,elevation)\n",
    "\n",
    "'''\n",
    "\n",
    "summary_attributes = {}\n",
    "\n",
    "#get trip date for the bike facility check\n",
    "trip_date_year = testtrip['trace'].iloc[0,2].year\n",
    "\n",
    "#get route and turns\n",
    "route = testtrip['edges']\n",
    "turns = [(route.values[i][0],route.values[i][1],route.values[i+1][0],route.values[i+1][1]) for i in range(0,len(route.values)-1)]\n",
    "turns = pd.DataFrame(turns,columns=['source_linkid','source_reverse_link','target_linkid','target_reverse_link'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#retrieve attributes\n",
    "route_w_attr = pd.merge(route,edges_w_attr,on='linkid')\n",
    "turns_w_attr = pd.merge(turns,turns_df,on=['source_linkid','source_reverse_link','target_linkid','target_reverse_link'])\n",
    "\n",
    "#turn to gdf\n",
    "route_w_attr = gpd.GeoDataFrame(route_w_attr,geometry='geometry',crs=config['projected_crs_epsg'])\n",
    "summary_attributes[\"geometry\"] = MultiLineString(route_w_attr['geometry'].tolist())\n",
    "\n",
    "#flip relevant attributes\n",
    "route_w_attr.loc[route_w_attr['reverse_link']==True,ascent_columns+descent_columns+bike_facils] = \\\n",
    "    route_w_attr.loc[route_w_attr['reverse_link']==True,descent_columns+ascent_columns+bike_facils[::-1]].values\n",
    "\n",
    "#set the bike facility to na if the trip date was before the bike facility\n",
    "route_w_attr.loc[route_w_attr['year'] > trip_date_year,bike_facils] = np.nan\n",
    "\n",
    "#summary columns\n",
    "summary_attributes[\"length_ft\"] = route_w_attr['length_ft'].sum().round(0)\n",
    "summary_attributes[\"ascent_m\"] = route_w_attr['ascent_m'].sum().round(0)\n",
    "summary_attributes[\"descent_m\"] = route_w_attr['descent_m'].sum().round(0)\n",
    "\n",
    "# average grade by category (from broach)\n",
    "zero_to_two = (route_w_attr['ascent_grade_%'] > 0) & (route_w_attr['ascent_grade_%'] <= 2)\n",
    "two_to_four = (route_w_attr['ascent_grade_%'] > 2) & (route_w_attr['ascent_grade_%'] <= 4)\n",
    "four_to_six = (route_w_attr['ascent_grade_%'] > 4) & (route_w_attr['ascent_grade_%'] <= 6)\n",
    "six_and_beyond = (route_w_attr['ascent_grade_%'] > 6)\n",
    "summary_attributes[\"zero_to_two_%_ft\"] = route_w_attr.loc[zero_to_two,'length_ft'].sum().round(0)\n",
    "summary_attributes[\"two_to_four_%_ft\"] = route_w_attr.loc[two_to_four,'length_ft'].sum().round(0)\n",
    "summary_attributes[\"four_to_six_%_ft\"] = route_w_attr.loc[four_to_six,'length_ft'].sum().round(0)\n",
    "summary_attributes[\"six_and_beyond_%_ft\"] = route_w_attr.loc[six_and_beyond,'length_ft'].sum().round(0)\n",
    "\n",
    "#TODO add this back in the elevation step and use the same limits?\n",
    "#add meters on grade segments (i.e. add all in length along x to x)\n",
    "#could possibly be a more accurate represntation of steep roads\n",
    "\n",
    "# #instance columns to summarize\n",
    "# count_cols = ['bridge','tunnel']\n",
    "# for count_col in count_cols:\n",
    "#     summary_attributes[count_col] = (route_w_attr[count_col]==True).sum().round(0)\n",
    "\n",
    "# length of route columns to summarize\n",
    "cols = ['link_type_new','facility_fwd']#['link_type','highway']#,'speedlimit_range_mph','lanes_per_direction']\n",
    "for col in cols:\n",
    "    #make a summary column for every unique value in that column\n",
    "    for unique_val in route_w_attr[col].unique():\n",
    "        #skip if nan\n",
    "        if isinstance(unique_val,str) == False:\n",
    "            continue\n",
    "        summary_attributes[col+'.'+unique_val+'_ft'] = route_w_attr.loc[route_w_attr[col]==unique_val,'length_ft'].sum().round(0)\n",
    "        \n",
    "# turns\n",
    "summary_attributes.update(turns_w_attr['turn_type'].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges['tup'] = list(zip(df_edges['linkid'],df_edges['reverse_link']))\n",
    "chosen_links = df_edges.set_index('tup').loc[list_of_edges]\n",
    "list_of_turns = [(list_of_edges[i][0],list_of_edges[i][1],list_of_edges[i+1][0],list_of_edges[i+1][1]) for i in range(0,len(list_of_edges)-1)]\n",
    "chosen_turns = pseudo_df.set_index(['source_linkid','source_reverse_link','target_linkid','target_reverse_link']).loc[list_of_turns]\n",
    "chosen_links.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialize summary dict\n",
    "summary_attributes = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trip distance\n",
    "summary_attributes['trip_distance_ft'] = chosen_links['length_ft'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#instance columns to summarize\n",
    "count_cols = ['bridge','tunnel']\n",
    "for count_col in count_cols:\n",
    "    summary_attributes[count_col] = (chosen_links[count_col]==True).sum()\n",
    "\n",
    "#general elevation\n",
    "total_ascent = (chosen_links['ascent_m'].sum() / 3.28).round(0)\n",
    "summary_attributes['ascent_ft'] = total_ascent \n",
    "summary_attributes['avg_ascent_grade'] = (total_ascent / chosen_links['length_ft'].sum() * 100).round(1)\n",
    "\n",
    "#elevation broken by segment\n",
    "elev_cols = ['(0,2]_ascent','(2,4]_ascent', '(4,6]_ascent', '(6,10]_ascent', '(10,15]_ascent','(15,inf]_ascent']\n",
    "for elev_col in elev_cols:\n",
    "    total_length = (chosen_links[elev_col].sum() / 3.28).round(0)\n",
    "    summary_attributes[elev_col+'_ft'] = total_length\n",
    "\n",
    "#pct of route columns to summarize\n",
    "cols = ['link_type','highway','bike_facility_type','speedlimit_range_mph','lanes_per_direction']\n",
    "for col in cols:\n",
    "    #make a summary column for every unique value in that column\n",
    "    for unique_val in chosen_links[col].unique():\n",
    "        if (unique_val != None) | (unique_val == np.nan):\n",
    "            total_length = chosen_links[chosen_links[col] == unique_val]['length_ft'].sum()\n",
    "        else:\n",
    "            continue\n",
    "        if isinstance(unique_val,str) == False:\n",
    "            unique_val = str(unique_val)\n",
    "        summary_attributes[col+'.'+unique_val] = np.round(total_length/chosen_links['length_ft'].sum(),2)\n",
    "\n",
    "# signalized and turns\n",
    "summary_attributes['signalized'] = (chosen_turns['signalized']==True).sum()\n",
    "summary_attributes['unsignalized'] = (chosen_turns['unsignalized']==True).sum()\n",
    "turn_dict = chosen_turns['turn_type'].value_counts().to_dict()\n",
    "summary_attributes.update(turn_dict)\n",
    "\n",
    "summary_dict[tripid] = summary_attributes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn into geodataframe\n",
    "trips_df_info = pd.DataFrame.from_dict(summary_dict,orient='index')\n",
    "trips_df_info.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df = trips_df.merge(trips_df_info,left_on='tripid',right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO find the visualiztion code that we had already made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_edges = list(zip(edges['linkid'],edges['reverse_link']))\n",
    "# list_of_turns = [(list_of_edges[i][0],list_of_edges[i][1],list_of_edges[i+1][0],list_of_edges[i+1][1]) for i in range(0,len(list_of_edges)-1)]\n",
    "\n",
    "# chosen_links = df_edges.set_index(['linkid','reverse_link'],drop=False).loc[list_of_edges]\n",
    "\n",
    "# chosen_links['bridge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add user info\n",
    "trip_and_user = pd.read_pickle(fp/'gps_traces/trip_and_user.pkl')\n",
    "\n",
    "trips_df = trips_df_info.merge(trip_and_user,left_index=True,right_on='tripid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.to_csv(fp/'all_attrs.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of visualizing here visualize elsewhere?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import stochastic_optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = Path.home() / 'Documents/BikewaySimData/Projects/gdot'\n",
    "# #fp = Path.home() / 'Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/BikewaySim/Data'\n",
    "\n",
    "# with (fp / 'impedance_calibration.pkl').open('rb') as fh:\n",
    "#     (df_edges,pseudo_df,pseudo_G) = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve link/turn costs\n",
    "# default below is link distance\n",
    "# link_costs = dict(zip(list(zip(df_edges['source'],df_edges['target'],df_edges['linkid'])),df_edges['length_ft']))\n",
    "# tup = list(zip(pseudo_df['source_A'],pseudo_df['source_B'],pseudo_df['source_linkid']))\n",
    "# pseudo_df['source_cost'] = list(map(link_costs.get,tup))\n",
    "# tup = list(zip(pseudo_df['target_A'],pseudo_df['target_B'],pseudo_df['target_linkid']))\n",
    "\n",
    "# pseudo_df['target_cost'] = list(map(link_costs.get,tup))\n",
    "# pseudo_df['total_cost'] = pseudo_df['source_cost'] + pseudo_df['target_cost'] #+turn_cost\n",
    "\n",
    "# costs = pseudo_df.groupby(['source','target'])['total_cost'].min()\n",
    "# nx.set_edge_attributes(pseudo_G,values=costs,name='weight')\n",
    "# source = list(pseudo_G.nodes())[0]\n",
    "# target = list(pseudo_G.nodes())[420]\n",
    "# print(source,target)\n",
    "# import networkx as nx\n",
    "# length, edge_list = nx.single_source_dijkstra(pseudo_G,source,target,weight=\"weight\")\n",
    "# turn_list = [[edge_list[i][0],edge_list[i][1],edge_list[i+1][0],edge_list[i+1][1]] for i in range(len(edge_list)-1)]\n",
    "\n",
    "# turn_cols = ['turn_type','signalized_left_straight','unsignalized_left_straight_nonlocal']\n",
    "# linkid_cols = ['source_linkid','source_reverse_link','target_linkid','target_reverse_link']\n",
    "# chosen_turns = pseudo_df.set_index(['source_A','source_B','target_A','target_B'],drop=False).loc[turn_list,linkid_cols+turn_cols]\n",
    "\n",
    "# tripid = 302\n",
    "\n",
    "# #make a single row dataframe to attach to trips_df\n",
    "# stats_dict = {}\n",
    "# stats_dict[tripid] = {\n",
    "#     'tripid':tripid,\n",
    "#     'signalized_left_straight': chosen_turns['signalized_left_straight'].sum(),\n",
    "#     'unsignalized_left_straight_nonlocal': chosen_turns['unsignalized_left_straight_nonlocal'].sum()\n",
    "# }\n",
    "# turn_dict = chosen_turns['turn_type'].value_counts().to_dict()\n",
    "# stats_dict[tripid].update(turn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1: Need to run shortest paths to create link sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_links = chosen_turns[['source_linkid','source_reverse_link']]\n",
    "# target_links = chosen_turns[['target_linkid','target_reverse_link']]\n",
    "# source_links.columns = ['linkid','reverse_link']\n",
    "# target_links.columns = ['linkid','reverse_link']\n",
    "# linkids = pd.concat([source_links,target_links],ignore_index=True).drop_duplicates()\n",
    "# chosen_links = df_edges.merge(linkids,on=['linkid','reverse_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO recalculate bearing\n",
    "#create pseudo graph for modeling turns\n",
    "df_edges, pseudo_links, pseudo_G = modeling_turns.create_pseudo_dual_graph(links,'A','B','linkid','oneway')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = list(pseudo_G.nodes())[0]\n",
    "target = list(pseudo_G.nodes())[420]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = (68209677, 68209675)\n",
    "target = (69200243, 69465418)\n",
    "\n",
    "import networkx as nx\n",
    "length, path = nx.single_source_dijkstra(pseudo_G,source,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = [(linkids[i],linkids[i+1]) for i in range(len(linkids)-1)]\n",
    "edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = [(*path[i],*path[i+1]) for i in range(len(path)-1)]\n",
    "edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_links.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_links.set_index(['source_A','source_B','target_A','target_B']).loc[edge_list,'turn_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
