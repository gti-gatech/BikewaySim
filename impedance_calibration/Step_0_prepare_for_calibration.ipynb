{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Matched Traces for Calibration\n",
    "- Decide which trace to keep using match_ratio\n",
    "- Determine which links should be included for routing (+ include all links found during map matching)\n",
    "- Calculate shortest paths on the impedance network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from shapely.ops import MultiLineString, LineString\n",
    "import geopandas as gpd\n",
    "\n",
    "from bikewaysim.paths import config\n",
    "from bikewaysim.impedance_calibration import speedfactor, stochastic_optimization\n",
    "from bikewaysim.map_matching import map_match\n",
    "from bikewaysim.network import prepare_network, modeling_turns\n",
    "from bikewaysim.routing import rustworkx_routing_funcs\n",
    "from bikewaysim.map_matching import post_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the match map results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the available match dicts\n",
    "print([x.stem for x in config['matching_fp'].glob('match_dict_full_*.pkl')])\n",
    "matching_index = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['matching_fp'] / f'match_dict_full_{matching_index}.pkl').open('rb') as fh:\n",
    "    match_dict = pickle.load(fh)\n",
    "\n",
    "cutoff = 0.90 # set pct of points that need to be matched\n",
    "above_threshold, below_threshold, failed_matches, match_ratios = post_process.mapmatch_results(match_dict,cutoff)\n",
    "match_dict = {key:item for key,item in match_dict.items() if key in above_threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get route attributes\n",
    "trips = pd.read_pickle(config['cycleatl_fp']/'trips_4.pkl')\n",
    "trips = trips.loc[trips['tripid'].isin(list(match_dict.keys()))]#,'userid'].nunique()\n",
    "users = pd.read_pickle(config['cycleatl_fp']/'users_4.pkl')\n",
    "users = users[users['userid'].isin(set(trips['userid'].tolist()))]\n",
    "print(trips.shape[0],'trips')\n",
    "print(users.shape[0],'users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group trips to users for the user calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_for_calibration_users = [(userid,list(trip_ids)) for userid, trip_ids in trips.groupby('userid')['tripid'].unique().reset_index().values]\n",
    "with (config['calibration_fp'] / 'read_for_calibration_users.pkl').open('wb') as fh:\n",
    "    pickle.dump(ready_for_calibration_users,fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Exceptions\n",
    "Get list of all links used in map matching to make sure these are kept in the calibration network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_matching_links = set()\n",
    "for tripid, items in match_dict.items():\n",
    "    map_matching_links.update(set([tuple(x) for x in items['edges'].values]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create calibration network\n",
    "Create dummy variables and make any other changes that weren't done in the final network export step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directed_links = pd.read_parquet(config['network_fp']/'directed_edges.parquet') # has the directional variables\n",
    "links = gpd.read_file(config['network_fp']/'final_network.gpkg',layer='edges') # has the non-directional variables\n",
    "nodes = gpd.read_file(config['network_fp']/'final_network.gpkg',layer='nodes')\n",
    "nodes = dict(zip(nodes['N'],nodes.geometry))\n",
    "turns = pd.read_parquet(config['network_fp']/'turns_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with links\n",
    "link_cols_drop = ['A','B','ascent_ft','ascent_grade_cat','descent_ft','descent_grade_cat','facility_fwd','facility_rev']\n",
    "links.drop(columns=link_cols_drop,inplace=True)\n",
    "directed_cols_to_add = ['linkid','A','B','reverse_link','ascent_ft','ascent_grade_cat','facility_fwd']\n",
    "links = pd.merge(links,directed_links[directed_cols_to_add],on='linkid')\n",
    "# del directed_links\n",
    "# links.rename(columns={'source':'A','target':'B'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove wrongway\n",
    "oneway_dict = dict(zip(links['linkid'],links['oneway']))\n",
    "turns['source_oneway'] = turns['source_linkid'].map(oneway_dict)\n",
    "turns['target_oneway'] = turns['target_linkid'].map(oneway_dict)\n",
    "del oneway_dict\n",
    "\n",
    "source_exception = [(linkid,reverse_link) in map_matching_links for linkid, reverse_link in turns[['source_linkid','source_reverse_link']].values]\n",
    "target_exception = [(linkid,reverse_link) in map_matching_links for linkid, reverse_link in turns[['target_linkid','target_reverse_link']].values]\n",
    "# source_wrongway = ((turns['source_oneway'] == True) & (turns['source_reverse_link'] == True)) == False\n",
    "# target_wrongway = ((turns['target_oneway'] == True) & (turns['target_reverse_link'] == True)) == False\n",
    "source_wrongway = ((turns[['source_oneway','source_reverse_link']] == True).all(axis=1)==False) | (source_exception)\n",
    "target_wrongway = ((turns[['target_oneway','target_reverse_link']] == True).all(axis=1)==False) | (target_exception)\n",
    "turns = turns[source_wrongway & target_wrongway]\n",
    "\n",
    "#remove wrongway links\n",
    "#TODO did we remove these in the export network step too?\n",
    "exception = [(linkid,reverse_link) in map_matching_links for linkid, reverse_link in links[['linkid','reverse_link']].values]\n",
    "links = links.loc[((links[['oneway','reverse_link']]==True).all(axis=1) == False) | exception]\n",
    "\n",
    "#TODO post GDOT\n",
    "#add elevation adjusted travel times based on assumed speed on flat ground\n",
    "# speedfactor.calculate_adjusted_speed(links,9)\n",
    "assumed_speed_mph = 9\n",
    "links['travel_time_min'] = (links.length / 5280 / assumed_speed_mph * 60).round(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy variables for modeling\n",
    "links['2lpd'] = (links['lanes'] == 2).astype(int)\n",
    "links['3+lpd'] = (links['lanes'] == 3).astype(int)\n",
    "links['(30,40] mph'] = (links['speed']=='(30,40]').astype(int)\n",
    "links['(40,inf) mph'] = (links['speed']=='(40,inf)').astype(int)\n",
    "links['[4k,10k) aadt'] = (links['AADT']=='[4k,10k)').astype(int)\n",
    "links['[10k,inf) aadt'] = (links['AADT']=='[10k,inf)').astype(int)\n",
    "links['[4,6) grade'] = (links['ascent_grade_cat']=='[4,6)').astype(int)\n",
    "links['[6,inf) grade'] = (links['ascent_grade_cat']=='[6,inf)').astype(int)\n",
    "links['bike lane'] = links['facility_fwd'].isin(['bike lane','bufferred bike lane']).astype(int)\n",
    "links['cycletrack'] = links['facility_fwd'].isin(['cycletrack']).astype(int)\n",
    "links['multi use path'] = links['facility_fwd'].isin(['multi use path']).astype(int)\n",
    "\n",
    "#condensed variables\n",
    "links['(30,inf) mph'] = (links[['(30,40] mph','(40,inf) mph']] == 1).any(axis=1)\n",
    "links['multi use path and cycletrack'] = (links[['cycletrack','multi use path']] == 1).any(axis=1)\n",
    "\n",
    "#TODO add sidepath variables here\n",
    "\n",
    "# the report variables\n",
    "links0 = links.copy()\n",
    "links0['multi use path report'] = links0['facility_fwd'].isin(['multi use path','cycletrack']).astype(int)\n",
    "links0['bike lane report'] = links0['facility_fwd'].isin(['bike lane','bufferred bike lane']).astype(int)\n",
    "links0.loc[(links0['multi use path report']==True) | (links['link_type'].isin(['bike','pedestrian','sidewalk'])),'lanes'] = 0\n",
    "links0\n",
    "links0['above_4 report'] = links0['ascent_grade_cat'].isin(['[4,6)','[6,inf)'])\n",
    "links0 = links0[['linkid','reverse_link','multi use path report','bike lane report','lanes','above_4 report']]\n",
    "links = pd.merge(links,links0,suffixes=('',' report'),on=['linkid','reverse_link'])\n",
    "\n",
    "turns.loc[turns['unsig_crossing'].isna(),'unsig_crossing'] = False\n",
    "turns['unsig_crossing'] = turns['unsig_crossing'].astype(int)\n",
    "\n",
    "# #create layer of unsignalized crossings for examining (#NOTE I think this is a duplicate)\n",
    "# unsig__crossing = set(turns.loc[turns['unsig__crossing']==True,'source_B'].tolist())\n",
    "# nodes = gpd.read_file(config['network_fp']/'final_network.gpkg',layer='nodes')\n",
    "# nodes = nodes[nodes['N'].isin(unsig__crossing)]\n",
    "# nodes.to_file(config['calibration_fp']/'unsig__crossing.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only allow these types for routing\n",
    "link_types_allowed = ['bike','pedestrian','road']\n",
    "exception = [(linkid,reverse_link) in map_matching_links for linkid, reverse_link in links[['linkid','reverse_link']].values]\n",
    "links = links[links['link_type'].isin(link_types_allowed) | exception]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = links.copy()\n",
    "exception = [(linkid,reverse_link) in map_matching_links for linkid, reverse_link in before[['linkid','reverse_link']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove isolated links\n",
    "links, turns = prepare_network.remove_isolates(links,turns)\n",
    "\n",
    "print(links.shape[0],'links and',len(set(links['A'].append(links['B']).tolist())),'nodes')\n",
    "print(turns.shape[0],'turns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure this out later\n",
    "# assign attributes to the sidepaths\n",
    "sidepaths = gpd.read_file(config['bicycle_facilities_fp']/'sidepaths.gpkg',layer='sidepaths',ignore_geometry=True)[['linkid','sidepath_linkid']]\n",
    "replace = sidepaths.merge(links,on='linkid')\n",
    "replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all the attributes\n",
    "all_attrs = ['2lpd', '3+lpd', '(30,40] mph', '(40,inf) mph',\n",
    "       '[4k,10k) aadt', '[10k,inf) aadt', '[4,6) grade', '[6,inf) grade',\n",
    "       'bike lane', 'cycletrack', 'multi use path', '(30,inf) mph',\n",
    "       'multi use path and cycletrack', 'multi use path report',\n",
    "       'bike lane report', 'lanes report', 'above_4 report']\n",
    "replace = replace[['sidepath_linkid']+all_attrs]\n",
    "replace = replace[replace['sidepath_linkid'].duplicated()==False]\n",
    "replace\n",
    "\n",
    "links = pd.merge(links,replace,left_on='linkid',right_on='sidepath_linkid',suffixes=(None,'_new'),how='left')\n",
    "for col in replace.columns:\n",
    "    if col != 'sidepath_linkid':\n",
    "        links[col] = links[f'{col}_new'].fillna(links[col])\n",
    "links.drop(columns=[x+'_new' for x in all_attrs],inplace=True)\n",
    "# replace\n",
    "#[['multi use path','sidepath']]\n",
    "# links.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE for the set inf variable\n",
    "links['not_street'] = links['link_type'] != 'road'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export calibration network\n",
    "with (config['calibration_fp']/\"calibration_network.pkl\").open('wb') as fh:\n",
    "    pickle.dump((links,turns),fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links.to_file(config['calibration_fp']/'calibration_network.gpkg',layer='final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without_isolates = set([tuple([x,y]) for x,y in links[['linkid','reverse_link']].values])\n",
    "# missing = [tuple([x,y]) not in without_isolates for x,y in before[['linkid','reverse_link']].values]\n",
    "# before[np.array(exception) & np.array(missing)].explore()#.to_file(Path.home()/'Downloads/')\n",
    "# ((links[['linkid','reverse_link']]==(35062.0,False)).all(axis=1)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble the match data for shortest path routing and calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links, turns, length_dict, geo_dict, turn_G = stochastic_optimization.import_calibration_network(config)\n",
    "# base_impedance_col = \"travel_time_min\"\n",
    "# stochastic_optimization.back_to_base_impedance(base_impedance_col,links,turns,turn_G)\n",
    "# links.set_index(['linkid','reverse_link'],inplace=True,drop=False)\n",
    "# match_results = {}\n",
    "# #shortest_results = {}\n",
    "# failed_shortest_path = []\n",
    "# for tripid, items in tqdm(match_dict.items()):\n",
    "\n",
    "#     #get start and end linkid\n",
    "#     start = tuple(match_dict[tripid]['edges'].iloc[0,:].values)\n",
    "#     end = tuple(match_dict[tripid]['edges'].iloc[-1,:].values)\n",
    "\n",
    "#     #get start and end node for shortest and impedance routing\n",
    "#     start = links.loc[start,'A']\n",
    "#     end = links.loc[end,'B']\n",
    "\n",
    "#     match_results[tripid] = {\n",
    "#     'origin_node': start,\n",
    "#     'destination_node': end,\n",
    "#     'trip_start_time': items['trace'].iloc[0,2].year,\n",
    "#     'match_ratio': items['match_ratio'],\n",
    "#     'matched_edges': match_dict[tripid]['edges'],\n",
    "#     'shortest_edges': pd.DataFrame(stochastic_optimization.impedance_path(turns,turn_G,links,start,end)['edge_list'],columns=['linkid','reverse_link'])\n",
    "#     }\n",
    "# # trip_ods = pd.DataFrame.from_dict(match_results,orient='index')\n",
    "# # trip_ods.reset_index(inplace=True)\n",
    "# # trip_ods.rename(columns={'index':'tripid'},inplace=True)\n",
    "# #export for impedance calibration\n",
    "# with (config['calibration_fp']/'ready_for_calibration.pkl').open('wb') as fh:\n",
    "#     pickle.dump(match_results,fh)\n",
    "# # links.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(rustworkx_routing_funcs)\n",
    "links, turns, length_dict, geo_dict, turn_G = rustworkx_routing_funcs.import_calibration_network(config)\n",
    "base_impedance_col = \"travel_time_min\"\n",
    "rustworkx_routing_funcs.back_to_base_impedance(base_impedance_col,links,turns,turn_G)\n",
    "links.set_index(['linkid','reverse_link'],inplace=True,drop=False)\n",
    "match_results = {}\n",
    "#shortest_results = {}\n",
    "failed_shortest_path = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rustworkx verison\n",
    "reload(post_process)\n",
    "starts, ends = post_process.get_ods_from_match_dict(match_dict,links)\n",
    "\n",
    "added_nodes = rustworkx_routing_funcs.add_virtual_edges(starts,ends,links,turns,turn_G)\n",
    "\n",
    "import numpy as np\n",
    "time_periods = []\n",
    "link_dates = sorted(links['year'].dropna().unique())[::-1]\n",
    "for trip_date in [items['trace'].iloc[0,2].year for tripid, items in match_dict.items()]:\n",
    "    cond = (trip_date < np.array(link_dates))\n",
    "    if cond.all():\n",
    "        print('trip is before any built infrastructure')\n",
    "        continue\n",
    "    idx = cond.argmin() # grabs the closest year in descending order\n",
    "    time_periods.append(link_dates[idx])\n",
    "\n",
    "shortest_lengths, shortest_edges = rustworkx_routing_funcs.rx_shortest_paths(list(zip(starts,ends)),turn_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tripid, start, end, year, shortest_edge in zip(match_dict.keys(),starts,ends,time_periods,shortest_edges):\n",
    "    match_results[tripid] = {\n",
    "        'origin_node': start,\n",
    "        'destination_node': end,\n",
    "        'trip_start_time': year,\n",
    "        'match_ratio': match_dict[tripid]['match_ratio'],\n",
    "        'matched_edges': match_dict[tripid]['edges'],\n",
    "        'shortest_edges': pd.DataFrame(shortest_edge,columns=['linkid','reverse_link'])\n",
    "    }\n",
    "\n",
    "with (config['calibration_fp']/'ready_for_calibration.pkl').open('wb') as fh:\n",
    "    pickle.dump(match_results,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     match_results[tripid] = {\n",
    "#     'origin_node': start,\n",
    "#     'destination_node': end,\n",
    "#     'trip_start_time': items['trace'].iloc[0,2].year,\n",
    "#     'match_ratio': items['match_ratio'],\n",
    "#     'matched_edges': match_dict[tripid]['edges'],\n",
    "#     'shortest_edges': pd.DataFrame(stochastic_optimization.impedance_path(turns,turn_G,links,start,end)['edge_list'],columns=['linkid','reverse_link'])\n",
    "#     }\n",
    "# trip_dates = sorted(list(set(trip_start_time)))[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goes throught the trips and sorts them according to what date network they should belong to\n",
    "For example, a trip in 2017 would be assigned to the 2017 or below (so if the last infra change was in 2016 it will get assigned to that)\n",
    "\n",
    "if a trip, was before any of the infra, return an error becauses something went wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turns_G_dict = {}\n",
    "for year in time_periods.keys():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "(trip_dates[0] < np.array(link_dates)).argmin() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_dates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(set(trip_start_time)))[-1::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many networks need to be made\n",
    "years = sorted(list(set.intersection(set(links['year'].unique().tolist()),set(trip_start_time))))[-1::]\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_networks = {}\n",
    "\n",
    "#assign which network to use for trips\n",
    "\n",
    "#create year networks\n",
    "year_networks = {} # keys are the year and items are the network to use for routing\n",
    "\n",
    "\n",
    "def adjust_costs_w_year(year,links,turn_G,set_to_zero=[],set_to_inf=[]):\n",
    "    # make a copy of the network\n",
    "    turn_G = turn_G.copy()\n",
    "    # if infra is on street (i.e., the link is still traversable but the impedance doesn't apply)\n",
    "    links.loc[links['year'] > year,set_to_zero] = 0 \n",
    "    # if it's off-street then assign it a very high cost\n",
    "    links.loc[(links['year'] > year) & (links.loc[:,set_to_inf]==1).any(axis=1),'link_cost_override'] = True\n",
    "    # update impedances\n",
    "    rustworkx_routing_funcs.impedance_update(betas,betas_tup,\n",
    "            link_impedance_function,\n",
    "            base_impedance_col,\n",
    "            turn_impedance_function,\n",
    "            links,turns,turn_G)\n",
    "    # re-add virtual links\n",
    "    added_nodes = rustworkx_routing_funcs.add_virtual_edges(starts,ends,links,turns,turn_G)         \n",
    "    return turn_G\n",
    "\n",
    "\n",
    "\n",
    "for year in years\n",
    "    turn_G_copy = turn_G.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_start_time = [items['trace'].iloc[0,2].year for tripid, items in match_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(match_dict.keys(),starts,ends,trip_start_times,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    match_results[tripid] = {\n",
    "    'origin_node': start,\n",
    "    'destination_node': end,\n",
    "    'trip_start_time': items['trace'].iloc[0,2].year,\n",
    "    'match_ratio': items['match_ratio'],\n",
    "    'matched_edges': match_dict[tripid]['edges'],\n",
    "    'shortest_edges': pd.DataFrame(\n",
    "    }\n",
    "# trip_ods = pd.DataFrame.from_dict(match_results,orient='index')\n",
    "# trip_ods.reset_index(inplace=True)\n",
    "# trip_ods.rename(columns={'index':'tripid'},inplace=True)\n",
    "#export for impedance calibration\n",
    "with (config['calibration_fp']/'ready_for_calibration.pkl').open('wb') as fh:\n",
    "    pickle.dump(match_results,fh)\n",
    "# links.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add this to the export network section\n",
    "# # add this for later\n",
    "# link_types = dict(zip(links['linkid'],links['link_type']))\n",
    "# turns['source_link_type'] = turns['source_linkid'].map(link_types)\n",
    "# turns['target_link_type'] = turns['source_linkid'].map(link_types)\n",
    "\n",
    "# # #unit conversions\n",
    "# links['length_mi'] = (links['length_ft'] / 5280).round(2)\n",
    "# links['ascent_ft'] = (links['ascent_m'] * 3.28084).round(0)\n",
    "# #links.drop(columns=['length_ft','ascent_m'],inplace=True)\n",
    "\n",
    "# #get node degree\n",
    "# degree = links['A'].append(links['B']).value_counts()\n",
    "# links['A_deg'] = links['A'].map(degree)\n",
    "# links['B_deg'] = links['B'].map(degree)\n",
    "# #remove excess dead end pedestrian links\n",
    "# dead_ends = (links['link_type']=='pedestrian')&((links['A_deg']==1)|(links['B_deg']==1))\n",
    "# links = links[dead_ends==False]\n",
    "# #unique scenario but there's an expressway tag that needs to be removed\n",
    "# import ast\n",
    "# john_lewis_freedom_pkwy = links['all_tags'].apply(lambda x: ast.literal_eval(x).get('expressway',0)=='yes')\n",
    "# links = links[john_lewis_freedom_pkwy==False]\n",
    "# surfaces = ['dirt','unpaved','gravel','fine_gravel','dirt/sand','ground']\n",
    "# unpaved = links['all_tags'].apply(lambda x: ast.literal_eval(x).get('surface',0) in surfaces)\n",
    "# #links[unpaved].explore(tooltip=False)\n",
    "# links = links[unpaved==False]\n",
    "# #unpaved.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move on after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     # #euclidean distance between points\n",
    "#     # snode = nodes.loc[nodes['N']==start,'geometry'].item()\n",
    "#     # dnode = nodes.loc[nodes['N']==end,'geometry'].item()\n",
    "    \n",
    "#     # #add geo features\n",
    "#     # edge_geo = pd.merge(match_dict[tripid]['edges'],edges[['linkid','geometry']],on=['linkid'],how='left')\n",
    "#     # edge_geo = gpd.GeoDataFrame(edge_geo,geometry='geometry')\n",
    "#     # edge_geo_dissolved = MultiLineString(edge_geo['geometry'].tolist())\n",
    "#     # linkids = set(edge_geo['linkid'].tolist())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     #TODO use .array version to get rid of errors\n",
    "\n",
    "#     forward = pd.merge(edge_df,edges[['source','target','linkid','geometry']],on=['source','target'])[['linkid','geometry']]\n",
    "#     reverse = pd.merge(edge_df,edges[['source','target','linkid','geometry']],left_on=['target','source'],right_on=['source','target'])[['linkid','geometry']]\n",
    "#     shortest_path = pd.concat([forward,reverse],ignore_index=True)\n",
    "#     shortest_linkids = set(shortest_path['linkid'].tolist())\n",
    "#     shortest_geo = gpd.GeoDataFrame(shortest_path)\n",
    "#     shortest_geo_dissolved = MultiLineString(shortest_geo['geometry'].tolist())\n",
    "\n",
    "#     #exact overlap\n",
    "#     chosen_and_shortest = linkids & shortest_linkids\n",
    "#     overlap_length = edges.set_index('linkid').loc[list(chosen_and_shortest)]['length_ft'].sum()\n",
    "#     exact_overlap = overlap_length / edge_geo.length.sum()\n",
    "\n",
    "#     #buffer overlap\n",
    "#     buffer_ft = 500\n",
    "#     chosen = edge_geo_dissolved.buffer(buffer_ft)\n",
    "#     shortest = shortest_geo_dissolved.buffer(buffer_ft)\n",
    "#     intersection = chosen.intersection(shortest)\n",
    "#     buffer_overlap = intersection.area / (chosen.area + shortest.area - intersection.area)\n",
    "\n",
    "#     #collapse to multilinestring with length\n",
    "#     #add length\n",
    "#     matched_trips[tripid] = {'start':start,\n",
    "#                             'end':end,\n",
    "#                             'start_end_dist_ft': snode.distance(dnode),\n",
    "#                             'match_ratio': match_dict[tripid]['match_ratio'], \n",
    "#                             'linkids':str(linkids),\n",
    "#                             'geometry':edge_geo_dissolved,\n",
    "#                             'length_ft':edge_geo.length.sum(),\n",
    "#                             'shortest_length_ft': impedance,\n",
    "#                             'shortest_linkids': shortest_linkids,\n",
    "#                             'shortest_geo': shortest_geo_dissolved,\n",
    "#                             'shortest_exact_overlap_length': overlap_length,\n",
    "#                             'shortest_exact_overlap_prop': exact_overlap,\n",
    "#                             'shortest_buffer_overlap': buffer_overlap,\n",
    "#                             'shortest_intersect_geo': intersection\n",
    "#                             }\n",
    "# # matched_trips = {}\n",
    "# # loop_trips = []\n",
    "\n",
    "# # for tripid,items in tqdm(match_dict.items()):\n",
    "\n",
    "# #     #failed matches will be str type\n",
    "# #     if isinstance(items,dict):\n",
    "\n",
    "# #         #get start and end linkid\n",
    "# #         start = match_dict[tripid]['edges'].iloc[0,:]\n",
    "# #         end = match_dict[tripid]['edges'].iloc[-1,:]\n",
    "        \n",
    "# #         #get start and end node\n",
    "# #         start_a_b = edges.loc[(edges['linkid']==start['linkid']) & (edges['reverse_link']==start['reverse_link']),['source','target']]\n",
    "# #         end_a_b = edges.loc[(edges['linkid']==end['linkid']) & (edges['reverse_link']==end['reverse_link']),['source','target']]\n",
    "\n",
    "# #         if start['reverse_link']:\n",
    "# #             start = start_a_b['source'].item()\n",
    "# #         else:\n",
    "# #             start = start_a_b['target'].item()\n",
    "\n",
    "# #         if end['reverse_link']:\n",
    "# #             end = end_a_b['target'].item()\n",
    "# #         else:\n",
    "# #             end = end_a_b['source'].item()\n",
    "\n",
    "# #         #euclidean distance between points\n",
    "# #         snode = nodes.loc[nodes['N']==start,'geometry'].item()\n",
    "# #         dnode = nodes.loc[nodes['N']==end,'geometry'].item()\n",
    "        \n",
    "# #         #add geo features\n",
    "# #         edge_geo = pd.merge(match_dict[tripid]['edges'],edges[['linkid','geometry']],on=['linkid'],how='left')\n",
    "# #         edge_geo = gpd.GeoDataFrame(edge_geo,geometry='geometry')\n",
    "# #         edge_geo_dissolved = MultiLineString(edge_geo['geometry'].tolist())\n",
    "# #         linkids = set(edge_geo['linkid'].tolist())\n",
    "\n",
    "# #         #shortest path routing here\n",
    "# #         impedance, path = nx.single_source_dijkstra(MDG,start,end,weight=\"length_ft\")\n",
    "        \n",
    "# #         if len(path) < 2:\n",
    "# #             loop_trips.append(tripid)\n",
    "# #             continue\n",
    "\n",
    "# #         #turn to edge list\n",
    "# #         edge_list = [(path[i],path[i+1]) for i in range(len(path)-1)]\n",
    "# #         edge_df = pd.DataFrame(edge_list,columns=['source','target'])\n",
    "\n",
    "# #         #TODO use .array version to get rid of errors\n",
    "\n",
    "# #         forward = pd.merge(edge_df,edges[['source','target','linkid','geometry']],on=['source','target'])[['linkid','geometry']]\n",
    "# #         reverse = pd.merge(edge_df,edges[['source','target','linkid','geometry']],left_on=['target','source'],right_on=['source','target'])[['linkid','geometry']]\n",
    "# #         shortest_path = pd.concat([forward,reverse],ignore_index=True)\n",
    "# #         shortest_linkids = set(shortest_path['linkid'].tolist())\n",
    "# #         shortest_geo = gpd.GeoDataFrame(shortest_path)\n",
    "# #         shortest_geo_dissolved = MultiLineString(shortest_geo['geometry'].tolist())\n",
    "\n",
    "# #         #exact overlap\n",
    "# #         chosen_and_shortest = linkids & shortest_linkids\n",
    "# #         overlap_length = edges.set_index('linkid').loc[list(chosen_and_shortest)]['length_ft'].sum()\n",
    "# #         exact_overlap = overlap_length / edge_geo.length.sum()\n",
    "\n",
    "# #         #buffer overlap\n",
    "# #         buffer_ft = 500\n",
    "# #         chosen = edge_geo_dissolved.buffer(buffer_ft)\n",
    "# #         shortest = shortest_geo_dissolved.buffer(buffer_ft)\n",
    "# #         intersection = chosen.intersection(shortest)\n",
    "# #         buffer_overlap = intersection.area / (chosen.area + shortest.area - intersection.area)\n",
    "\n",
    "# #         #collapse to multilinestring with length\n",
    "# #         #add length\n",
    "# #         matched_trips[tripid] = {'start':start,\n",
    "# #                               'end':end,\n",
    "# #                               'start_end_dist_ft': snode.distance(dnode),\n",
    "# #                               'match_ratio': match_dict[tripid]['match_ratio'], \n",
    "# #                               'linkids':str(linkids),\n",
    "# #                               'geometry':edge_geo_dissolved,\n",
    "# #                               'length_ft':edge_geo.length.sum(),\n",
    "# #                               'shortest_length_ft': impedance,\n",
    "# #                               'shortest_linkids': shortest_linkids,\n",
    "# #                               'shortest_geo': shortest_geo_dissolved,\n",
    "# #                               'shortest_exact_overlap_length': overlap_length,\n",
    "# #                               'shortest_exact_overlap_prop': exact_overlap,\n",
    "# #                               'shortest_buffer_overlap': buffer_overlap,\n",
    "# #                               'shortest_intersect_geo': intersection\n",
    "# #                               }\n",
    "# Want to display when a trip goes through a signalized intersection and also how many times they do it. Need to take the list of edges from the matched_traces_dict and contruct a list of turns from it. This list of turns can then be used to get the right node ids. Later turn this into a function.\n",
    "# tripid = 4100\n",
    "# edges = match_dict[tripid]['edges']\n",
    "\n",
    "# # make list of edges and turns\n",
    "# list_of_edges = list(zip(edges['linkid'],edges['reverse_link']))\n",
    "# df_edges['tup'] = list(zip(df_edges['linkid'],df_edges['reverse_link']))\n",
    "# chosen_links = df_edges.set_index('tup').loc[list_of_edges]\n",
    "# list_of_turns = [(list_of_edges[i][0],list_of_edges[i][1],list_of_edges[i+1][0],list_of_edges[i+1][1]) for i in range(0,len(list_of_edges)-1)]\n",
    "# df_of_turns = pd.DataFrame(list_of_turns,columns=['source_linkid','source_reverse_link','target_linkid','target_reverse_link'])\n",
    "# df_of_turns\n",
    "# subset = pseudo_df.merge(df_of_turns,on=['source_linkid','source_reverse_link','target_linkid','target_reverse_link'])\n",
    "\n",
    "# # get list of nodes\n",
    "# signals = subset.loc[subset['signalized']==True,'source_B'].value_counts()\n",
    "# two_way_stops = subset.loc[subset['unsignalized']==True,'source_B'].value_counts()\n",
    "\n",
    "# #get node coordinates\n",
    "# #nodes.merge(signals,left_on='N',right_index=True)\n",
    "\n",
    "# test = nodes.merge(signals,left_on='N',right_index=True)\n",
    "# test.columns = ['N','geometry','num_times']\n",
    "\n",
    "\n",
    "# # now value counts \n",
    "# #two_way_stops.value_counts().head(20)\n",
    "# ## Use linkids to add network summaries \n",
    "\n",
    "# #turn into dataframe\n",
    "# df = pd.DataFrame.from_dict(matched_trips,orient='index')\n",
    "# #into geodataframe\n",
    "# gdf = gpd.GeoDataFrame(df,geometry='geometry',crs='epsg:2240')\n",
    "\n",
    "# gdf.reset_index(inplace=True)\n",
    "# gdf.rename(columns={'index':'tripid'},inplace=True)\n",
    "\n",
    "# test_merge = pd.read_csv(config['network_fp'].parent/'all_attrs.csv')\n",
    "# prev = gdf.copy()\n",
    "# gdf = gdf.merge(test_merge,on='tripid')\n",
    "# gdf\n",
    "# def visualize(tripid,gdf,nodes):\n",
    "\n",
    "#    '''\n",
    "#    This function displays the matched vs shortest route for a particular trip\n",
    "#    It also displays the trip characteristics side be side and plots the any signalized\n",
    "#    intersections and stressful turns passed through.\n",
    "#    '''\n",
    "\n",
    "#    #gdf contains all the trips and the trip gemometries as mutlilinestrings\n",
    "#    gdf = gdf.copy()\n",
    "\n",
    "#    # Your GeoDataFrames\n",
    "#    chosen_path = gdf.loc[gdf['tripid']==tripid,['tripid','geometry']]\n",
    "#    shortest_path = gdf.loc[gdf['tripid']==tripid,['tripid','shortest_geo']].set_geometry('shortest_geo').set_crs(gdf.crs)\n",
    "#    intersection = gdf.loc[gdf['tripid']==tripid,['tripid','shortest_intersect_geo']].set_geometry('shortest_intersect_geo').set_crs(gdf.crs)\n",
    "\n",
    "#    #from these we want to get the locations and number of singalized intersections and stressful crossing passed through\n",
    "#    edges = match_dict[tripid]['edges']\n",
    "#    list_of_edges = list(zip(edges['linkid'],edges['reverse_link']))\n",
    "#    list_of_turns = [(list_of_edges[i][0],list_of_edges[i][1],list_of_edges[i+1][0],list_of_edges[i+1][1]) for i in range(0,len(list_of_edges)-1)]\n",
    "#    df_of_turns = pd.DataFrame(list_of_turns,columns=['source_linkid','source_reverse_link','target_linkid','target_reverse_link'])\n",
    "#    subset = pseudo_df.merge(df_of_turns,on=['source_linkid','source_reverse_link','target_linkid','target_reverse_link'])\n",
    "\n",
    "#    # from this subset we can get the right node ids\n",
    "#    #TODO turns should be by edges probably?\n",
    "#    #turns = subset[['source_B','turn_type']]\n",
    "#    signals = subset.loc[subset['signalized']==True,'source_B'].value_counts()\n",
    "#    two_way_stops = subset.loc[subset['unsignalized']==True,'source_B'].value_counts()\n",
    "\n",
    "#    #and then get the correct rows of the gdf\n",
    "#    #turns = nodes.merge(signals,left_on='N',right_on='')\n",
    "#    signals = nodes.merge(signals,left_on='N',right_index=True)\n",
    "#    signals.columns = ['N','geometry','num_times']\n",
    "#    two_way_stops = nodes.merge(two_way_stops,left_on='N',right_index=True)\n",
    "#    two_way_stops.columns = ['N','geometry','num_times']\n",
    "\n",
    "#    # get the start and end point for plotting\n",
    "#    start_N = gdf.loc[gdf['tripid']==tripid,'start'].item()\n",
    "#    start_pt = nodes.to_crs('epsg:4326').loc[nodes['N']==start_N,'geometry'].item()\n",
    "#    end_N = gdf.loc[gdf['tripid']==tripid,'end'].item()\n",
    "#    end_pt = nodes.to_crs('epsg:4326').loc[nodes['N']==end_N,'geometry'].item()\n",
    "\n",
    "#    # Create a Folium map centered around the mean of the chosen route\n",
    "#    x_mean = chosen_path.to_crs(epsg='4326').geometry.item().centroid.x\n",
    "#    y_mean = chosen_path.to_crs(epsg='4326').geometry.item().centroid.y\n",
    "#    center = [y_mean,x_mean]\n",
    "#    mymap = folium.Map(location=center, zoom_start=14)\n",
    "\n",
    "#    # Convert GeoDataFrames to GeoJSON\n",
    "#    chosen_path_geojson = chosen_path.to_crs(epsg='4326').to_json()\n",
    "#    shortest_path_geojson = shortest_path.to_crs(epsg='4326').to_json()\n",
    "#    intersection_geojson = intersection.to_crs(epsg='4326').to_json()\n",
    "\n",
    "#    # Create FeatureGroups for each GeoDataFrame\n",
    "#    chosen_path_fg = FeatureGroup(name='Chosen Path')\n",
    "#    shortest_path_fg = FeatureGroup(name='Shortest Path',show=False)\n",
    "#    intersection_fg = FeatureGroup(name='Buffer Intersection',show=False)\n",
    "\n",
    "#    # Add GeoJSON data to FeatureGroups\n",
    "#    folium.GeoJson(chosen_path_geojson, name='Chosen Path', style_function=lambda x: {'color': 'red'}).add_to(chosen_path_fg)\n",
    "#    folium.GeoJson(shortest_path_geojson, name='Shortest Path', style_function=lambda x: {'color': 'blue'}).add_to(shortest_path_fg)\n",
    "#    folium.GeoJson(intersection_geojson, name='Buffer Intersection', style_function=lambda x: {'color': 'yellow'}).add_to(intersection_fg)\n",
    "\n",
    "#    # Add FeatureGroups to the map\n",
    "#    chosen_path_fg.add_to(mymap)\n",
    "#    shortest_path_fg.add_to(mymap)\n",
    "#    intersection_fg.add_to(mymap)\n",
    "\n",
    "#    if signals.shape[0] > 0:\n",
    "#       signals_geojson = signals.to_crs(epsg='4326').to_json()\n",
    "#       signals_fg = FeatureGroup(name='Signals')\n",
    "\n",
    "#       folium.GeoJson(\n",
    "#       signals_geojson,\n",
    "#       name=\"Traffic Signal Turn Movement\",\n",
    "#       marker=folium.Circle(radius=20, fill_color=\"red\", fill_opacity=.5, color=\"black\", weight=1),\n",
    "#       tooltip=folium.GeoJsonTooltip(fields=['N','num_times']),\n",
    "#       popup=folium.GeoJsonPopup(fields=['N','num_times']),\n",
    "#       #    style_function= lambda feature: {\n",
    "#       #        'fillColor': colormap(feature['properties']['speed_mph']),\n",
    "#       #    },\n",
    "#       highlight_function=lambda feature: {\"color\":\"yellow\",\"weight\":3}\n",
    "#       ).add_to(signals_fg)\n",
    "#       signals_fg.add_to(mymap)\n",
    "\n",
    "#    if two_way_stops.shape[0] > 0:\n",
    "#       two_way_stops_geojson = two_way_stops.to_crs(epsg='4326').to_json()\n",
    "#       two_way_stops_fg = FeatureGroup(name='Two Way Stop (chosen)')\n",
    "\n",
    "#       folium.GeoJson(\n",
    "#       two_way_stops_geojson,\n",
    "#       name=\"Two Way Stop with High Stress Cross Street\",\n",
    "#       marker=folium.Circle(radius=20, fill_color=\"yellow\", fill_opacity=.5, color=\"black\", weight=1),\n",
    "#       tooltip=folium.GeoJsonTooltip(fields=['N','num_times']),\n",
    "#       popup=folium.GeoJsonPopup(fields=['N','num_times']),\n",
    "#       #    style_function= lambda feature: {\n",
    "#       #        'fillColor': colormap(feature['properties']['speed_mph']),\n",
    "#       #    },\n",
    "#       highlight_function=lambda feature: {\"color\":\"yellow\",\"weight\":3}\n",
    "#       ).add_to(two_way_stops_fg)\n",
    "\n",
    "#       two_way_stops_fg.add_to(mymap)\n",
    "\n",
    "\n",
    "#    # Add start and end points with play and stop buttons\n",
    "#    start_icon = folium.Icon(color='green',icon='play',prefix='fa')\n",
    "#    end_icon = folium.Icon(color='red',icon='stop',prefix='fa')\n",
    "#    folium.Marker(location=[start_pt.y, start_pt.x],icon=start_icon).add_to(mymap)\n",
    "#    folium.Marker(location=[end_pt.y, end_pt.x],icon=end_icon).add_to(mymap)\n",
    "\n",
    "#    #autofit content not in this version?\n",
    "#    #folium.FitOverlays().add_to(mymap)\n",
    "\n",
    "#    # Add layer control to toggle layers on/off\n",
    "#    folium.LayerControl().add_to(mymap)\n",
    "\n",
    "#    #retrive overlap\n",
    "#    exact_overlap = gdf.loc[gdf['tripid']==tripid,'shortest_exact_overlap_prop'].item()\n",
    "#    buffer_overlap = gdf.loc[gdf['tripid']==tripid,'shortest_buffer_overlap'].item()\n",
    "\n",
    "#    attr = gdf.loc[gdf['tripid']==tripid].squeeze()\n",
    "\n",
    "#    # Add legend with statistics\n",
    "#    legend_html = f'''\n",
    "#    <div style=\"position: fixed; \n",
    "#             bottom: 5px; left: 5px; width: 300px; height: 500px; \n",
    "#             border:2px solid grey; z-index:9999; font-size:14px;\n",
    "#             background-color: white;\n",
    "#             opacity: 0.9;\">\n",
    "#    &nbsp; <b>Tripid: {tripid}</b> <br>\n",
    "#    &nbsp; Start Point &nbsp; <i class=\"fa fa-play\" style=\"color:green\"></i><br>\n",
    "#    &nbsp; End Point &nbsp; <i class=\"fa fa-stop\" style=\"color:red\"></i><br>\n",
    "#    &nbsp; Exact Overlap: {exact_overlap*100:.2f}% <br>\n",
    "#    &nbsp; Buffer Overlap: {buffer_overlap*100:.2f}% <br>\n",
    "\n",
    "#    &nbsp; Trip Type: {attr['trip_type']} <br>\n",
    "#    &nbsp; Length (mi): {attr['length_ft']/5280:.0f} <br>\n",
    "#    &nbsp; Age: {attr['age']} <br>\n",
    "#    &nbsp; Gender: {attr['gender']} <br>\n",
    "#    &nbsp; Income: {attr['income']} <br>\n",
    "#    &nbsp; Ethnicity: {attr['ethnicity']} <br>\n",
    "#    &nbsp; Cycling Frequency: {attr['cyclingfreq']} <br>\n",
    "#    &nbsp; Rider History: {attr['rider_history']} <br>\n",
    "#    &nbsp; Rider Type: {attr['rider_type']} <br><br>\n",
    "\n",
    "#    &nbsp; Residential %: {attr['highway.residential']*100:.2f}% <br>\n",
    "#    &nbsp; Secondary %: {attr['highway.secondary']*100:.2f}% <br>\n",
    "#    &nbsp; Tertiary %: {attr['highway.tertiary']*100:.2f}% <br>\n",
    "\n",
    "#    &nbsp; # of bridges: {int(attr['bridge'])} <br>\n",
    "#    &nbsp; # of left turns: {int(attr['left'])} <br>\n",
    "#    &nbsp; # of straight turns: {int(attr['straight'])} <br>\n",
    "#    &nbsp; # of right turns: {int(attr['right'])} <br>\n",
    "#    &nbsp; # of stressful turns: {int(attr['unsignalized'])} <br>\n",
    "#    &nbsp; # of signalized turns: {int(attr['signalized'])} <br>\n",
    "\n",
    "#    </div>\n",
    "#    '''\n",
    "\n",
    "#    mymap.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "#    # Save the map to an HTML file or display it in a Jupyter notebook\n",
    "#    #mymap.save('map.html')\n",
    "#    # mymap.save('/path/to/save/map.html')  # Use an absolute path if needed\n",
    "#    return mymap  # Uncomment if you are using Jupyter notebook\n",
    "\n",
    "#    #TODO add in the legend with trip info and then we're golden\n",
    "\n",
    "# gdf\n",
    "# examined = []\n",
    "# #TODO add dots for signals and unsignalized\n",
    "# #have slides on turns\n",
    "# gdf.head()\n",
    "# tripid = gdf['tripid'].sample(1).item()\n",
    "# tripid = 2499\n",
    "# examined.append(tripid)\n",
    "# visualize(tripid,gdf,nodes)\n",
    "# with (export_fp/'ready4calibration.pkl').open('wb') as fh:\n",
    "#     pickle.dump(gdf,fh)\n",
    "# with (export_fp/'ready4calibration.pkl').open('wb') as fh:\n",
    "#     pickle.dump(gdf,fh)\n",
    "# #viz version (used for optimization too)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
