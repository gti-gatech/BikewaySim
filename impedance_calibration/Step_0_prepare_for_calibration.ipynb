{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Matched Traces for Calibration\n",
    "- Decide which traces to keep using match_ratio\n",
    "- Determine which links should be included for routing (+ include all links found during map matching)\n",
    "- Format the link attributes for calibration\n",
    "- Calculate shortest paths on the impedance network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from shapely.ops import MultiLineString, LineString\n",
    "import geopandas as gpd\n",
    "\n",
    "from bikewaysim.paths import config\n",
    "from bikewaysim.impedance_calibration import speedfactor, stochastic_optimization\n",
    "from bikewaysim.map_matching import map_match\n",
    "from bikewaysim.network import prepare_network, modeling_turns\n",
    "from bikewaysim.routing import rustworkx_routing_funcs\n",
    "from bikewaysim.map_matching import post_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the match map results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['match_dict_full_0']\n"
     ]
    }
   ],
   "source": [
    "# print the available match dicts\n",
    "print([x.stem for x in config['matching_fp'].glob('match_dict_full_*.pkl')])\n",
    "matching_index = 0  # Change this to the index of the match dict you want to use\n",
    "# matching_index = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597 / 682 (88%) successful matches\n",
      "81 / 682 (12%) partial matches\n",
      "4 / 682 (1%) failed matches\n"
     ]
    }
   ],
   "source": [
    "with (config['matching_fp'] / f'match_dict_full_{matching_index}.pkl').open('rb') as fh:\n",
    "    match_dict = pickle.load(fh)\n",
    "\n",
    "cutoff = 0.90 # set pct of points that need to be matched\n",
    "above_threshold, below_threshold, failed_matches, match_ratios = post_process.mapmatch_results(match_dict,cutoff)\n",
    "match_dict = {key:item for key,item in match_dict.items() if key in above_threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597 trips\n",
      "259 users\n"
     ]
    }
   ],
   "source": [
    "#get route attributes\n",
    "trips = pd.read_pickle(config['cycleatl_fp']/'trips_4.pkl')\n",
    "trips = trips.loc[trips['tripid'].isin(list(match_dict.keys()))]#,'userid'].nunique()\n",
    "users = pd.read_pickle(config['cycleatl_fp']/'users_4.pkl')\n",
    "users = users[users['userid'].isin(set(trips['userid'].tolist()))]\n",
    "print(trips.shape[0],'trips')\n",
    "print(users.shape[0],'users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group trips for calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by user\n",
    "ready_for_calibration_users = [(userid,list(trip_ids)) for userid, trip_ids in trips.groupby('userid')['tripid'].unique().reset_index().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one trip from each user\n",
    "import random\n",
    "random.seed(2)\n",
    "subset = trips.groupby('userid')['tripid'].apply(lambda x: random.choice(list(x))).reset_index()\n",
    "\n",
    "# debug\n",
    "random.seed(2)\n",
    "debug = random.sample(trips['tripid'].tolist(),20)\n",
    "\n",
    "ready_for_calibration_users.append(('random',subset['tripid'].tolist()))\n",
    "ready_for_calibration_users.append(('debug',debug))\n",
    "\n",
    "# by rider type\n",
    "not_fearless_users = users.loc[users['rider_type']!='Strong & fearless','userid'].tolist()\n",
    "not_fearless = [tripid for userid, tripid in subset.values if userid in not_fearless_users]\n",
    "fearless = [tripid for userid, tripid in subset.values if userid not in not_fearless_users]\n",
    "not_fearless = ('notfearless',not_fearless)\n",
    "fearless = ('fearless',fearless)\n",
    "ready_for_calibration_users.append(not_fearless)\n",
    "ready_for_calibration_users.append(fearless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['calibration_fp'] / 'subsets.pkl').open('wb') as fh:\n",
    "    pickle.dump(ready_for_calibration_users,fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Exceptions\n",
    "Get list of all links used in map matching to make sure these are kept in the calibration network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_matching_links = set()\n",
    "for tripid, items in match_dict.items():\n",
    "    map_matching_links.update(set([tuple(x) for x in items['edges'].values]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create calibration network\n",
    "Create dummy variables and make any other changes that weren't done in the final network export step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "directed_links = pd.read_parquet(config['network_fp']/'directed_edges.parquet') # has the directional variables\n",
    "links = pd.read_pickle(config['network_fp']/'final_network_edges.parquet') # non-directed\n",
    "nodes = gpd.read_file(config['network_fp']/'final_network.gpkg',layer='nodes')\n",
    "nodes = dict(zip(nodes['N'],nodes.geometry))\n",
    "turns = pd.read_parquet(config['network_fp']/'turns_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with links\n",
    "link_cols_drop = ['A','B','ascent_ft','ascent_grade_cat','descent_ft','descent_grade_cat','facility_fwd','facility_rev'] # drop directional attributes\n",
    "links.drop(columns=link_cols_drop,inplace=True)\n",
    "directed_cols_to_add = ['linkid','A','B','reverse_link','ascent_ft','ascent_grade_cat','facility_fwd']\n",
    "links = pd.merge(links,directed_links[directed_cols_to_add],on='linkid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove wrongway\n",
    "oneway_dict = dict(zip(links['linkid'],links['oneway']))\n",
    "turns['source_oneway'] = turns['source_linkid'].map(oneway_dict)\n",
    "turns['target_oneway'] = turns['target_linkid'].map(oneway_dict)\n",
    "del oneway_dict\n",
    "\n",
    "source_exception = [(linkid,reverse_link) in map_matching_links for linkid, reverse_link in turns[['source_linkid','source_reverse_link']].values]\n",
    "target_exception = [(linkid,reverse_link) in map_matching_links for linkid, reverse_link in turns[['target_linkid','target_reverse_link']].values]\n",
    "# source_wrongway = ((turns['source_oneway'] == True) & (turns['source_reverse_link'] == True)) == False\n",
    "# target_wrongway = ((turns['target_oneway'] == True) & (turns['target_reverse_link'] == True)) == False\n",
    "source_wrongway = ((turns[['source_oneway','source_reverse_link']] == True).all(axis=1)==False) | (source_exception)\n",
    "target_wrongway = ((turns[['target_oneway','target_reverse_link']] == True).all(axis=1)==False) | (target_exception)\n",
    "turns = turns[source_wrongway & target_wrongway]\n",
    "\n",
    "#remove wrongway links\n",
    "#TODO did we remove these in the export network step too?\n",
    "exception = [(linkid,reverse_link) in map_matching_links for linkid, reverse_link in links[['linkid','reverse_link']].values]\n",
    "links = links.loc[((links[['oneway','reverse_link']]==True).all(axis=1) == False) | exception]\n",
    "\n",
    "#TODO post GDOT\n",
    "#add elevation adjusted travel times based on assumed speed on flat ground\n",
    "# speedfactor.calculate_adjusted_speed(links,9)\n",
    "assumed_speed_mph = 9\n",
    "links['travel_time_min'] = (links.length / 5280 / assumed_speed_mph * 60).round(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dummy variables for modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[0,4)', '[4,6)', '[6,inf)'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links['ascent_grade_cat'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lanes\n",
    "links['1lpd'] = (links['lanes'] == 1).astype(int)\n",
    "links['2lpd'] = (links['lanes'] == 2).astype(int)\n",
    "links['3+lpd'] = (links['lanes'] == 3).astype(int)\n",
    "\n",
    "# speed\n",
    "links['[0,30] mph'] = (links['speed']=='[0,30]').astype(int)\n",
    "links['(30,40] mph'] = (links['speed']=='(30,40]').astype(int)\n",
    "links['(40,inf) mph'] = (links['speed']=='(40,inf)').astype(int)\n",
    "links['(30,inf) mph'] = (links[['(30,40] mph','(40,inf) mph']] == 1).any(axis=1).astype(int)\n",
    "\n",
    "# aadt\n",
    "links['[0,4k) aadt'] = (links['AADT']=='[0,4k)').astype(int)\n",
    "links['[4k,10k) aadt'] = (links['AADT']=='[4k,10k)').astype(int)\n",
    "links['[10k,inf) aadt'] = (links['AADT']=='[10k,inf)').astype(int)\n",
    "\n",
    "# grade\n",
    "links['[0,4) grade'] = (links['ascent_grade_cat']=='[0,4)').astype(int)\n",
    "links['[4,6) grade'] = (links['ascent_grade_cat']=='[4,6)').astype(int)\n",
    "links['[6,inf) grade'] = (links['ascent_grade_cat']=='[6,inf)').astype(int)\n",
    "\n",
    "# bicycle infra\n",
    "links['bike lane'] = links['facility_fwd'].isin(['bike lane','bufferred bike lane']).astype(int)\n",
    "links['cycletrack'] = links['facility_fwd'].isin(['cycletrack']).astype(int)\n",
    "links['multi use path'] = links['facility_fwd'].isin(['multi use path']).astype(int)\n",
    "links['multi use path and cycletrack'] = (links[['cycletrack','multi use path']] == 1).any(axis=1).astype(int)\n",
    "\n",
    "# turns\n",
    "turns.loc[turns['unsig_crossing'].isna(),'unsig_crossing'] = False\n",
    "turns['unsig_crossing'] = turns['unsig_crossing'].astype(int)\n",
    "turns['left_turn'] = ((turns['turn_type']=='left') & (turns['source_link_type']=='road') & (turns['target_link_type']=='road')).astype(int)\n",
    "turns['right_turn'] = ((turns['turn_type']=='right') & (turns['source_link_type']=='road') & (turns['target_link_type']=='road')).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AADT",
         "rawType": "category",
         "type": "unknown"
        }
       ],
       "ref": "13e7afe6-c898-4182-b84a-1ba7d73bee3e",
       "rows": [
        [
         "0",
         "[0,4k)"
        ],
        [
         "1",
         "[0,4k)"
        ],
        [
         "2",
         "[4k,10k)"
        ],
        [
         "3",
         "[4k,10k)"
        ],
        [
         "4",
         "[0,4k)"
        ],
        [
         "5",
         "[0,4k)"
        ],
        [
         "6",
         "[0,4k)"
        ],
        [
         "7",
         "[0,4k)"
        ],
        [
         "8",
         "[0,4k)"
        ],
        [
         "9",
         "[0,4k)"
        ],
        [
         "10",
         "[0,4k)"
        ],
        [
         "11",
         "[0,4k)"
        ],
        [
         "12",
         "[0,4k)"
        ],
        [
         "13",
         "[0,4k)"
        ],
        [
         "14",
         "[0,4k)"
        ],
        [
         "15",
         "[0,4k)"
        ],
        [
         "16",
         "[0,4k)"
        ],
        [
         "17",
         "[0,4k)"
        ],
        [
         "18",
         "[0,4k)"
        ],
        [
         "19",
         "[0,4k)"
        ],
        [
         "20",
         "[0,4k)"
        ],
        [
         "21",
         "[0,4k)"
        ],
        [
         "22",
         "[0,4k)"
        ],
        [
         "23",
         "[0,4k)"
        ],
        [
         "24",
         "[0,4k)"
        ],
        [
         "25",
         "[0,4k)"
        ],
        [
         "26",
         "[0,4k)"
        ],
        [
         "27",
         "[0,4k)"
        ],
        [
         "28",
         "[0,4k)"
        ],
        [
         "29",
         "[0,4k)"
        ],
        [
         "30",
         "[0,4k)"
        ],
        [
         "31",
         "[0,4k)"
        ],
        [
         "32",
         "[0,4k)"
        ],
        [
         "33",
         "[0,4k)"
        ],
        [
         "34",
         "[0,4k)"
        ],
        [
         "35",
         "[0,4k)"
        ],
        [
         "36",
         "[0,4k)"
        ],
        [
         "37",
         "[0,4k)"
        ],
        [
         "38",
         "[0,4k)"
        ],
        [
         "39",
         "[0,4k)"
        ],
        [
         "40",
         "[0,4k)"
        ],
        [
         "41",
         "[0,4k)"
        ],
        [
         "42",
         "[0,4k)"
        ],
        [
         "43",
         "[0,4k)"
        ],
        [
         "44",
         "[0,4k)"
        ],
        [
         "45",
         "[0,4k)"
        ],
        [
         "46",
         "[0,4k)"
        ],
        [
         "47",
         "[0,4k)"
        ],
        [
         "48",
         "[4k,10k)"
        ],
        [
         "49",
         "[4k,10k)"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 60756
       }
      },
      "text/plain": [
       "0          [0,4k)\n",
       "1          [0,4k)\n",
       "2        [4k,10k)\n",
       "3        [4k,10k)\n",
       "4          [0,4k)\n",
       "           ...   \n",
       "64475      [0,4k)\n",
       "64476      [0,4k)\n",
       "64477      [0,4k)\n",
       "64478      [0,4k)\n",
       "64479      [0,4k)\n",
       "Name: AADT, Length: 60756, dtype: category\n",
       "Categories (3, object): ['[0,4k)' < '[4k,10k)' < '[10k,inf)']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links['AADT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GDOT report variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "links0 = links.copy()\n",
    "links0['multi use path report'] = links0['facility_fwd'].isin(['multi use path','cycletrack']).astype(int)\n",
    "links0['bike lane report'] = links0['facility_fwd'].isin(['bike lane','bufferred bike lane']).astype(int)\n",
    "# non road links were given lanes = 0\n",
    "links0['lanes report'] = links0['lanes']\n",
    "links0.loc[(links0['multi use path report']==True) | (links['link_type'].isin(['bike','pedestrian','sidewalk'])),'lanes report'] = 0\n",
    "# just above 4% grade\n",
    "links0['above_4 report'] = links0['ascent_grade_cat'].isin(['[4,6)','[6,inf)']).astype(int)\n",
    "# merge back into links\n",
    "links0 = links0[['linkid','reverse_link','multi use path report','bike lane report','lanes report','above_4 report']]\n",
    "links = pd.merge(links,links0,suffixes=('',' report'),on=['linkid','reverse_link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDOT Base Case\n",
    "Pedestrian paths that are NOT multi-use trails and are flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "links['gdot_base'] = (links[['multi use path report','bike lane report','lanes report','above_4 report']] == 0).all(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Base Case\n",
    "Pedestrian paths + residential roads (1 lane per direction, no bicycle facility, < 4k aadt, < 4%, speed 40 or below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "links['new_base'] = (links[['2lpd','3+lpd','(30,inf) mph','[4k,10k) aadt','[10k,inf) aadt','[4,6) grade','[6,inf) grade','bike lane','cycletrack','multi use path']]==0).all(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only allow these types for routing unless there's an exception in the matched traces\n",
    "link_types_allowed = ['bike','pedestrian','road']\n",
    "exception = [(linkid,reverse_link) in map_matching_links for linkid, reverse_link in links[['linkid','reverse_link']].values]\n",
    "links = links[links['link_type'].isin(link_types_allowed) | exception]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = links.copy()\n",
    "exception = [(linkid,reverse_link) in map_matching_links for linkid, reverse_link in before[['linkid','reverse_link']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before connected components: Links 23919 Nodes 11586\n",
      "After connected components: Links 21755 Nodes 10368\n",
      "Before connected components: Turns 123976\n",
      "After connected components: Turns 33289\n",
      "21755 links and 10368 nodes\n",
      "33289 turns\n"
     ]
    }
   ],
   "source": [
    "# remove isolated links\n",
    "links, turns = prepare_network.remove_isolates(links,turns)\n",
    "\n",
    "print(links.shape[0],'links and',len(set(pd.concat([links['A'],links['B']]).tolist())),'nodes')\n",
    "print(turns.shape[0],'turns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # figure this out later\n",
    "# # assign attributes to the sidepaths\n",
    "# sidepaths = gpd.read_file(config['bicycle_facilities_fp']/'sidepaths.gpkg',layer='sidepaths',ignore_geometry=True)[['linkid','sidepath_linkid']]\n",
    "# replace = sidepaths.merge(links,on='linkid')\n",
    "# replace\n",
    "# # this approach currenlty \n",
    "\n",
    "# #list of all the attributes\n",
    "# all_attrs = ['2lpd', '3+lpd', '(30,40] mph', '(40,inf) mph',\n",
    "#        '[4k,10k) aadt', '[10k,inf) aadt', '[4,6) grade', '[6,inf) grade',\n",
    "#        'bike lane', 'cycletrack', 'multi use path', '(30,inf) mph',\n",
    "#        'multi use path and cycletrack', 'multi use path report',\n",
    "#        'bike lane report', 'lanes report', 'above_4 report']\n",
    "# replace = replace[['sidepath_linkid']+all_attrs]\n",
    "# replace = replace[replace['sidepath_linkid'].duplicated()==False]\n",
    "# replace\n",
    "\n",
    "# links = pd.merge(links,replace,left_on='linkid',right_on='sidepath_linkid',suffixes=(None,'_new'),how='left')\n",
    "# for col in replace.columns:\n",
    "#     if col != 'sidepath_linkid':\n",
    "#         links[col] = links[f'{col}_new'].fillna(links[col])\n",
    "# links.drop(columns=[x+'_new' for x in all_attrs],inplace=True)\n",
    "# # replace\n",
    "# #[['multi use path','sidepath']]\n",
    "# # links.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE for the set inf variable\n",
    "links['not_street'] = links['link_type'] != 'road'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export calibration network\n",
    "with (config['calibration_fp']/\"calibration_network.pkl\").open('wb') as fh:\n",
    "    pickle.dump((links,turns),fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export network for QGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:01<00:00, 39.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# drop cols we don't need\n",
    "fwd_links = links[links['reverse_link']==False].set_index('linkid')\n",
    "rev_links = links[links['reverse_link']==True].set_index('linkid')\n",
    "merged = pd.merge(fwd_links,rev_links,left_index=True,right_index=True,how='outer')\n",
    "\n",
    "cols = set([x.removesuffix('_x').removesuffix('_y') for x in merged.columns if ('_x' in x) | ('_y' in x)])\n",
    "\n",
    "# Function to condense two columns\n",
    "def condense_columns(col1, col2):\n",
    "    if pd.isna(col1):  # if col1 is NaN\n",
    "        return col2\n",
    "    elif pd.isna(col2):  # if col2 is NaN\n",
    "        return col1\n",
    "    elif col1 == col2:  # if values are equal\n",
    "        return col1\n",
    "    else:  # if values are different and neither is NaN\n",
    "        return str([col1, col2])\n",
    "    \n",
    "new_cols = {}\n",
    "\n",
    "for col in tqdm(cols):\n",
    "    new_cols[col] = [condense_columns(col1,col2) for col1, col2 in merged[[col+'_x',col+'_y']].values]\n",
    "\n",
    "undirected_links = pd.DataFrame.from_dict(new_cols,orient='columns')\n",
    "undirected_links.index = merged.index\n",
    "undirected_links.reset_index(inplace=True)\n",
    "undirected_links = gpd.GeoDataFrame(undirected_links,crs=config['projected_crs_epsg'])\n",
    "order_cols = ['linkid', 'osmid', 'link_type', 'oneway', 'highway', 'name', 'all_tags',\n",
    "              'lanes', 'speed', 'AADT', 'ascent_grade_cat', 'facility_fwd', 'year',\n",
    "              '2lpd','3+lpd',\n",
    "              '(30,40] mph', '(40,inf) mph', '(30,inf) mph',\n",
    "              '[4k,10k) aadt', '[10k,inf) aadt', \n",
    "              '[4,6) grade', '[6,inf) grade',\n",
    "              'bike lane', 'cycletrack', 'multi use path',\n",
    "              'bike lane report', 'multi use path report','lanes report','above_4 report',\n",
    "              'gdot_base','new_base',\n",
    "              'travel_time_min', 'geometry']\n",
    "undirected_links[order_cols].to_file(config['calibration_fp']/'calibration_network.gpkg',layer='final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without_isolates = set([tuple([x,y]) for x,y in links[['linkid','reverse_link']].values])\n",
    "# missing = [tuple([x,y]) not in without_isolates for x,y in before[['linkid','reverse_link']].values]\n",
    "# before[np.array(exception) & np.array(missing)].explore()#.to_file(Path.home()/'Downloads/')\n",
    "# ((links[['linkid','reverse_link']]==(35062.0,False)).all(axis=1)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble the match data for shortest path routing and calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links, turns, length_dict, geo_dict, turn_G = stochastic_optimization.import_calibration_network(config)\n",
    "# base_impedance_col = \"travel_time_min\"\n",
    "# stochastic_optimization.back_to_base_impedance(base_impedance_col,links,turns,turn_G)\n",
    "# links.set_index(['linkid','reverse_link'],inplace=True,drop=False)\n",
    "# match_results = {}\n",
    "# #shortest_results = {}\n",
    "# failed_shortest_path = []\n",
    "# for tripid, items in tqdm(match_dict.items()):\n",
    "\n",
    "#     #get start and end linkid\n",
    "#     start = tuple(match_dict[tripid]['edges'].iloc[0,:].values)\n",
    "#     end = tuple(match_dict[tripid]['edges'].iloc[-1,:].values)\n",
    "\n",
    "#     #get start and end node for shortest and impedance routing\n",
    "#     start = links.loc[start,'A']\n",
    "#     end = links.loc[end,'B']\n",
    "\n",
    "#     match_results[tripid] = {\n",
    "#     'origin_node': start,\n",
    "#     'destination_node': end,\n",
    "#     'trip_start_time': items['trace'].iloc[0,2].year,\n",
    "#     'match_ratio': items['match_ratio'],\n",
    "#     'matched_edges': match_dict[tripid]['edges'],\n",
    "#     'shortest_edges': pd.DataFrame(stochastic_optimization.impedance_path(turns,turn_G,links,start,end)['edge_list'],columns=['linkid','reverse_link'])\n",
    "#     }\n",
    "# # trip_ods = pd.DataFrame.from_dict(match_results,orient='index')\n",
    "# # trip_ods.reset_index(inplace=True)\n",
    "# # trip_ods.rename(columns={'index':'tripid'},inplace=True)\n",
    "# #export for impedance calibration\n",
    "# with (config['calibration_fp']/'ready_for_calibration.pkl').open('wb') as fh:\n",
    "#     pickle.dump(match_results,fh)\n",
    "# # links.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(rustworkx_routing_funcs)\n",
    "links, turns, length_dict, geo_dict, turn_G = rustworkx_routing_funcs.import_calibration_network(config)\n",
    "base_impedance_col = \"travel_time_min\"\n",
    "rustworkx_routing_funcs.back_to_base_impedance(base_impedance_col,links,turns,turn_G)\n",
    "links.set_index(['linkid','reverse_link'],inplace=True,drop=False)\n",
    "match_results = {}\n",
    "#shortest_results = {}\n",
    "failed_shortest_path = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rustworkx verison\n",
    "reload(post_process)\n",
    "starts, ends = post_process.get_ods_from_match_dict(match_dict,links)\n",
    "\n",
    "added_nodes = rustworkx_routing_funcs.add_virtual_edges(starts,ends,links,turns,turn_G)\n",
    "\n",
    "import numpy as np\n",
    "time_periods = []\n",
    "link_dates = sorted(links['year'].dropna().unique())[::-1]\n",
    "for trip_date in [items['trace'].iloc[0,2].year for tripid, items in match_dict.items()]:\n",
    "    cond = (trip_date < np.array(link_dates))\n",
    "    if cond.all():\n",
    "        print('trip is before any built infrastructure')\n",
    "        continue\n",
    "    idx = cond.argmin() # grabs the closest year in descending order\n",
    "    time_periods.append(link_dates[idx])\n",
    "\n",
    "shortest_tripid, shortest_length, shortest_edges = rustworkx_routing_funcs.rx_shortest_paths(list(zip(starts,ends)),turn_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tripid, start, end, year, shortest_edge in zip(match_dict.keys(),starts,ends,time_periods,shortest_edges):\n",
    "    match_results[tripid] = {\n",
    "        'origin_node': start,\n",
    "        'destination_node': end,\n",
    "        'trip_start_time': year,\n",
    "        'match_ratio': match_dict[tripid]['match_ratio'],\n",
    "        'matched_edges': match_dict[tripid]['edges'],\n",
    "        'shortest_edges': pd.DataFrame(shortest_edge,columns=['linkid','reverse_link'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['calibration_fp']/'ready_for_calibration.pkl').open('wb') as fh:\n",
    "    pickle.dump(match_results,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add this to the export network section\n",
    "# # add this for later\n",
    "# link_types = dict(zip(links['linkid'],links['link_type']))\n",
    "# turns['source_link_type'] = turns['source_linkid'].map(link_types)\n",
    "# turns['target_link_type'] = turns['source_linkid'].map(link_types)\n",
    "\n",
    "# # #unit conversions\n",
    "# links['length_mi'] = (links['length_ft'] / 5280).round(2)\n",
    "# links['ascent_ft'] = (links['ascent_m'] * 3.28084).round(0)\n",
    "# #links.drop(columns=['length_ft','ascent_m'],inplace=True)\n",
    "\n",
    "# #get node degree\n",
    "# degree = links['A'].append(links['B']).value_counts()\n",
    "# links['A_deg'] = links['A'].map(degree)\n",
    "# links['B_deg'] = links['B'].map(degree)\n",
    "# #remove excess dead end pedestrian links\n",
    "# dead_ends = (links['link_type']=='pedestrian')&((links['A_deg']==1)|(links['B_deg']==1))\n",
    "# links = links[dead_ends==False]\n",
    "# #unique scenario but there's an expressway tag that needs to be removed\n",
    "# import ast\n",
    "# john_lewis_freedom_pkwy = links['all_tags'].apply(lambda x: ast.literal_eval(x).get('expressway',0)=='yes')\n",
    "# links = links[john_lewis_freedom_pkwy==False]\n",
    "# surfaces = ['dirt','unpaved','gravel','fine_gravel','dirt/sand','ground']\n",
    "# unpaved = links['all_tags'].apply(lambda x: ast.literal_eval(x).get('surface',0) in surfaces)\n",
    "# #links[unpaved].explore(tooltip=False)\n",
    "# links = links[unpaved==False]\n",
    "# #unpaved.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bikewaysim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
