{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Calibration\n",
    "In this we look at the quality of the calibrated impedance functions.\n",
    "---\n",
    "1. Recalculate shortest path results using the calibrated coefficients\n",
    "1. Calculate objective functions and other performance metrics\n",
    "1. Get route attribute summary for the chosen/shortest/modeled routes (right now)\n",
    "\n",
    "\n",
    "Still Working On\n",
    "1. Look at where calibrated function did the best/worst job for both the training/testing set\n",
    "1. Cluster/segment results based on loss function value?\n",
    "4. Export for application in BikewaySim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from stochopy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "import similaritymeasures\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True)\n",
    "from shapely.ops import LineString, MultiLineString\n",
    "from importlib import reload\n",
    "\n",
    "from bikewaysim.paths import config\n",
    "from bikewaysim.network import modeling_turns\n",
    "from bikewaysim.impedance_calibration import stochastic_optimization, post_calibration, loss_functions\n",
    "from bikewaysim.routing import rustworkx_routing_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3605/3605 [01:18<00:00, 45.97it/s]  \n"
     ]
    }
   ],
   "source": [
    "beta_vals = post_calibration.betas_dataframe()\n",
    "loss_vals = post_calibration.aggregated_loss_dataframe()\n",
    "meta = post_calibration.metadata_dataframe()\n",
    "dissaggregate = post_calibration.post_calibration_disaggregate()\n",
    "shortest_dissaggregate = post_calibration.shortest_disaggregate()\n",
    "\n",
    "# drop the user calibrations\n",
    "meta = meta[pd.to_numeric(meta['subset'], errors='coerce').isna()]\n",
    "beta_vals = beta_vals[pd.to_numeric(beta_vals['subset'], errors='coerce').isna()]\n",
    "loss_vals = loss_vals[pd.to_numeric(loss_vals['subset'], errors='coerce').isna()]\n",
    "\n",
    "# concatenate columns of interestss\n",
    "join_cols = ['subset','calibration_name','run_num']\n",
    "loss_cols = ['jaccard_buffer_mean','jaccard_exact_mean','jaccard_buffer_total','jaccard_exact_total','shortest_jaccard_exact_mean', 'shortest_jaccard_exact_total',\n",
    "       'shortest_jaccard_buffer_mean', 'shortest_jaccard_buffer_total']\n",
    "meta_cols = ['objective_function','time','set_to_zero','set_to_inf','method','popsize','num_trips','status']\n",
    "\n",
    "concat = [beta_vals.set_index(join_cols), loss_vals.set_index(join_cols)[loss_cols], meta.set_index(join_cols)[meta_cols]] \n",
    "concat = pd.concat(concat,ignore_index=False,axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat has all of the model results to examine the different types of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['calibration_fp'] / 'results/random,traffic_test,0.pkl').open('rb') as fh:\n",
    "    test = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'betas_tup': ({'col': '2lpd',\n",
       "   'type': 'link',\n",
       "   'range': [0, 1],\n",
       "   'beta': 0.3162},\n",
       "  {'col': '3+lpd', 'type': 'link', 'range': [0, 1], 'beta': 0.3255},\n",
       "  {'col': '(30,inf) mph', 'type': 'link', 'range': [0, 1], 'beta': -0.0389},\n",
       "  {'col': '[4k,10k) aadt', 'type': 'link', 'range': [0, 1], 'beta': -0.165},\n",
       "  {'col': '[10k,inf) aadt', 'type': 'link', 'range': [0, 1], 'beta': 0.313},\n",
       "  {'col': '[4,6) grade', 'type': 'link', 'range': [0, 3], 'beta': 0.6382},\n",
       "  {'col': '[6,inf) grade', 'type': 'link', 'range': [0, 3], 'beta': 2.7639},\n",
       "  {'col': 'bike lane', 'type': 'link', 'range': [-1, 0], 'beta': -0.4469},\n",
       "  {'col': 'cycletrack', 'type': 'link', 'range': [-1, 0], 'beta': -0.6965},\n",
       "  {'col': 'multi use path', 'type': 'link', 'range': [-1, 0], 'beta': -0.4475},\n",
       "  {'col': 'unsig_crossing', 'type': 'turn', 'range': [0, 2], 'beta': 0.4876}),\n",
       " 'set_to_zero': ['bike lane', 'cycletrack', 'multi use path'],\n",
       " 'set_to_inf': ['not_street'],\n",
       " 'base_impedance_col': 'travel_time_min',\n",
       " 'base_link_col': None,\n",
       " 'link_impedance_function': <function bikewaysim.impedance_calibration.impedance_functions.link_impedance_function(betas: <built-in function array>, betas_tup: tuple, links: pandas.core.frame.DataFrame, base_impedance_col: str, base_link_col: str = None, trip_specific=None)>,\n",
       " 'turn_impedance_function': <function bikewaysim.impedance_calibration.impedance_functions.turn_impedance_function(betas: <built-in function array>, betas_tup: tuple, turns: pandas.core.frame.DataFrame)>,\n",
       " 'settings': {'method': 'pso',\n",
       "  'options': {'maxiter': 10,\n",
       "   'popsize': 25,\n",
       "   'return_all': True,\n",
       "   'ftol': -0.65}},\n",
       " 'objective_function': 'jaccard_buffer_mean',\n",
       " 'results':      fun: -0.426\n",
       "  message: 'maximum number of iterations is reached'\n",
       "     nfev: 250\n",
       "      nit: 10\n",
       "   status: -1\n",
       "  success: False\n",
       "        x: array([ 0.31619737,  0.32550525, -0.03893125, -0.16501814,  0.3129706 ,\n",
       "         0.63819229,  2.7639259 , -0.44687735, -0.69650851, -0.44745525,\n",
       "         0.48759783]),\n",
       " 'trips_calibrated': {68,\n",
       "  80,\n",
       "  85,\n",
       "  103,\n",
       "  115,\n",
       "  131,\n",
       "  136,\n",
       "  137,\n",
       "  143,\n",
       "  150,\n",
       "  153,\n",
       "  165,\n",
       "  167,\n",
       "  184,\n",
       "  194,\n",
       "  197,\n",
       "  211,\n",
       "  213,\n",
       "  219,\n",
       "  235,\n",
       "  243,\n",
       "  247,\n",
       "  251,\n",
       "  255,\n",
       "  261,\n",
       "  266,\n",
       "  284,\n",
       "  290,\n",
       "  293,\n",
       "  309,\n",
       "  313,\n",
       "  322,\n",
       "  337,\n",
       "  341,\n",
       "  349,\n",
       "  358,\n",
       "  372,\n",
       "  382,\n",
       "  393,\n",
       "  404,\n",
       "  411,\n",
       "  416,\n",
       "  417,\n",
       "  450,\n",
       "  463,\n",
       "  468,\n",
       "  469,\n",
       "  489,\n",
       "  501,\n",
       "  509,\n",
       "  515,\n",
       "  520,\n",
       "  555,\n",
       "  560,\n",
       "  562,\n",
       "  570,\n",
       "  571,\n",
       "  574,\n",
       "  580,\n",
       "  605,\n",
       "  607,\n",
       "  610,\n",
       "  677,\n",
       "  686,\n",
       "  688,\n",
       "  702,\n",
       "  707,\n",
       "  743,\n",
       "  745,\n",
       "  749,\n",
       "  761,\n",
       "  767,\n",
       "  784,\n",
       "  817,\n",
       "  819,\n",
       "  823,\n",
       "  835,\n",
       "  837,\n",
       "  861,\n",
       "  881,\n",
       "  897,\n",
       "  933,\n",
       "  939,\n",
       "  957,\n",
       "  988,\n",
       "  1001,\n",
       "  1005,\n",
       "  1008,\n",
       "  1010,\n",
       "  1026,\n",
       "  1041,\n",
       "  1086,\n",
       "  1087,\n",
       "  1103,\n",
       "  1106,\n",
       "  1109,\n",
       "  1127,\n",
       "  1148,\n",
       "  1151,\n",
       "  1152,\n",
       "  1177,\n",
       "  1190,\n",
       "  1191,\n",
       "  1195,\n",
       "  1212,\n",
       "  1229,\n",
       "  1230,\n",
       "  1254,\n",
       "  1270,\n",
       "  1287,\n",
       "  1380,\n",
       "  1428,\n",
       "  1450,\n",
       "  1481,\n",
       "  1538,\n",
       "  1585,\n",
       "  1627,\n",
       "  1635,\n",
       "  1659,\n",
       "  1664,\n",
       "  1685,\n",
       "  1705,\n",
       "  1803,\n",
       "  1842,\n",
       "  1869,\n",
       "  1881,\n",
       "  1926,\n",
       "  1940,\n",
       "  1944,\n",
       "  1945,\n",
       "  1951,\n",
       "  2008,\n",
       "  2026,\n",
       "  2110,\n",
       "  2163,\n",
       "  2164,\n",
       "  2173,\n",
       "  2232,\n",
       "  2235,\n",
       "  2270,\n",
       "  2347,\n",
       "  2424,\n",
       "  2447,\n",
       "  2469,\n",
       "  2502,\n",
       "  2512,\n",
       "  2528,\n",
       "  2533,\n",
       "  2538,\n",
       "  2548,\n",
       "  2568,\n",
       "  2586,\n",
       "  2591,\n",
       "  2621,\n",
       "  2653,\n",
       "  2660,\n",
       "  2751,\n",
       "  2775,\n",
       "  2859,\n",
       "  2860,\n",
       "  2874,\n",
       "  2877,\n",
       "  2895,\n",
       "  2937,\n",
       "  2953,\n",
       "  2975,\n",
       "  3006,\n",
       "  3035,\n",
       "  3054,\n",
       "  3104,\n",
       "  3108,\n",
       "  3112,\n",
       "  3126,\n",
       "  3158,\n",
       "  3159,\n",
       "  3228,\n",
       "  3254,\n",
       "  3361,\n",
       "  3393,\n",
       "  3396,\n",
       "  3426,\n",
       "  3459,\n",
       "  3491,\n",
       "  3554,\n",
       "  3692,\n",
       "  3716,\n",
       "  3751,\n",
       "  3783,\n",
       "  3806,\n",
       "  3834,\n",
       "  3863,\n",
       "  3921,\n",
       "  3939,\n",
       "  3961,\n",
       "  3995,\n",
       "  4030,\n",
       "  4052,\n",
       "  4058,\n",
       "  4117,\n",
       "  4205,\n",
       "  4265,\n",
       "  4268,\n",
       "  4276,\n",
       "  4294,\n",
       "  4309,\n",
       "  4335,\n",
       "  4399,\n",
       "  4416,\n",
       "  4425,\n",
       "  4455,\n",
       "  4499,\n",
       "  4574,\n",
       "  4592,\n",
       "  4647,\n",
       "  4663,\n",
       "  4683,\n",
       "  4692,\n",
       "  4697,\n",
       "  4767,\n",
       "  4768,\n",
       "  4772,\n",
       "  4792,\n",
       "  4803,\n",
       "  4811,\n",
       "  4812,\n",
       "  4849,\n",
       "  4861,\n",
       "  4894,\n",
       "  4943,\n",
       "  4955,\n",
       "  4973,\n",
       "  5025,\n",
       "  5036,\n",
       "  5143,\n",
       "  5212,\n",
       "  5246,\n",
       "  5334,\n",
       "  5344,\n",
       "  5388,\n",
       "  5444,\n",
       "  5481,\n",
       "  5500,\n",
       "  5574,\n",
       "  5626,\n",
       "  5636,\n",
       "  5648,\n",
       "  5695,\n",
       "  5767,\n",
       "  5799,\n",
       "  5985,\n",
       "  6081,\n",
       "  6102,\n",
       "  6125,\n",
       "  6186,\n",
       "  6210,\n",
       "  6236,\n",
       "  6298,\n",
       "  6347,\n",
       "  6375,\n",
       "  6394,\n",
       "  6462,\n",
       "  6488,\n",
       "  6498,\n",
       "  6509,\n",
       "  6535,\n",
       "  6541,\n",
       "  6558,\n",
       "  6568,\n",
       "  6601,\n",
       "  6676,\n",
       "  6722,\n",
       "  6746,\n",
       "  6755,\n",
       "  6841,\n",
       "  6890,\n",
       "  6904,\n",
       "  6917,\n",
       "  6980,\n",
       "  7010,\n",
       "  7014,\n",
       "  7035,\n",
       "  7055,\n",
       "  7065,\n",
       "  7119,\n",
       "  7125,\n",
       "  7202,\n",
       "  7231,\n",
       "  7236,\n",
       "  7269,\n",
       "  7313,\n",
       "  7319,\n",
       "  7339,\n",
       "  7358,\n",
       "  7365,\n",
       "  7419,\n",
       "  7499,\n",
       "  7510,\n",
       "  7523,\n",
       "  7546,\n",
       "  7559,\n",
       "  7593,\n",
       "  7599,\n",
       "  7613,\n",
       "  7641,\n",
       "  7667,\n",
       "  7716,\n",
       "  7740,\n",
       "  7761,\n",
       "  7832,\n",
       "  7936,\n",
       "  7939,\n",
       "  7942,\n",
       "  7955,\n",
       "  7978,\n",
       "  7996,\n",
       "  8010,\n",
       "  8040,\n",
       "  8046,\n",
       "  8105,\n",
       "  8121,\n",
       "  8153,\n",
       "  8178,\n",
       "  8202,\n",
       "  8232,\n",
       "  8239,\n",
       "  8275,\n",
       "  8282,\n",
       "  8344,\n",
       "  8348,\n",
       "  8363,\n",
       "  8389,\n",
       "  8423,\n",
       "  8429,\n",
       "  8485,\n",
       "  8589,\n",
       "  8594,\n",
       "  8626,\n",
       "  8668,\n",
       "  8684,\n",
       "  8694,\n",
       "  8703,\n",
       "  8760,\n",
       "  8768,\n",
       "  8882,\n",
       "  8917,\n",
       "  8929,\n",
       "  8974,\n",
       "  9022,\n",
       "  9039,\n",
       "  9112,\n",
       "  9158,\n",
       "  9179,\n",
       "  9191,\n",
       "  9239,\n",
       "  9272,\n",
       "  9382,\n",
       "  9516,\n",
       "  9556,\n",
       "  9581,\n",
       "  9590,\n",
       "  9616,\n",
       "  9706,\n",
       "  9794,\n",
       "  9847,\n",
       "  9872,\n",
       "  9885,\n",
       "  9927,\n",
       "  10054,\n",
       "  10156,\n",
       "  10510,\n",
       "  10669,\n",
       "  10708,\n",
       "  10772,\n",
       "  10799,\n",
       "  10827,\n",
       "  10879,\n",
       "  10889,\n",
       "  10903,\n",
       "  10924,\n",
       "  10933,\n",
       "  11057,\n",
       "  11060,\n",
       "  11096,\n",
       "  11098,\n",
       "  11130,\n",
       "  11207,\n",
       "  11287,\n",
       "  11293,\n",
       "  11334,\n",
       "  11364,\n",
       "  11412,\n",
       "  11434,\n",
       "  11481,\n",
       "  11527,\n",
       "  11535,\n",
       "  11553,\n",
       "  11599,\n",
       "  11611,\n",
       "  11628,\n",
       "  11671,\n",
       "  11694,\n",
       "  11705,\n",
       "  11733,\n",
       "  11779,\n",
       "  11793,\n",
       "  11804,\n",
       "  11840,\n",
       "  11925,\n",
       "  11931,\n",
       "  11954,\n",
       "  11993,\n",
       "  12023,\n",
       "  12070,\n",
       "  12295,\n",
       "  12349,\n",
       "  12360,\n",
       "  12422,\n",
       "  12445,\n",
       "  12471,\n",
       "  12479,\n",
       "  12514,\n",
       "  12517,\n",
       "  12615,\n",
       "  12695,\n",
       "  12783,\n",
       "  12811,\n",
       "  12921,\n",
       "  12931,\n",
       "  12940,\n",
       "  12962,\n",
       "  12975,\n",
       "  13020,\n",
       "  13119,\n",
       "  13198,\n",
       "  13206,\n",
       "  13232,\n",
       "  13241,\n",
       "  13256,\n",
       "  13266,\n",
       "  13268,\n",
       "  13325,\n",
       "  13355,\n",
       "  13464,\n",
       "  13575,\n",
       "  13580,\n",
       "  13591,\n",
       "  13667,\n",
       "  13716,\n",
       "  13722,\n",
       "  13754,\n",
       "  13785,\n",
       "  13805,\n",
       "  13871,\n",
       "  13927,\n",
       "  14035,\n",
       "  14053,\n",
       "  14057,\n",
       "  14120,\n",
       "  14132,\n",
       "  14144,\n",
       "  14235,\n",
       "  14265,\n",
       "  14279,\n",
       "  14341,\n",
       "  14466,\n",
       "  14496,\n",
       "  14516,\n",
       "  14517,\n",
       "  14606,\n",
       "  14612,\n",
       "  14613,\n",
       "  14705,\n",
       "  14744,\n",
       "  14814,\n",
       "  14876,\n",
       "  14952,\n",
       "  14976,\n",
       "  14979,\n",
       "  14994,\n",
       "  15047,\n",
       "  15054,\n",
       "  15072,\n",
       "  15120,\n",
       "  15129,\n",
       "  15145,\n",
       "  15189,\n",
       "  15259,\n",
       "  15295,\n",
       "  15325,\n",
       "  15356,\n",
       "  15417,\n",
       "  15454,\n",
       "  15461,\n",
       "  15463,\n",
       "  15476,\n",
       "  15599,\n",
       "  15630,\n",
       "  15665,\n",
       "  15683,\n",
       "  15742,\n",
       "  15784,\n",
       "  15804,\n",
       "  15865,\n",
       "  15883,\n",
       "  15943,\n",
       "  15971,\n",
       "  15987,\n",
       "  15996,\n",
       "  16003,\n",
       "  16010,\n",
       "  16011,\n",
       "  16052,\n",
       "  16067,\n",
       "  16086,\n",
       "  16129,\n",
       "  16186,\n",
       "  16285,\n",
       "  16451,\n",
       "  16547,\n",
       "  16587,\n",
       "  16592,\n",
       "  16597,\n",
       "  16611,\n",
       "  16656,\n",
       "  16765,\n",
       "  16771,\n",
       "  16823,\n",
       "  16825,\n",
       "  16841,\n",
       "  16855,\n",
       "  16869,\n",
       "  16914,\n",
       "  16935,\n",
       "  16959,\n",
       "  17003,\n",
       "  17037,\n",
       "  17079,\n",
       "  17100,\n",
       "  17103,\n",
       "  17123,\n",
       "  17287,\n",
       "  17297,\n",
       "  17432,\n",
       "  17451,\n",
       "  17471,\n",
       "  17546,\n",
       "  17575,\n",
       "  17659,\n",
       "  17746,\n",
       "  17829,\n",
       "  17870,\n",
       "  18023,\n",
       "  18060,\n",
       "  18324,\n",
       "  18328,\n",
       "  18345,\n",
       "  18361,\n",
       "  18451,\n",
       "  18539,\n",
       "  18553,\n",
       "  24061,\n",
       "  24168,\n",
       "  24185,\n",
       "  24296,\n",
       "  24316,\n",
       "  24329,\n",
       "  24392,\n",
       "  24417,\n",
       "  24460,\n",
       "  24493,\n",
       "  24495,\n",
       "  24505,\n",
       "  24731,\n",
       "  24781,\n",
       "  24783,\n",
       "  24828,\n",
       "  24836,\n",
       "  24899,\n",
       "  25050,\n",
       "  25104,\n",
       "  25107,\n",
       "  25157,\n",
       "  25226,\n",
       "  25412,\n",
       "  25432,\n",
       "  25507,\n",
       "  25620,\n",
       "  25810,\n",
       "  25918,\n",
       "  26076,\n",
       "  26354,\n",
       "  26421,\n",
       "  26443,\n",
       "  26444,\n",
       "  26470,\n",
       "  26535,\n",
       "  26612,\n",
       "  26623,\n",
       "  26668,\n",
       "  26703,\n",
       "  27028,\n",
       "  27278,\n",
       "  27310,\n",
       "  27481,\n",
       "  27524,\n",
       "  27542,\n",
       "  27550,\n",
       "  27552,\n",
       "  27615,\n",
       "  27786,\n",
       "  27848,\n",
       "  27975,\n",
       "  28141,\n",
       "  28209,\n",
       "  28436,\n",
       "  28523,\n",
       "  28614,\n",
       "  28762,\n",
       "  28900,\n",
       "  28925,\n",
       "  29139,\n",
       "  29189,\n",
       "  29307,\n",
       "  29310,\n",
       "  29325,\n",
       "  29349,\n",
       "  29352,\n",
       "  29399,\n",
       "  29441,\n",
       "  29830,\n",
       "  30030,\n",
       "  30039,\n",
       "  30167,\n",
       "  30199,\n",
       "  30350,\n",
       "  30368,\n",
       "  30440,\n",
       "  30727,\n",
       "  30744,\n",
       "  30801,\n",
       "  31031,\n",
       "  31203,\n",
       "  31288,\n",
       "  31298,\n",
       "  31353,\n",
       "  31405,\n",
       "  31563,\n",
       "  31770,\n",
       "  31816,\n",
       "  32038,\n",
       "  32102,\n",
       "  32275,\n",
       "  32512,\n",
       "  32606,\n",
       "  32848,\n",
       "  32992,\n",
       "  33059,\n",
       "  33510,\n",
       "  33604,\n",
       "  33729,\n",
       "  33804,\n",
       "  34031,\n",
       "  34277,\n",
       "  34289},\n",
       " 'runtime': datetime.timedelta(seconds=4853, microseconds=675359),\n",
       " 'time': datetime.datetime(2024, 11, 25, 11, 55, 30, 743095)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subset                                                            random\n",
       "calibration_name                                            traffic_test\n",
       "run_num                                                                0\n",
       "2lpd                                                               0.316\n",
       "3+lpd                                                              0.326\n",
       "(30,40] mph                                                          NaN\n",
       "(40,inf) mph                                                         NaN\n",
       "[4k,10k) aadt                                                     -0.165\n",
       "[10k,inf) aadt                                                     0.313\n",
       "[4,6) grade                                                        0.638\n",
       "[6,inf) grade                                                      2.764\n",
       "bike lane                                                         -0.447\n",
       "cycletrack                                                        -0.696\n",
       "multi use path                                                    -0.448\n",
       "unsig_crossing                                                     0.488\n",
       "(30,inf) mph                                                      -0.039\n",
       "multi use path report                                                NaN\n",
       "bike lane report                                                     NaN\n",
       "lanes report                                                         NaN\n",
       "above_4 report                                                       NaN\n",
       "left_turn                                                            NaN\n",
       "right_turn                                                           NaN\n",
       "multi use path and cycletrack                                        NaN\n",
       "jaccard_buffer_mean                                                  NaN\n",
       "jaccard_exact_mean                                                   NaN\n",
       "jaccard_buffer_total                                                 NaN\n",
       "jaccard_exact_total                                                  NaN\n",
       "shortest_jaccard_exact_mean                                       0.3328\n",
       "shortest_jaccard_exact_total                                      0.2258\n",
       "shortest_jaccard_buffer_mean                                      0.3578\n",
       "shortest_jaccard_buffer_total                                      0.243\n",
       "objective_function                                   jaccard_buffer_mean\n",
       "time                                          2024-11-25 11:55:30.743095\n",
       "set_to_zero                      [bike lane, cycletrack, multi use path]\n",
       "set_to_inf                                                  [not_street]\n",
       "method                                                               pso\n",
       "popsize                                                             25.0\n",
       "num_trips                                                          664.0\n",
       "status                                                              -1.0\n",
       "Name: 1131, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat.sort_values('time',ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get shortest path objective function values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the trips\n",
    "with (config['calibration_fp'] / 'ready_for_calibration_stats.pkl').open('rb') as fh:\n",
    "    ready_for_calibration_stats = pickle.load(fh)\n",
    "all_trips = list(ready_for_calibration_stats.keys())\n",
    "# get the random trips\n",
    "with (config['calibration_fp'] / 'subsets.pkl').open('rb') as fh:\n",
    "    subsets = pickle.load(fh)\n",
    "training_set = [x[1] for x in subsets if x[0] == 'random'][0]\n",
    "testing_set = list(set(all_trips) - set(training_set))\n",
    "\n",
    "def quick_function(trips,name,ready_for_calibration_stats):\n",
    "    test_dict = {key:item for key,item in ready_for_calibration_stats.items() if key in trips}\n",
    "    output = {}\n",
    "    output['name'] = name\n",
    "    # jaccard exact mean\n",
    "    output['jaccard_exact_mean'] = np.mean([item['shortest_jaccard_exact'] for key, item in test_dict.items()])\n",
    "    output['jaccard_exact_total'] = np.sum([item['shortest_jaccard_exact_intersection'] for key, item in test_dict.items()]) / np.sum([item['shortest_jaccard_exact_union'] for key, item in test_dict.items()])\n",
    "    output['jaccard_buffer_mean'] = np.mean([item['shortest_jaccard_buffer'] for key, item in test_dict.items()])\n",
    "    output['jaccard_buffer_total'] = np.sum([item['shortest_jaccard_buffer_intersection'] for key, item in test_dict.items()]) / np.sum([item['shortest_jaccard_buffer_union'] for key, item in test_dict.items()])\n",
    "    return output\n",
    "shortest_objective_values = [quick_function(trips,name,ready_for_calibration_stats) for trips, name in [(all_trips,'All'),(training_set,'Train'),(testing_set,'Test')]]\n",
    "shortest_objective_values = pd.DataFrame(shortest_objective_values).round(2)\n",
    "shortest_objective_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rider type\n",
    "rider_type_models = concat[concat['subset'].isin(['fearless','notfearless'])]\n",
    "# rider_type_models.to_csv(config['scratch_fp']/'rider_type.csv',index=False)\n",
    "\n",
    "# random/all models\n",
    "best_models = concat[concat['subset'].isin(['random','all'])].dropna(axis=1,how='all').sort_values('jaccard_buffer_mean',ascending=False).dropna(axis=1,how='all')[concat['num_trips'] > 100].head(50)\n",
    "# best_models.to_csv(config['scratch_fp']/'best_models.csv',index=False)\n",
    "\n",
    "# just the random ones\n",
    "best_models[best_models['subset']=='random'].sort_values('time',ascending=False).to_csv(config['scratch_fp']/'random.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results of testing the different objective funciton values\n",
    "run_nums = ['7','6','1','0']\n",
    "obj_func_test = best_models.loc[(best_models['subset']=='random') & (best_models['calibration_name']=='no traffic') & (best_models['run_num'].isin(run_nums)),['objective_function']+[x for x in loss_cols if 'shortest_' not in x]]\n",
    "obj_func_test.set_index('objective_function',inplace=True)\n",
    "obj_func_test.loc[['jaccard_exact_mean','jaccard_exact_total','jaccard_buffer_mean','jaccard_buffer_total'],['jaccard_exact_mean','jaccard_exact_total','jaccard_buffer_mean','jaccard_buffer_total']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the objective function values over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['calibration_fp']/f'results/random,iterations,3.pkl').open('rb') as fh:\n",
    "    best = pickle.load(fh)\n",
    "np.array(best['results'].funall).min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best['results'].funall[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dissaggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to the best model so far\n",
    "cond = (dissaggregate.columns.get_level_values(0) == 'all') & \\\n",
    "    (dissaggregate.columns.get_level_values(1) == 'jaccard_buffer_mean') & \\\n",
    "    (dissaggregate.columns.get_level_values(2) == '2') & \\\n",
    "    (dissaggregate.columns.get_level_values(3).isin(['jaccard_buffer','detour','length']))\n",
    "best_model = dissaggregate.loc[:,cond].droplevel([0,1,2],axis=1)\n",
    "shortest_dissaggregate = shortest_dissaggregate[['trip_start_time','match_ratio','chosen_length','shortest_length','chosen_detour','shortest_jaccard_buffer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = pd.concat([shortest_dissaggregate,best_model],ignore_index=False,axis=1)\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the loss function plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO make this into a function\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#aggregated results and overlap figures\n",
    "mean_values = []\n",
    "for idx, post_calibration_result_fp in enumerate(post_calibration_result_fps):\n",
    "    print('Calibration result',post_calibration_result_fp.stem)\n",
    "    with post_calibration_result_fp.open('rb') as fh:\n",
    "        post_calibration_result = pickle.load(fh)\n",
    "\n",
    "    jaccard_mean = np.array([item['modeled_jaccard'] for tripid, item in post_calibration_result.items()]).mean()\n",
    "    buffer_mean = np.array([item['modeled_buffer'] for tripid, item in post_calibration_result.items()]).mean()\n",
    "    mean_values.append((post_calibration_result_fp.stem,round(jaccard_mean,2),round(buffer_mean,2)))\n",
    "\n",
    "    print('Mean Jaccard Index',round(jaccard_mean,2),'Mean Buffer',round(buffer_mean,2))\n",
    "    # with (config['calibration_fp']/'post_calibration'/(calibration_result_fp.stem+'.pkl')).open('wb') as fh:\n",
    "    #     pickle.dump(modeled_results_dict,fh)\n",
    "\n",
    "    #get just jaccard values\n",
    "    modeled_jaccard = {tripid:item['modeled_jaccard'] for tripid, item in post_calibration_result.items()}\n",
    "    modeled_jaccard = pd.Series(modeled_jaccard)\n",
    "    modeled_jaccard.name = 'modeled_jaccard'\n",
    "    df = pd.concat([shortest_jaccard,modeled_jaccard],axis=1,ignore_index=False)\n",
    "\n",
    "    # make figures\n",
    "    # Create the histogram\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.hist(df['shortest_jaccard'], bins=20, alpha=0.5, label='Shortest Path Overlap', color='grey')\n",
    "    plt.hist(df['modeled_jaccard'], bins=20, alpha=0.3, label='Calibrated Overlap', color='blue')\n",
    "\n",
    "    # Adding labels, title, and legend with font size adjustments\n",
    "    plt.xlabel('Overlap', fontsize=22)\n",
    "    plt.ylabel(f'Frequency (N={df.shape[0]})', fontsize=22)\n",
    "    plt.title(post_calibration_result_fp.stem, fontsize=16)\n",
    "    plt.legend(title='Jaccard Index', fontsize=22, title_fontsize=22)\n",
    "    plt.ylim([0,700])\n",
    "\n",
    "    # Adjusting the font size of the tick labels\n",
    "    plt.xticks(fontsize=22)\n",
    "    plt.yticks(fontsize=22)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(config['calibration_fp']/'calibration_performance'/(post_calibration_result_fp.stem + '.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SORT below here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate all the possible overlap metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import impedance_calibration.src.summarize_route as summarize_route\n",
    "reload(summarize_route)\n",
    "\n",
    "chosen_attr = {tripid:summarize_route.route_attributes1(tripid,full_set[tripid]['matched_edges'].values,links,turns) for tripid in full_set.keys()}\n",
    "shortest_attr = {tripid:summarize_route.route_attributes1(tripid,full_set[tripid]['shortest_edges'].values,links,turns) for tripid in full_set.keys()} \n",
    "modeled_attr = {tripid:summarize_route.route_attributes1(tripid,full_set[tripid]['modeled_edges'].values,links,turns) for tripid in full_set.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_attr = pd.DataFrame.from_dict(chosen_attr,orient='index')\n",
    "shortest_attr = pd.DataFrame.from_dict(shortest_attr,orient='index')\n",
    "modeled_attr = pd.DataFrame.from_dict(modeled_attr,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.reset_index().to_csv(config['calibration_fp']/'objective_functions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(config['calibration_fp']/'objective_functions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.mean().drop('index').to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmenting/labeling trips\n",
    "Want to know which attributes help lead to a loss function value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree #1\n",
    "Want to see if percent detour or if any of the chosen route characterstics (that were accounted for in the calibration process) contributed to the overlap value (modeled_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df = pd.concat([results_df[['chosen_length','chosen_detour','modeled_jaccard']],chosen_attr],axis=1)\n",
    "\n",
    "tree_df['lanes_above_1'] = tree_df['lane_2'] + tree_df['lane_3']\n",
    "tree_df.drop(columns=['lane_0','lane_1','lane_2','lane_3'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First split is the chosen detour rate. 1,377 trips (over half) have a detour rate above 10.5% and this is associated with a bigger reduction in overlap. Past this it appears that trips that took a lot \n",
    "\n",
    "Second split is on chosen length and chosen detour rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "from sklearn import tree\n",
    "\n",
    "X, y = tree_df.drop(columns=['modeled_jaccard']).values, tree_df['modeled_jaccard'].values\n",
    "clf = tree.DecisionTreeRegressor(max_depth=3,min_samples_split=50)\n",
    "clf = clf.fit(X, y)\n",
    "fig, ax = plt.subplots(figsize=(20, 10), dpi=300)\n",
    "plot_tree(clf, feature_names=tree_df.drop(columns=['modeled_jaccard']).columns, filled=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal regression\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X, y = tree_df.drop(columns=['modeled_jaccard']), tree_df['modeled_jaccard']\n",
    "X = sm.add_constant(X)\n",
    "model_sm = sm.OLS(y, X).fit()\n",
    "print(model_sm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df.sort_values('unsig_major_road_crossing',ascending=False).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = one_hot_data.values, merged['impedance'].values\n",
    "clf = tree.DecisionTreeRegressor(max_depth=5)\n",
    "clf = clf.fit(X, y)\n",
    "fig, ax = plt.subplots(figsize=(20, 10), dpi=300)\n",
    "plot_tree(clf, feature_names=one_hot_data.columns, filled=True, ax=ax)\n",
    "plt.show()\n",
    "how to compare predicted values vs actual?\n",
    "SS_res = ((y - clf.predict(X))**2).sum()\n",
    "SS_tot = ((y - y.mean())**2).sum()\n",
    "R2 = 1 - (SS_res/SS_tot)\n",
    "R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with (config['calibration_fp']/\"full_modeled_results.pkl\").open('rb') as fh:\n",
    "#     modeled_results = pickle.load(fh)\n",
    "## Add Route Attributes\n",
    "import impedance_calibration.src.summarize_route as summarize_route\n",
    "cols_to_summarize = {\n",
    "    'facility_fwd': \"category\",\n",
    "    'AADT': (\"threshold\",[10000]),\n",
    "    'truck_pct': (\"threshold\",[5]),\n",
    "    'speed': \"category\",\n",
    "    'lanes': \"category\",\n",
    "    'mixed_traffic_no_facil': \"boolean\",\n",
    "    'mixed_traffic_w_facil': \"boolean\"\n",
    "}\n",
    "links.set_index(['linkid','reverse_link'],inplace=True)\n",
    "turns.set_index(['source_linkid','source_reverse_link','target_linkid','target_reverse_link'],inplace=True)\n",
    "links.columns\n",
    "# #unit conversions\n",
    "#links['length_mi'] = (links['length_ft'] / 5280).round(2)\n",
    "#links['ascent_ft'] = (links['ascent_m'] * 3.28084).round(0)\n",
    "#links.drop(columns=['length_ft','ascent_m'],inplace=True)\n",
    "test_summary = [summarize_route.route_attributes(key,item,'modeled_edges',cols_to_summarize,links,turns) for key, item in full_set.items()]\n",
    "test_summary = summarize_route.procees_summary_results(test_summary,config['projected_crs_epsg'])\n",
    "test_summary.drop(columns=['tripid','geometry']).describe()\n",
    "train_summary = [summarize_route.route_attributes(key,item,'modeled_edges',cols_to_summarize,links,turns) for key, item in train_set.items()]\n",
    "train_summary = summarize_route.procees_summary_results(train_summary,config['projected_crs_epsg'])\n",
    "train_summary.drop(columns=['tripid','geometry']).describe()\n",
    "train_summary = [summarize_route.route_attributes(key,item,'modeled_edges',cols_to_summarize,links,turns) for key, item in train_set.items()]\n",
    "train_summary = summarize_route.procees_summary_results(train_summary,config['projected_crs_epsg'])\n",
    "train_summary.drop(columns=['tripid','geometry']).describe()\n",
    "summary = pd.concat([test_summary,train_summary],ignore_index=True)\n",
    "# summary.to_file(config['calibration_fp']/\"route_attributes.gpkg\",layer='modeled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the percent change in impedance at the link level for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impedance_change = links.copy()\n",
    "impedance_change['imp_prop'] = (impedance_change['link_cost'] - impedance_change['travel_time_min']) / impedance_change['travel_time_min']\n",
    "impedance_change['imp_prop'] = impedance_change['imp_prop'].round(3)\n",
    "impedance_change = impedance_change[impedance_change['reverse_link']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo automate the plot generation for when qgis isn't available\n",
    "impedance_change.plot('imp_prop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impedance_change.to_file(config['calibration_fp']/\"network_impedance_change.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impedance_change['imp_prop'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(impedance_change['imp_prop'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(impedance_change['imp_prop']==0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impedance_change.imp_prop.round(3).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot distribution of overlap values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#loss_data = pd.DataFrame({'loss_shortest_full':loss_shortest_full,'loss_full':loss_full})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Create the histogram\n",
    "#ax = loss_full_df.plot.hist(stacked=True, bins=20, figsize=(12, 12), color=['grey', 'lightgrey'])\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.hist(full['shortest'], bins=20, alpha=0.5, label='Shortest Path Overlap', color='grey')\n",
    "plt.hist(full['impedance'], bins=20, alpha=0.3, label='Calibrated Overlap', color='blue')\n",
    "\n",
    "# Adding labels, title, and legend with font size adjustments\n",
    "plt.xlabel('Overlap', fontsize=22)\n",
    "plt.ylabel(f'Frequency (N={full.shape[0]})', fontsize=22)\n",
    "#plt.title('Histogram of Training Losses', fontsize=16)\n",
    "#plt.legend(title='Tra Overlap', fontsize=22, title_fontsize=22)\n",
    "\n",
    "# Adjusting the font size of the tick labels\n",
    "plt.xticks(fontsize=22)\n",
    "plt.yticks(fontsize=22)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# loss_data = pd.DataFrame({'loss_shortest_train':loss_shortest_train,'loss_train':loss_train})\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# # Create the histogram\n",
    "# #ax = loss_train_df.plot.hist(stacked=True, bins=20, figsize=(12, 12), color=['grey', 'lightgrey'])\n",
    "\n",
    "# # Create the histogram\n",
    "# plt.figure(figsize=(12, 12))\n",
    "# plt.hist(loss_data['loss_shortest_train'], bins=20, alpha=0.5, label='Shortest Path Overlap', color='grey')\n",
    "# plt.hist(loss_data['loss_train'], bins=20, alpha=0.3, label='Calibrated Overlap', color='blue')\n",
    "\n",
    "# # Adding labels, title, and legend with font size adjustments\n",
    "# plt.xlabel('Overlap', fontsize=22)\n",
    "# plt.ylabel('Frequency', fontsize=22)\n",
    "# #plt.title('Histogram of Training Losses', fontsize=16)\n",
    "# plt.legend(title='Training Overlap', fontsize=22, title_fontsize=22)\n",
    "\n",
    "# # Adjusting the font size of the tick labels\n",
    "# plt.xticks(fontsize=22)\n",
    "# plt.yticks(fontsize=22)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_data = pd.DataFrame({'loss_shortest_test':loss_shortest_test,'loss_test':loss_test})\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# # Create the histogram\n",
    "# #ax = loss_train_df.plot.hist(stacked=True, bins=20, figsize=(12, 12), color=['grey', 'lightgrey'])\n",
    "\n",
    "# # Create the histogram\n",
    "# plt.figure(figsize=(12, 12))\n",
    "# plt.hist(loss_data['loss_shortest_test'], bins=20, alpha=0.5, label='Shortest Path Overlap', color='grey')\n",
    "# plt.hist(loss_data['loss_test'], bins=20, alpha=0.3, label='Calibrated Overlap', color='blue')\n",
    "\n",
    "# # Adding labels, title, and legend with font size adjustments\n",
    "# plt.xlabel('Overlap', fontsize=22)\n",
    "# plt.ylabel('Frequency', fontsize=22)\n",
    "# #plt.title('Histogram of Training Losses', fontsize=16)\n",
    "# plt.legend(title='Testing Overlap', fontsize=22, title_fontsize=22)\n",
    "\n",
    "# # Adjusting the font size of the tick labels\n",
    "# plt.xticks(fontsize=22)\n",
    "# plt.yticks(fontsize=22)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAQC (separate into a separate notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with (config['calibration_fp']/\"full_modeled_results.pkl\").open('rb') as fh:\n",
    "#     modeled_results = pickle.load(fh)\n",
    "# ## Add Route Attributes\n",
    "# import summarize_route\n",
    "# cols_to_summarize = {\n",
    "#     'facility_fwd': \"category\",\n",
    "#     'AADT': (\"threshold\",[10000]),\n",
    "#     'truck_pct': (\"threshold\",[5]),\n",
    "#     'here_speed': \"category\",\n",
    "#     'lanes': \"category\",\n",
    "#     'mixed_traffic_no_facil': \"boolean\",\n",
    "#     'mixed_traffic_w_facil': \"boolean\"\n",
    "# }\n",
    "# links.set_index(['linkid','reverse_link'],inplace=True)\n",
    "# turns.set_index(['source_linkid','source_reverse_link','target_linkid','target_reverse_link'],inplace=True)\n",
    "# links.columns\n",
    "# # #unit conversions\n",
    "# #links['length_mi'] = (links['length_ft'] / 5280).round(2)\n",
    "# #links['ascent_ft'] = (links['ascent_m'] * 3.28084).round(0)\n",
    "# #links.drop(columns=['length_ft','ascent_m'],inplace=True)\n",
    "# test_summary = [summarize_route.route_attributes(key,item,'modeled_edges',cols_to_summarize,links,turns) for key, item in full_set.items()]\n",
    "# test_summary = summarize_route.procees_summary_results(test_summary,config['projected_crs_epsg'])\n",
    "# test_summary.drop(columns=['tripid','geometry']).describe()\n",
    "# train_summary = [summarize_route.route_attributes(key,item,'modeled_edges',cols_to_summarize,links,turns) for key, item in train_set.items()]\n",
    "# train_summary = summarize_route.procees_summary_results(train_summary,config['projected_crs_epsg'])\n",
    "# train_summary.drop(columns=['tripid','geometry']).describe()\n",
    "# train_summary = [summarize_route.route_attributes(key,item,'modeled_edges',cols_to_summarize,links,turns) for key, item in train_set.items()]\n",
    "# train_summary = summarize_route.procees_summary_results(train_summary,config['projected_crs_epsg'])\n",
    "# train_summary.drop(columns=['tripid','geometry']).describe()\n",
    "# summary = pd.concat([test_summary,train_summary],ignore_index=True)\n",
    "# summary.to_file(config['calibration_fp']/\"route_attributes.gpkg\",layer='modeled')\n",
    "# summary.columns\n",
    "# summary = pd.concat([test_summary,train_summary],ignore_index=True)\n",
    "# summary.to_file(config['calibration_fp']/\"route_attributes.gpkg\",layer='modeled')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Regression Trees\n",
    "# from sklearn import tree\n",
    "\n",
    "# testing = pd.read_csv(config['calibration_fp']/'testing_results.csv')\n",
    "# training = pd.read_csv(config['calibration_fp']/'training_results.csv')\n",
    "\n",
    "# #assume that keys are in the right order?\n",
    "# loss_df = pd.concat([testing,training],ignore_index=True)\n",
    "# #import trip and user characteristics\n",
    "# trips_df = pd.read_pickle(cycleatl_fp/\"trips_3.pkl\")\n",
    "# users_df = pd.read_pickle(cycleatl_fp/\"users_1.pkl\")\n",
    "# trips_df.reset_index(drop=True,inplace=True)\n",
    "# #import route attributes\n",
    "# matched_summary = gpd.read_file(config['calibration_fp']/\"route_attributes.gpkg\",layer=\"matched\")\n",
    "# shortest_summary = gpd.read_file(config['calibration_fp']/\"route_attributes.gpkg\",layer=\"shortest\")\n",
    "# #consolidate trip types\n",
    "# trips_df.loc[trips_df['trip_type']=='other','trip_type'] = 'Other'\n",
    "# trips_df.loc[trips_df['trip_type']=='Work-related','trip_type'] = 'Work-Related'\n",
    "# trips_df['trip_type'].value_counts()\n",
    "# #replace userid with just the first one\n",
    "# def take_first(x):\n",
    "#     if isinstance(x,list):\n",
    "#         return x[0]\n",
    "#     return x\n",
    "# users_df['userid'] = users_df['userid'].apply(take_first)\n",
    "# #consolidate trip types\n",
    "# trips_df.loc[trips_df['trip_type']=='other','trip_type'] = 'Other'\n",
    "# trips_df.loc[trips_df['trip_type']=='Work-related','trip_type'] = 'Work-Related'\n",
    "# trips_df['trip_type'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Tree #1\n",
    "# - First tree is on the non-null variables\n",
    "# - The dist. between shortest and impedance were similar and so are the trees\n",
    "# - Shorter trips better explained by impedance/shortest path which makes sense\n",
    "#     - Use this to split longer trips? and retrain?\n",
    "# - Shopping is the only significant trip type variable\n",
    "# - Speed above 9 mph is usually better explained by impedance\n",
    "# ## Tree #1\n",
    "# - First tree is on the non-null variables\n",
    "# - The dist. between shortest and impedance were similar and so are the trees\n",
    "# - Shorter trips better explained by impedance/shortest path which makes sense\n",
    "#     - Use this to split longer trips? and retrain?\n",
    "# - Shopping is the only significant trip type variable\n",
    "# - Speed above 9 mph is usually better explained by impedance\n",
    "# nonulls = ['trip_type','length_mi','avg_speed_mph','(0,4]_prop', '(4,8]_prop',\n",
    "#        '(8,inf]_prop', 'AADT_10000_prop', 'facility_fwd_bike lane_prop',\n",
    "#        'facility_fwd_cycletrack_prop', 'facility_fwd_multi use path_prop',\n",
    "#        'facility_fwd_sharrow_prop', 'here_speed_1_prop', 'here_speed_2_prop',\n",
    "#        'here_speed_3_prop', 'here_speed_4_prop', 'lanes_1_prop',\n",
    "#        'lanes_2_prop', 'lanes_3_prop', 'left', 'right', 'signalized',\n",
    "#        'straight', 'truck_pct_5_prop', 'uturn']\n",
    "# nonulls_tree_df = merged[nonulls]\n",
    "# from sklearn.tree import plot_tree\n",
    "\n",
    "# #convert nominal categorical to numeric\n",
    "# one_hot_data = pd.get_dummies(nonulls_tree_df,drop_first=True)\n",
    "# one_hot_data.columns\n",
    "# X, y = one_hot_data.values, merged['impedance'].values\n",
    "# clf = tree.DecisionTreeRegressor(max_depth=3,min_samples_split=50)\n",
    "# clf = clf.fit(X, y)\n",
    "# fig, ax = plt.subplots(figsize=(20, 10), dpi=300)\n",
    "# plot_tree(clf, feature_names=one_hot_data.columns, filled=True, ax=ax)\n",
    "# plt.show()\n",
    "# X, y = one_hot_data.values, merged['impedance'].values\n",
    "# clf = tree.DecisionTreeRegressor(max_depth=5)\n",
    "# clf = clf.fit(X, y)\n",
    "# fig, ax = plt.subplots(figsize=(20, 10), dpi=300)\n",
    "# plot_tree(clf, feature_names=one_hot_data.columns, filled=True, ax=ax)\n",
    "# plt.show()\n",
    "# how to compare predicted values vs actual?\n",
    "# SS_res = ((y - clf.predict(X))**2).sum()\n",
    "# SS_tot = ((y - y.mean())**2).sum()\n",
    "# R2 = 1 - (SS_res/SS_tot)\n",
    "# R2\n",
    "# ## Tree #2\n",
    "# This one takes the previous variables and adds the user characterstics. Sample size is smaller due to null values\n",
    "\n",
    "# - Looks like trip distance is still the dominant one here, really think i should start there\n",
    "# - When removing trip distance and avg speed, trip type and age 55+ are the best\n",
    "\n",
    "# **NOTE:** just noticed that age should not be dummies, need to fix that and try again\n",
    "# import json\n",
    "# user_data_definitions = json.load((Path.home()/'Documents/GitHub/cycleatlanta/user_data_definition.json').open('rb'))\n",
    "\n",
    "# #add the 55+ column\n",
    "# user_data_definitions['age']['6'] = '55+'\n",
    "# #income has too many nulls\n",
    "# tree_cols = ['age','gender','rider_history','rider_type'] + nonulls #,'total_distance_ft','avg_speed_mph','count']#,'count']#[,'cycling_freq'\n",
    "# tree_df = merged[tree_cols]\n",
    "\n",
    "# #use to detect null values\n",
    "# isnull = ((tree_df == -1) | (tree_df == 'NULL'))\n",
    "\n",
    "# #TODO do cross-sectionals to see which combination results in the most retained entries\n",
    "# #remove rows with null values\n",
    "# tree_df = tree_df[(isnull==False).all(axis=1)]\n",
    "\n",
    "# loss_vals = merged.loc[tree_df.index]\n",
    "# get_factor = ['age','rider_history','rider_type']\n",
    "# # just fyi\n",
    "# # select_max_cols = ['age','income','cycling_freq']\n",
    "# # #select the min for these (i.e. strong and fearless over interested but...)\n",
    "# # select_min_cols = ['rider_type','rider_history']\n",
    "\n",
    "# for col in get_factor:\n",
    "#     ivd = {v:k for k, v in user_data_definitions[col].items()}\n",
    "#     tree_df[col] = tree_df[col].map(ivd)\n",
    "# tree_df\n",
    "# #this is where i left off\n",
    "# #convert nominal categorical to numeric\n",
    "# dummy_cols = ['gender','trip_type']\n",
    "# one_hot_data = pd.get_dummies(tree_df[dummy_cols],drop_first=True)\n",
    "# comb = pd.concat([tree_df.drop(columns=dummy_cols),one_hot_data],ignore_index=False,axis=1)\n",
    "# comb\n",
    "# X, y = comb.values, loss_vals['impedance'].values\n",
    "# clf = tree.DecisionTreeRegressor(max_depth=3,min_samples_split=50)\n",
    "# clf = clf.fit(X, y)\n",
    "# #tree.plot_tree(clf,feature_names=one_hot_data.columns)\n",
    "# # Visualize the tree with higher resolution\n",
    "# plt.figure(figsize=(20, 10), dpi=300)\n",
    "# from sklearn.tree import plot_tree\n",
    "# plot_tree(clf, feature_names=comb.columns, filled=True)\n",
    "# plt.show()\n",
    "# # how to compare predicted values vs actual?\n",
    "# SS_res = ((y - clf.predict(X))**2).sum()\n",
    "# SS_tot = ((y - y.mean())**2).sum()\n",
    "# R2 = 1 - (SS_res/SS_tot)\n",
    "# R2\n",
    "# What we actually want to know, is what variables help increase the overlap? So wouldn't that be more of an application for linear regression?\n",
    "\n",
    "# clf\n",
    "# Since the squared error is already pretty high and our histogram tells us, most of these trips are not currently well predicted with the current impedances. The idea behind using regression trees here is can we figure out if attributes help exmplain the bad overlap?\n",
    "\n",
    "# Or we could just try knocking off \n",
    "# #correlation matrix\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Calculate the correlation matrix\n",
    "# correlation_matrix = one_hot_data.corr()\n",
    "\n",
    "# # Display the correlation matrix\n",
    "# print(\"Correlation Matrix:\")\n",
    "# print(correlation_matrix)\n",
    "\n",
    "# # Plot the correlation matrix using seaborn heatmap\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "# plt.title('Correlation Matrix Heatmap')\n",
    "# plt.show()\n",
    "\n",
    "# one_hot_data.corr()\n",
    "# # Distribution of Loss Function\n",
    "# # Export to get route attributes\n",
    "# # Using BRouter Results\n",
    "# To compare across we'll use Frechet distance. Will need to use next time.\n",
    "# with (config['calibration_fp']/'brouter_links.txt').open('r') as file:\n",
    "#     my_list = file.readlines()\n",
    "#     # Remove any extra whitespace or newline characters\n",
    "#     my_list = [line.strip() for line in my_list]\n",
    "# len(my_list)\n",
    "# geojsons = list((config['calibration_fp']/'GeoJSON_Out').glob('*.geojson'))\n",
    "# both_ods = list(set.union(set(train_ods),set(test_ods)))\n",
    "# len(both_ods)\n",
    "# test_ods\n",
    "# len(geojsons)\n",
    "# #use the results dict combined with the geo dict one\n",
    "# results_dict[(68166811, 8789832117)]\n",
    "# #\n",
    "# geojson_geos = []\n",
    "# for geojson in geojsons:\n",
    "#     geojson_geo = gpd.read_file(geojson).to_crs(config['projected_crs_epsg'])\n",
    "#     geojson_geo = np.array(geojson_geo.geometry.item().coords)\n",
    "#     # geojson_geo = [(x, y) for x, y, z in geojson_geo.coords]\n",
    "#     # geojson_geo = LineString(geojson_geo)\n",
    "#     geojson_geos.append(geojson_geo)\n",
    "# frechet_distance = similaritymeasures.frechet_dist(chosen_coords,modeled_coords)\n",
    "# #import and compare frechet distance across them with geodict?\n",
    "# list(geojson_geo.coords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
