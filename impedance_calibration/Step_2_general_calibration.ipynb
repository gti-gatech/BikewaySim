{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impedance Calibration Test Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview:**\n",
    "1. Network Preparation\n",
    "1. Import Train and Test Sets\n",
    "2. Specify Calibration Parameters\n",
    "    - Link Impedance Function\n",
    "    - Turn Impedance Function\n",
    "    - Objective/Loss Function\n",
    "        - First Preference Recovery\n",
    "        - Exact Overlap\n",
    "        - Buffer Overlap (in development)\n",
    "        - Frechet Distance/Area (in development)\n",
    "3. Run Calibration\n",
    "    - Particle Swarm Optimization (constrained & non-probabilistic)\n",
    "    - Maximum likelihood estimation (unconstrained & probabilistic, in development)\n",
    "4. Export and Run Post Calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from stochopy.optimize import minimize\n",
    "import stochastic_optimization\n",
    "from tqdm import tqdm\n",
    "import similaritymeasures\n",
    "import random\n",
    "\n",
    "from shapely.ops import LineString, MultiLineString\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,str(Path.cwd().parent))\n",
    "import file_structure_setup\n",
    "config = file_structure_setup.filepaths()\n",
    "\n",
    "from network.src import modeling_turns\n",
    "import speedfactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export calibration network\n",
    "with (config['calibration_fp']/\"calibration_network.pkl\").open('rb') as fh:\n",
    "    links, turns = pickle.load(fh)\n",
    "turn_G = modeling_turns.make_turn_graph(turns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = gpd.read_file(config['network_fp']/\"final_network.gpkg\",layer=\"edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dicts for referencing certain link attributes quickly\n",
    "length_dict = dict(zip(all_links['linkid'],all_links['length_ft'])) # need this for loss function\n",
    "geo_dict = dict(zip(all_links['linkid'],all_links['geometry']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove bike facilities that were installed after the data collection\n",
    "In the future, this will be done dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for comparison\n",
    "links['future_facility'] = links['facility_fwd']\n",
    "\n",
    "# Remove on-street bike facilities that were present after 2016.\n",
    "links.loc[(links['year'] > 2016) & links['year'].notna() & (links['link_type']=='road'),'facility_fwd'] = None\n",
    "\n",
    "# Remove cycletracks/mups if present after 2016\n",
    "cond = (links['year'] > 2016) & links['year'].notna() & (links['link_type']=='bike')\n",
    "removed = links[cond]\n",
    "print(removed.shape[0],'cycletracks/mup removed')\n",
    "# links = links[cond==False]\n",
    "\n",
    "# Remove on-street bike facilities if no date\n",
    "\n",
    "\n",
    "# new = set(links['linkid'].tolist())\n",
    "# turns = turns[turns['source_linkid'].isin(new) ]\n",
    "\n",
    "# if no date but bike lane or cycletrack remove\n",
    "links.loc[links['facility_fwd'].isin(['bike lane','bufferred bike lane','cycletrack']) & links['year'].isna(),'facility_fwd'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the removed links look correct\n",
    "# removed.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removed facilities\n",
    "# links.loc[(links['future_facility']!=links['facility_fwd']) & \\\n",
    "#           links['future_facility'].notna(),\n",
    "#           ['future_facility','facility_fwd','geometry']].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #remaining infra\n",
    "# links.loc[links['facility_fwd'].notna(),['facility_fwd','year','geometry']].explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Link Impedance Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links[links['facility_fwd'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links['multi use path'] = links['facility_fwd'].isin(['multi use path','cycletrack']).astype(int)\n",
    "links.loc[links['multi use path']==True,'lanes'] = 0\n",
    "\n",
    "links['bike lane'] = links['facility_fwd'].isin(['bike lane','bufferred bike lane']).astype(int)\n",
    "#links.loc[links['bike lane']==True,'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_links = {\n",
    "    0 : 'multi use path',\n",
    "    1 : 'bike lane',\n",
    "    2 : 'lanes',\n",
    "    3 : 'above_4'\n",
    "} \n",
    "\n",
    "betas_turns = {\n",
    "    4 : 'unsig_major_road_crossing'\n",
    "}\n",
    "\n",
    "# #this was only .14 overlap\n",
    "# betas_links = {\n",
    "#     0 : 'multi use path',\n",
    "#     1 : 'bike lane',\n",
    "#     2 : 'AADT',\n",
    "#     3 : 'above_4'\n",
    "# } \n",
    "\n",
    "# betas_turns = {\n",
    "#     4 : 'unsig_major_road_crossing'\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# #have position of beta next to name of variable\n",
    "# #NOTE: keys must be in the currect order used\n",
    "# betas_links = {\n",
    "#     0 : 'mixed_traffic_no_facil',\n",
    "#     1 : 'mixed_traffic_w_facil',\n",
    "#     #0 : 'major_road_w_class_2',\n",
    "#     # 1 : 'minor_road_w_class_2',\n",
    "#     # 2 : 'major_road_no_facil',\n",
    "#     # 3 : 'minor_road_no_facil',\n",
    "#     2 : 'above_4'\n",
    "#     #1 : 'motorized'\n",
    "#     #1 : 'ascent_grade_%'\n",
    "# } \n",
    "\n",
    "# betas_turns = {\n",
    "#     3 : 'unsig_major_road_crossing'\n",
    "#     #1 : 'left',\n",
    "#     #2 : 'right',\n",
    "#     #3 : 'signalized'\n",
    "# }\n",
    "\n",
    "\n",
    "# #have position of beta next to name of variable\n",
    "# #NOTE: keys must be in the currect order used\n",
    "# #TODO have this be named tuples or something similar\n",
    "# # (name=var_name,type,position=position,bounds=[0,3])\n",
    "# betas_links = {\n",
    "#     0 : 'AADT',\n",
    "#     1 : 'lanes',\n",
    "#     2 : 'here_speed',\n",
    "#     3 : 'above_4'\n",
    "# } \n",
    "\n",
    "# betas_turns = {\n",
    "#     4 : 'unsig_major_road_crossing',\n",
    "#     5 : 'signalized'\n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['calibration_fp']/'full_set.pkl').open('rb') as fh:\n",
    "    train_set = pickle.load(fh)\n",
    "\n",
    "train_ods = stochastic_optimization.match_results_to_ods(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with (config['calibration_fp']/'test_set.pkl').open('rb') as fh:\n",
    "#     test_set = pickle.load(fh)\n",
    "# with (config['calibration_fp']/'train_set.pkl').open('rb') as fh:\n",
    "#     train_set = pickle.load(fh)\n",
    "\n",
    "# # match the ods to the network\n",
    "# train_ods = stochastic_optimization.match_results_to_ods(train_set)\n",
    "# test_ods = stochastic_optimization.match_results_to_ods(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both_ods = list(set.union(set(train_ods),set(test_ods)))\n",
    "# html = \"\"\n",
    "# nodes = gpd.read_file(config['network_fp']/'final_network.gpkg',layer='nodes')\n",
    "# nodes.to_crs('epsg:4236',inplace=True)\n",
    "# nodes['lon'] = nodes.geometry.x\n",
    "# nodes['lat'] = nodes.geometry.y\n",
    "# latlon = tuple(zip(nodes['lon'],nodes['lat']))\n",
    "# nodes = dict(zip(nodes['N'],latlon))\n",
    "# nodes.get(68196100,0)\n",
    "# htmls = []\n",
    "# for od in both_ods:\n",
    "#     start = od[0]\n",
    "#     end = od[1]\n",
    "#     start_lonlat = nodes.get(start,0)\n",
    "#     end_lonlat = nodes.get(end,0)\n",
    "#     html = f\"https://brouter.damsy.net/latest/#map=12/33.7522/-84.3892/standard&lonlats={start_lonlat[1]},{start_lonlat[0]};{end_lonlat[1]},{end_lonlat[0]}&profile=safety\"\n",
    "#     htmls.append(html)\n",
    "# with (config['calibration_fp']/\"brouter_links.txt\").open('w') as fh:\n",
    "#     for html in htmls:\n",
    "#         fh.write(f\"{html}\\n\")\n",
    "# with (config['calibration_fp']/\"brouter_ods.txt\").open('w') as fh:\n",
    "#     for od in both_ods:\n",
    "#         fh.write(f\"{od}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf = train_set[random_trip]['matched_edges']\n",
    "# gdf['geometry'] = gdf['linkid'].map(geo_dict)\n",
    "# gdf = gpd.GeoDataFrame(gdf,crs=config['projected_crs_epsg'])\n",
    "# gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_impedance_col = \"travel_time_min\"\n",
    "loss_function = stochastic_optimization.jaccard_index\n",
    "loss_function_kwargs = {'length_dict':length_dict}#,'overlap_threshold':0.80}\n",
    "\n",
    "# loss_function = stochastic_optimization.buffer_overlap\n",
    "# loss_function_kwargs = {'geo_dict':geo_dict,'buffer_ft':100,'standardize':True}\n",
    "\n",
    "# link coefficients control the % increase in link travel time (units don't matter)\n",
    "# turn coefficients control the amount of seconds added from the turn (units matter)\n",
    "link_bounds = [[-1,0],[-1,0],[0,4],[0,4]]\n",
    "#[[-1, 2] for _ in range(0, len(betas_links))]\n",
    "turn_bounds = [[0, 4] for _ in range(0, len(betas_turns))]\n",
    "if (len(betas_links) > 0) & (len(betas_turns) > 0):\n",
    "    bounds = np.vstack([link_bounds,turn_bounds])\n",
    "elif (len(betas_links) > 0):\n",
    "    bounds = link_bounds\n",
    "elif (len(betas_turns) > 0):\n",
    "    bounds = turn_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_betas = []\n",
    "past_vals = []\n",
    "args = (\n",
    "    past_betas,\n",
    "    past_vals,\n",
    "    betas_links,betas_turns,\n",
    "    train_ods,train_set,\n",
    "    stochastic_optimization.link_impedance_function,\n",
    "    base_impedance_col,\n",
    "    stochastic_optimization.turn_impedance_function,\n",
    "    links,turns,turn_G,\n",
    "    loss_function,\n",
    "    loss_function_kwargs,\n",
    "    True #whether to print the results of each iteration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impedance Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO save the list of tripids and save the iterations of the sampled imepdances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(stochastic_optimization)\n",
    "\n",
    "start = time.time()\n",
    "print(list(betas_links.values())+list(betas_turns.values())+['objective_function'])\n",
    "x = minimize(stochastic_optimization.impedance_calibration, bounds, args=args, method='pso', options={'maxiter':50,\"popsize\":5})\n",
    "end = time.time()\n",
    "print(f'Took {(end-start)/60/60:.2f} hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('high stress,','ascent grade %,','left,','right,','signalized,','val')\n",
    "print(list(betas_links.values())+list(betas_turns.values())+['objective_function'])\n",
    "print(past_betas[np.array(past_vals).argmin()],np.array(past_vals).min().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution of loss function values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_betas = {**betas_links, **betas_turns}\n",
    "calibration_result = {}\n",
    "#get the best betas\n",
    "best_coefs = past_betas[np.array(past_vals).argmin()]\n",
    "best_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, item in combined_betas.items():\n",
    "    calibration_result[item] = best_coefs[key]\n",
    "\n",
    "calibration_result['loss'] = np.array(past_vals).min()\n",
    "calibration_result['beta_links'] = betas_links\n",
    "calibration_result['beta_turns'] = betas_turns\n",
    "calibration_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export coefficents\n",
    "if (config['calibration_fp']/\"calibration_results.pkl\").exists():\n",
    "    with (config['calibration_fp']/\"calibration_results.pkl\").open('rb') as fh:\n",
    "        calibration_results = pickle.load(fh)\n",
    "else:\n",
    "    calibration_results = []\n",
    "calibration_results.append(calibration_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del calibration_results[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['calibration_fp']/\"calibration_results.pkl\").open('wb') as fh:\n",
    "        pickle.dump(calibration_results,fh)\n",
    "calibration_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create GIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from io import BytesIO\n",
    "\n",
    "# Function to plot a GeoSeries and save the plot\n",
    "def plot_geoseries(geoseries,other_geoseries,i,past_val):\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    #cx.add_basemap(ax)\n",
    "    other_geoseries.plot(ax=ax,color='blue',style_kwds={'linewidth':2})\n",
    "    geoseries.plot(ax=ax,color='red')\n",
    "    ax.set_title(f\"Iter:{i} Overlap Function:{past_val}\")\n",
    "    ax.set_axis_off()\n",
    "    img_bytes = BytesIO()\n",
    "    plt.savefig(img_bytes, format='png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return img_bytes.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_trips = 10\n",
    "\n",
    "# for z in range(0,num_trips):\n",
    "\n",
    "#     #choose a random tripid\n",
    "#     tripid = random.choice(list(train_set.keys()))\n",
    "#     start_node = train_set[tripid]['start_node']\n",
    "#     end_node = train_set[tripid]['end_node']\n",
    "\n",
    "#     matched_edges = train_set[tripid]['matched_edges']\n",
    "#     matched_edges = np.array(matched_edges)\n",
    "#     matched_line = MultiLineString([geo_dict[linkid] for linkid, reverse_link in matched_edges])\n",
    "#     matched_line = gpd.GeoSeries(matched_line,crs='epsg:2240')\n",
    "#     matched_line = matched_line.to_crs('epsg:4326')\n",
    "\n",
    "#     modeled_lines = []\n",
    "\n",
    "#     for betas in past_betas:\n",
    "#         #update network with the correct impedances\n",
    "#         stochastic_optimization.impedance_update(betas,betas_links,betas_turns,\n",
    "#                                 link_impedance_function,\n",
    "#                                 turn_impedance_function,\n",
    "#                                 links,turns,turn_G)\n",
    "#         #find shortest path\n",
    "#         modeled_edges = stochastic_optimization.impedance_path(turns,turn_G,start_node,end_node)['edge_list']\n",
    "#         modeled_line = MultiLineString([geo_dict[linkid] for linkid, reverse_link in modeled_edges])\n",
    "#         modeled_line = gpd.GeoSeries(modeled_line,crs='epsg:2240')\n",
    "#         modeled_line = modeled_line.to_crs('epsg:4326')\n",
    "#         modeled_lines.append(modeled_line)\n",
    "\n",
    "#     # List of GeoSeries (Replace this with your own GeoSeries list)\n",
    "#     geoseries_list = modeled_lines\n",
    "\n",
    "#     # Loop through the list of GeoSeries, plot each one, and save the plot\n",
    "#     images = []\n",
    "#     for i, geoseries in enumerate(geoseries_list):\n",
    "#         past_val = past_vals[i]\n",
    "#         image_bytes = plot_geoseries(geoseries,matched_line,i,past_val)\n",
    "#         images.append(imageio.imread(BytesIO(image_bytes)))\n",
    "\n",
    "#     # Path for saving the GIF\n",
    "#     gif_path = f\"animations/stress_animation_{z}.gif\"\n",
    "\n",
    "#     # Save the images as a GIF\n",
    "#     imageio.mimsave(Path.cwd()/gif_path, images, format='gif', duration=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(stochastic_optimization)\n",
    "\n",
    "with (config['calibration_fp']/\"calibration_results.pkl\").open('rb') as fh:\n",
    "    calibration_results = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #link_impedance_col = \"adj_travel_time_min\"\n",
    "# stochastic_optimization.back_to_base_impedance(base_impedance_col,links,turns,turn_G)\n",
    "\n",
    "# #update impedances\n",
    "# betas = past_betas[np.array(past_vals).argmin()]#x.x\n",
    "# print(betas)\n",
    "# stochastic_optimization.impedance_update(betas,betas_links,betas_turns,\n",
    "#                           link_impedance_function,\n",
    "#                           base_impedance_col,\n",
    "#                           turn_impedance_function,\n",
    "#                           links,turns,turn_G)\n",
    "\n",
    "# #find shortest path\n",
    "# results_dict = {(start_node,end_node):stochastic_optimization.impedance_path(turns,turn_G,start_node,end_node) for start_node, end_node in test_ods}\n",
    "\n",
    "# #calulate objective function\n",
    "# val_to_minimize = loss_function(test_set,results_dict,**loss_function_kwargs)\n",
    "# val_to_minimize.mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link_impedance_col = \"adj_travel_time_min\"\n",
    "base_impedance_col = \"travel_time_min\"\n",
    "stochastic_optimization.back_to_base_impedance(base_impedance_col,links,turns,turn_G)\n",
    "\n",
    "#update impedances\n",
    "betas = past_betas[np.array(past_vals).argmin()]#x.x\n",
    "print(betas)\n",
    "stochastic_optimization.impedance_update(betas,betas_links,betas_turns,\n",
    "                          link_impedance_function,\n",
    "                          base_impedance_col,\n",
    "                          turn_impedance_function,\n",
    "                          links,turns,turn_G)\n",
    "\n",
    "#find shortest path\n",
    "results_dict = {(start_node,end_node):stochastic_optimization.impedance_path(turns,turn_G,start_node,end_node) for start_node, end_node in train_ods}\n",
    "\n",
    "#calulate objective function\n",
    "val_to_minimize = loss_function(train_set,results_dict,**loss_function_kwargs)\n",
    "val_to_minimize.mean().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize random trip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These did well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_results = stochastic_optimization.first_preference_recovery(train_set,results_dict,**{'length_dict':length_dict,'overlap_threshold':0.7})\n",
    "fpr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "tripid = random.choice(fpr_results)\n",
    "tripid\n",
    "#retrieve chosen path linkids and convert them to tuple\n",
    "chosen = [tuple(row) for row in train_set[tripid]['matched_edges'].to_numpy()]\n",
    "shortest = [tuple(row) for row in train_set[tripid]['shortest_edges'].to_numpy()]\n",
    "\n",
    "#retrieve modeled path linkids\n",
    "start_node = train_set[tripid]['origin_node']\n",
    "end_node = train_set[tripid]['destination_node']\n",
    "modeled_edges = results_dict[(start_node,end_node)]['edge_list']\n",
    "\n",
    "#get geos (non-directional)\n",
    "chosen_geo = [geo_dict[linkid[0]] for linkid in chosen]\n",
    "shortest_geo = [geo_dict[linkid[0]] for linkid in shortest]\n",
    "modeled_geo = [geo_dict[linkid[0]] for linkid in modeled_edges]\n",
    "\n",
    "chosen_lines = gpd.GeoSeries(chosen_geo,crs='epsg:2240')\n",
    "shortest_lines = gpd.GeoSeries(shortest_geo,crs='epsg:2240')\n",
    "modeled_lines = gpd.GeoSeries(modeled_geo,crs='epsg:2240')\n",
    "\n",
    "stochastic_optimization.visualize_three_no_legend(chosen_lines,shortest_lines,modeled_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and these not so much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "not_good = list(set(test_set.keys()) - set(fpr_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripid = random.choice(not_good)\n",
    "tripid\n",
    "#retrieve chosen path linkids and convert them to tuple\n",
    "chosen = [tuple(row) for row in test_set[tripid]['matched_edges'].to_numpy()]\n",
    "shortest = [tuple(row) for row in test_set[tripid]['shortest_edges'].to_numpy()]\n",
    "\n",
    "#retrieve modeled path linkids\n",
    "start_node = test_set[tripid]['origin_node']\n",
    "end_node = test_set[tripid]['destination_node']\n",
    "modeled_edges = results_dict[(start_node,end_node)]['edge_list']\n",
    "\n",
    "#get geos (non-directional)\n",
    "chosen_geo = [geo_dict[linkid[0]] for linkid in chosen]\n",
    "shortest_geo = [geo_dict[linkid[0]] for linkid in shortest]\n",
    "modeled_geo = [geo_dict[linkid[0]] for linkid in modeled_edges]\n",
    "\n",
    "chosen_lines = gpd.GeoSeries(chosen_geo,crs='epsg:2240')\n",
    "shortest_lines = gpd.GeoSeries(shortest_geo,crs='epsg:2240')\n",
    "modeled_lines = gpd.GeoSeries(modeled_geo,crs='epsg:2240')\n",
    "\n",
    "stochastic_optimization.visualize_three_no_legend(chosen_lines,shortest_lines,modeled_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
