{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from stochopy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "import similaritymeasures\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.ops import Point, MultiLineString, LineString\n",
    "from importlib import reload\n",
    "import datetime\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "np.set_printoptions(suppress=True)\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "from bikewaysim.paths import config, stadia_toner, maptiler_streets\n",
    "from bikewaysim.impedance_calibration import stochastic_optimization, speedfactor\n",
    "from bikewaysim.network import modeling_turns\n",
    "from bikewaysim.routing import rustworkx_routing_funcs\n",
    "from bikewaysim.impedance_calibration import optimization_viz, utils, post_calibration, impedance_functions, loss_functions\n",
    "\n",
    "from step_1_calibration_experiments import all_calibrations, full_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Validation\n",
    "- Retrieve loss values for testing and training set for each fold\n",
    "- Create confidence intervals of the betas from the bootstrapped results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_vals = post_calibration.aggregated_loss_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_loss_vals = loss_vals[loss_vals['subset'].str.contains('fold_') & (loss_vals['run_num'] == '0')]\n",
    "# testing_loss_vals = post_calibration.testing_aggregated_loss_dataframe()\n",
    "# testing_loss_vals = testing_loss_vals[testing_loss_vals['subset']!='bootstrap_final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['subset','jaccard_buffer_mean','shortest_jaccard_buffer_mean']\n",
    "# training_loss_vals = training_loss_vals[cols].sort_values('subset').set_index('subset')\n",
    "# training_loss_vals.columns = 'train_' + training_loss_vals.columns\n",
    "# testing_loss_vals = testing_loss_vals[cols].sort_values('subset').set_index('subset')\n",
    "# testing_loss_vals.columns = 'test_' + testing_loss_vals.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print the results of each\n",
    "# print(\"Training Overlap Values:\",str(training_loss_vals.sort_values('subset')['train_jaccard_buffer_mean'].tolist()))\n",
    "# print(\"Testing Overlap Values:\",str(testing_loss_vals.sort_values('subset')['test_jaccard_buffer_mean'].tolist()))\n",
    "\n",
    "# # report the averages\n",
    "# print(\"Training Overlap Mean:\",training_loss_vals['train_jaccard_buffer_mean'].mean().round(2))\n",
    "# print(\"Testing Overlap Mean:\",testing_loss_vals['test_jaccard_buffer_mean'].mean().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold_results = pd.concat([training_loss_vals,testing_loss_vals],ignore_index=False,axis=1)\n",
    "# mean_row = kfold_results.mean().to_frame().T\n",
    "# mean_row.index = ['mean']\n",
    "# kfold_results = pd.concat([kfold_results,mean_row])\n",
    "# kfold_results.to_csv(config['scratch_fp']/'kfold_results.csv')\n",
    "# kfold_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Bootstrapping Results\n",
    "- Need average betas for re-routing trips\n",
    "- Need to produce the overlap comparison between the shortest path and modeled route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_labels = { \n",
    "    '2lpd':'2 Lanes per Direction',\n",
    "    '3+lpd':'3+ Lanes per Direction',\n",
    "    '(30,inf) mph':'> 30 MPH',\n",
    "    '[4,6) grade':'[4%,6%) Grade',\n",
    "    '[6,inf) grade':'> 6% Grade',\n",
    "    'bike lane': 'Bike Lane',\n",
    "    'cycletrack': 'Cycletrack',\n",
    "    'multi use path': 'Multi-Use Path',\n",
    "    'unsig_crossing': 'Unsignalized',\n",
    "    'left_turn': 'Left Turn',\n",
    "    'right_turn': 'Right Turn'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import any of the results dicts\n",
    "bootstrap_model = list((config['calibration_fp'] / 'results').glob('bootsample_*.pkl'))[0]\n",
    "with bootstrap_model.open('rb') as fh:\n",
    "    bootstrap_model = pickle.load(fh)\n",
    "betas = [x['col'] for x in bootstrap_model['betas_tup']] # beta column names\n",
    "left_limit_lst = [x['range'][0] for x in bootstrap_model['betas_tup']] # left limits\n",
    "right_limit_lst = [x['range'][1] for x in bootstrap_model['betas_tup']] # right limits\n",
    "\n",
    "# import bootstrap betas\n",
    "beta_vals = post_calibration.betas_dataframe()\n",
    "beta_vals = beta_vals[beta_vals['subset'].str.contains('bootsample')].dropna(axis=1,how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model using median values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the beta vals with the mean vals\n",
    "mean_betas = beta_vals[betas].median().to_dict()\n",
    "new_beta_tup = []\n",
    "for x in bootstrap_model['betas_tup']:\n",
    "    x['beta'] = mean_betas.get(x['col'])\n",
    "    new_beta_tup.append(x)\n",
    "bootstrap_model['betas_tup'] = tuple(new_beta_tup)\n",
    "\n",
    "# replace trips calibrated with the trips in the random subset\n",
    "with (config['calibration_fp']/'subsets.pkl').open('rb') as fh:\n",
    "    subset = pickle.load(fh)\n",
    "subset = [x for x in subset if 'random' == x[0]][0][1]\n",
    "bootstrap_model['trips_calibrated'] = set(subset)\n",
    "\n",
    "with (config['calibration_fp'] / 'results/bootstrap_final,validation,0.pkl').open('wb') as fh:\n",
    "    pickle.dump(bootstrap_model,fh)\n",
    "\n",
    "## Make sure to run the post_calibration script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bootstrap results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = beta_vals[betas].mean()\n",
    "median = beta_vals[betas].median()\n",
    "se = beta_vals[betas].std()\n",
    "confidence = 0.95\n",
    "lower_bound_ci = beta_vals[betas].quantile((1-confidence) / 2)\n",
    "upper_bound_ci = beta_vals[betas].quantile(1 - ((1-confidence) / 2))\n",
    "bootstrap_result = pd.concat([mean,se,median,lower_bound_ci,upper_bound_ci],axis=1,ignore_index=False)\n",
    "bootstrap_result.columns = ['mean','standard_error','median','lower_bound_ci','upper_bound_ci']\n",
    "bootstrap_result = bootstrap_result.round(2)\n",
    "bootstrap_result.to_csv(config['scratch_fp']/'bootstrap_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram for the betas\n",
    "- Need search range lines\n",
    "- Need 2.5 and 97.5 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound_ci = lower_bound_ci.tolist()\n",
    "upper_bound_ci = upper_bound_ci.tolist()\n",
    "\n",
    "# Plot histogram for chosen_minus_modeled\n",
    "min_val = np.floor(beta_vals[betas].min().min()) - 1\n",
    "max_val = np.ceil(beta_vals[betas].max().max()) + 1\n",
    "bin_size = 0.1\n",
    "\n",
    "for beta, left_lim, right_lim, left_ci, right_ci in zip(betas,left_limit_lst,right_limit_lst,lower_bound_ci,upper_bound_ci):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.hist(beta_vals[beta], bins=np.arange(min_val, max_val + bin_size, bin_size), alpha=0.5, color='grey', label=None, edgecolor='Black')\n",
    "    ax.set_title(beta_labels[beta])\n",
    "    ax.set_xlabel(f'Beta Value (Bin Size = {bin_size})')\n",
    "    ax.set_ylabel(f'Frequency (N={beta_vals.shape[0]})')\n",
    "    ax.set_xlim(min_val,max_val)\n",
    "    ax.set_ylim(0,300)\n",
    "\n",
    "    # search limits\n",
    "    ax.axvline(x=left_lim, color='red', linestyle='solid', alpha=0.5, label = 'Search Limits')\n",
    "    ax.axvline(x=right_lim, color='red', alpha=0.5, linestyle='solid')\n",
    "\n",
    "    # # confidence intervals\n",
    "    ax.axvline(x=left_ci, color='black', linestyle='dashdot', label = 'Confidence Interval')\n",
    "    ax.axvline(x=right_ci, color='black', linestyle='dashdot')\n",
    "\n",
    "    # Set major ticks every 0.5 and minor ticks every 0.1\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(0.5))\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "\n",
    "    # Optionally, customize the tick labels if you need only certain major ticks labeled\n",
    "    ax.set_xticklabels([f\"{tick:.1f}\" if i % 2 == 1 else '' for i, tick in enumerate(ax.get_xticks())])\n",
    "\n",
    "    # TODO add legend that has the confidence interval labels\n",
    "    ax.legend()\n",
    "\n",
    "    # Add vertical lines for the search limits\n",
    "    plt.savefig(config['figures_fp']/f\"{beta.replace(' ','_')}_bootstrap_dist.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of Objective Function Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_loss_vals = loss_vals[loss_vals['subset'].str.contains('bootsample')]\n",
    "min_val = np.floor(bootstrap_loss_vals['jaccard_buffer_mean'].min() * 10) / 10\n",
    "max_val = np.ceil(bootstrap_loss_vals['jaccard_buffer_mean'].max() * 10) / 10\n",
    "bin_size = 0.005\n",
    "\n",
    "left_ci = bootstrap_loss_vals['jaccard_buffer_mean'].quantile((1-confidence) / 2)\n",
    "right_ci = bootstrap_loss_vals['jaccard_buffer_mean'].quantile(1 - ((1-confidence) / 2))\n",
    "# left_tail = bootstrap_loss_vals['jaccard_buffer_mean'].quantile((1-confidence))\n",
    "print(f\"Mean:\",bootstrap_loss_vals['jaccard_buffer_mean'].mean())\n",
    "print(f\"Standard Deviation:\",bootstrap_loss_vals['jaccard_buffer_mean'].std())\n",
    "print(f\"Median:\",bootstrap_loss_vals['jaccard_buffer_mean'].median())\n",
    "print(f\"Confidence Interval: [{left_ci:.2f},{right_ci:.2f}]\")\n",
    "# print(f\"Left Tail: {left_tail:.2f}\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(bootstrap_loss_vals['jaccard_buffer_mean'],bins=np.arange(min_val,max_val+bin_size,bin_size), alpha=0.5, color='grey', label=None, edgecolor='Black')\n",
    "ax.set_xlabel(f'Jaccard Buffer Mean (Bin Size = {bin_size})')\n",
    "ax.set_ylabel(f'Frequency (N={beta_vals.shape[0]})')\n",
    "\n",
    "# confidence intervals\n",
    "ax.axvline(x=left_ci, color='black', linestyle='dashdot', label = 'Confidence Interval')\n",
    "ax.axvline(x=right_ci, color='black', linestyle='dashdot')\n",
    "\n",
    "# Set major ticks every 0.5 and minor ticks every 0.1\n",
    "ax.xaxis.set_major_locator(MultipleLocator(0.01))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.005))\n",
    "\n",
    "ax.yaxis.set_major_locator(MultipleLocator(25))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(5))\n",
    "\n",
    "# TODO add legend that has the confidence interval labels\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig(config['figures_fp']/f\"bootstrap_jaccard_buffer.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing with the mean betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap_loss_valsloss_vals[loss_vals['subset']=='bootstrap_final'\n",
    "dissag = post_calibration.post_calibration_disaggregate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_loss = dissag.loc[:,(dissag.columns.get_level_values(0) == 'bootstrap_final') & (dissag.columns.get_level_values(3) == 'jaccard_buffer')].dropna()\n",
    "bootstrap_loss = bootstrap_loss.droplevel([0,1,2],axis=1)\n",
    "# get the shortest ones\n",
    "shortest_disaggregate = post_calibration.shortest_disaggregate()\n",
    "shortest_disaggregate = shortest_disaggregate[shortest_disaggregate.index.isin(bootstrap_loss.index.tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make overlap histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Modeled',round(bootstrap_loss['jaccard_buffer'].mean(),2),'Mean Shortest',round(shortest_disaggregate['shortest_jaccard_buffer'].mean(),2))\n",
    "\n",
    "bin_size = 0.05\n",
    "bins = np.arange(0,1+bin_size,bin_size)\n",
    "\n",
    "# make figures\n",
    "# Create the histogram\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(shortest_disaggregate['shortest_jaccard_buffer'], bins=bins, alpha=0.5, label='Shortest', color='grey')\n",
    "ax.hist(bootstrap_loss['jaccard_buffer'], bins=bins, alpha=0.3, label='Calibrated', color='blue')\n",
    "\n",
    "# Adding labels, title, and legend with font size adjustments\n",
    "ax.set_xlabel('Jaccard Index Buffer')\n",
    "ax.set_ylabel(f'Frequency (N={bootstrap_loss.shape[0]})')\n",
    "ax.legend()\n",
    "ax.set_ylim([0,100])\n",
    "\n",
    "# # Adjusting the font size of the tick labels\n",
    "# plt.xticks(fontsize=22)\n",
    "# plt.yticks(fontsize=22)\n",
    "\n",
    "# Set major ticks every 0.5 and minor ticks every 0.1\n",
    "ax.xaxis.set_major_locator(MultipleLocator(0.1))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.05))\n",
    "\n",
    "# Show the plot\n",
    "# plt.savefig(config['figures_fp']/'bootstrap_overlap_hist.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation on holdout trips\n",
    "Find the least impedance path on the rest of the trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (config['calibration_fp']/'subsets.pkl').open('rb') as fh:\n",
    "    subset = pickle.load(fh)\n",
    "subset = [x for x in subset if 'random' == x[0]][0][1]\n",
    "with (config['calibration_fp']/'ready_for_calibration.pkl').open('rb') as fh:\n",
    "    match_results = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_final_fp = config['calibration_fp'] / 'results/bootstrap_final,validation,0.pkl'\n",
    "holdout = [(bootstrap_final_fp,list(set(list(match_results.keys())) - set(subset)))]\n",
    "print(len(holdout[0][1]),'trips in the holdout sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to run shortest paths on the holdout sample\n",
    "post_calibration.validation_workflow(holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to import the results\n",
    "shortest_disaggregate = post_calibration.shortest_disaggregate()\n",
    "shortest_disaggregate = shortest_disaggregate[shortest_disaggregate.index.isin(holdout[0][1])]\n",
    "shortest_disaggregate['shortest_jaccard_buffer'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_loss_vals = post_calibration.testing_aggregated_loss_dataframe()\n",
    "testing_loss_vals = testing_loss_vals[testing_loss_vals['subset']=='bootstrap_final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_loss_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import network\n",
    "# links, turns, length_dict, geo_dict, turn_G = rustworkx_routing_funcs.import_calibration_network(config)\n",
    "\n",
    "# # import matched traces (all of them at first)\n",
    "# with (config['calibration_fp']/'ready_for_calibration.pkl').open('rb') as fh:\n",
    "#     match_results = pickle.load(fh)\n",
    "# with (config['calibration_fp']/'subsets.pkl').open('rb') as fh:\n",
    "#     subset = pickle.load(fh)\n",
    "# subset = [x for x in subset if 'random' == x[0]][0][1]\n",
    "\n",
    "# # subset to a user's trips if user argument is provided\n",
    "# match_results = {tripid:item for tripid, item in match_results.items() if tripid in subset}\n",
    "# ods = utils.match_results_to_ods_w_year(match_results)\n",
    "# betas = [x['beta'] for x in bootstrap_model['betas_tup']] # get betas\n",
    "\n",
    "# # for backwards compatibility\n",
    "# # these only just now got added to the bootstrap_model dictionary \n",
    "# if bootstrap_model.get('link_impedance_function') is None:\n",
    "#     bootstrap_model['link_impedance_function'] = impedance_functions.link_impedance_function\n",
    "# if bootstrap_model.get('turn_impedance_function') is None:\n",
    "#     bootstrap_model['turn_impedance_function'] = impedance_functions.turn_impedance_function\n",
    "# if bootstrap_model.get('base_impedance_col') is None:\n",
    "#     bootstrap_model['base_impedance_col'] = 'travel_time_min'\n",
    "# if bootstrap_model.get('base_link_col') is None:\n",
    "#     bootstrap_model['base_link_col'] = None\n",
    "\n",
    "# #PROBLEM, need the impedance function used\n",
    "# modeled_dict = stochastic_optimization.impedance_routing(\n",
    "#     betas,\n",
    "#     bootstrap_model['betas_tup'],\n",
    "#     bootstrap_model['link_impedance_function'],\n",
    "#     bootstrap_model['base_impedance_col'],\n",
    "#     bootstrap_model['base_link_col'],       \n",
    "#     bootstrap_model['turn_impedance_function'],\n",
    "#     links,\n",
    "#     turns,\n",
    "#     turn_G,\n",
    "#     ods,\n",
    "#     bootstrap_model['set_to_zero'],\n",
    "#     bootstrap_model['set_to_inf']\n",
    "#     )\n",
    "\n",
    "# loss_dict = {}\n",
    "# # we could try storing these\n",
    "# for tripid, item in match_results.items(): # make sure you import this\n",
    "#     skip = False            \n",
    "    \n",
    "#     chosen = item['matched_edges'].values\n",
    "#     shortest = item['shortest_edges'].values\n",
    "    \n",
    "#     # NOTE one limitation of this is that if the origin node and destination node change at any point then\n",
    "#     # than this step will break\n",
    "#     od = (item['origin_node'],item['destination_node'])\n",
    "#     modeled = modeled_dict.get(od)\n",
    "#     if modeled is None:\n",
    "#         # print(f\"Missing shortest paths in calibration {loss_stem}\")\n",
    "#         skip = True\n",
    "#         break\n",
    "#     modeled = modeled['edge_list']\n",
    "\n",
    "#     #compute the loss values (store the intermediates too so total vs mean methods can be compared)\n",
    "#     modeled_jaccard_exact_intersection, modeled_jaccard_exact_union =  loss_functions.jaccard_exact(chosen,modeled,length_dict)\n",
    "#     modeled_jaccard_exact = modeled_jaccard_exact_intersection / modeled_jaccard_exact_union\n",
    "#     modeled_jaccard_buffer_intersection, modeled_jaccard_buffer_union =  loss_functions.jaccard_buffer(chosen,modeled,geo_dict)\n",
    "#     modeled_jaccard_buffer = modeled_jaccard_buffer_intersection / modeled_jaccard_buffer_union\n",
    "\n",
    "#     loss_dict[tripid] = {\n",
    "#         'modeled_edges': pd.DataFrame(modeled,columns=['linkid','reverse_link']),\n",
    "#         'modeled_length': round(np.array([length_dict.get(tripid[0],0) for tripid in modeled]).sum()/5280,1),\n",
    "#         'modeled_detour': round(loss_functions.detour_factor(modeled,shortest,length_dict),2),\n",
    "#         'modeled_jaccard_exact': round(modeled_jaccard_exact,2),\n",
    "#         'modeled_jaccard_exact_intersection': round(modeled_jaccard_exact_intersection,2),\n",
    "#         'modeled_jaccard_exact_union': round(modeled_jaccard_exact_union,2),\n",
    "#         'modeled_jaccard_buffer': round(modeled_jaccard_buffer,2),\n",
    "#         'modeled_jaccard_buffer_intersection': round(modeled_jaccard_buffer_intersection,2),\n",
    "#         'modeled_jaccard_buffer_union': round(modeled_jaccard_buffer_union,2),\n",
    "#     }\n",
    "\n",
    "# bootstrap_loss = pd.DataFrame().from_dict(loss_dict,orient='index')\n",
    "# bootstrap_loss['modeled_jaccard_buffer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For looking at the difference in route attributes\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define the number of columns to plot\n",
    "# columns = chosen_minus_modeled.columns  # Assuming both DataFrames have the same columns\n",
    "# num_columns = len(columns)\n",
    "\n",
    "# # Set up the figure and subplots\n",
    "# fig, axes = plt.subplots(num_columns, 1, figsize=(10, 100))\n",
    "\n",
    "# # Loop through each column and create a histogram\n",
    "# for i, column in enumerate(columns):\n",
    "#     # Plot histogram for chosen_minus_modeled\n",
    "#     axes[i].hist(chosen_minus_modeled[column].round(2), bins=50, alpha=0.5, color='blue', label='Chosen Minus Modeled', edgecolor='black')\n",
    "\n",
    "#     # Plot histogram for chosen_minus_shortest\n",
    "#     axes[i].hist(chosen_minus_shortest[column].round(2), bins=50, alpha=0.5, color='orange', label='Chosen Minus Shortest', edgecolor='black')\n",
    "\n",
    "#     # Set the title and labels for each subplot\n",
    "#     axes[i].set_title(f'Histogram for {column}')\n",
    "#     axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "#     # Center the histograms around zero by setting limits\n",
    "#     max_val = max(chosen_minus_modeled[column].max(), chosen_minus_shortest[column].max())\n",
    "#     min_val = min(chosen_minus_modeled[column].min(), chosen_minus_shortest[column].min())\n",
    "#     val = max(abs(max_val),abs(min_val))\n",
    "\n",
    "#     axes[i].set_xlim(min(val * -1, -1), max(val, 1))  # Adjust these limits based on your data\n",
    "\n",
    "#     # Add gridlines for better readability\n",
    "#     axes[i].grid(True)\n",
    "\n",
    "#     axes[i].legend()\n",
    "\n",
    "# # Set a common xlabel\n",
    "# axes[-1].set_xlabel('Value')\n",
    "\n",
    "# # Add a legend to the last subplot\n",
    "# # axes[-1].legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.tight_layout()  # Adjust subplots to fit in the figure area.\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
